var documenterSearchIndex = {"docs":
[{"location":"writing_good_rules.html#On-writing-good-rrule-/-frule-methods","page":"Writing Good Rules","title":"On writing good rrule / frule methods","text":"","category":"section"},{"location":"writing_good_rules.html#Code-Style","page":"Writing Good Rules","title":"Code Style","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Use named local functions for the pullback in an rrule.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"# good:\nfunction rrule(::typeof(foo), x)\n    Y = foo(x)\n    function foo_pullback(Ȳ)\n        return NoTangent(), bar(Ȳ)\n    end\n    return Y, foo_pullback\nend\n#== output\njulia> rrule(foo, 2)\n(4, var\"#foo_pullback#11\"())\n==#\n\n# bad:\nfunction rrule(::typeof(foo), x)\n    return foo(x), x̄ -> (NoTangent(), bar(x̄))\nend\n#== output:\njulia> rrule(foo, 2)\n(4, var\"##9#10\"())\n==#","category":"page"},{"location":"writing_good_rules.html#Use-ZeroTangent()-as-the-return-value","page":"Writing Good Rules","title":"Use ZeroTangent() as the return value","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"The ZeroTangent() object exists as an alternative to directly returning 0 or zeros(n). It allows more optimal computation when chaining pullbacks/pushforwards, to avoid work. They should be used where possible.","category":"page"},{"location":"writing_good_rules.html#Use-Thunks-appropriately","page":"Writing Good Rules","title":"Use Thunks appropriately","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"If work is only required for one of the returned differentials, then it should be wrapped in a @thunk (potentially using a begin-end block).","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"If there are multiple return values, their computation should almost always be wrapped in a @thunk.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Do not wrap variables in a @thunk; wrap the computations that fill those variables in @thunk:","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"# good:\n∂A = @thunk(foo(x))\nreturn ∂A\n\n# bad:\n∂A = foo(x)\nreturn @thunk(∂A)","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"In the bad example foo(x) gets computed eagerly, and all that the thunk is doing is wrapping the already calculated result in a function that returns it.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Do not use @thunk if this would be equal or more work than actually evaluating the expression itself. Examples being:","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"The expression being a constant\nThe expression is merely wrapping something in a struct, such as Adjoint(x) or Diagonal(x)\nThe expression being itself a thunk\nThe expression being from another rrule or frule; it would be @thunked if required by the defining rule already.\nThere is only one derivative being returned, so from the fact that the user called frule/rrule they clearly will want to use that one.","category":"page"},{"location":"writing_good_rules.html#Ensure-you-remain-in-the-primal's-subspace-(i.e.-use-ProjectTo-appropriately)","page":"Writing Good Rules","title":"Ensure you remain in the primal's subspace (i.e. use ProjectTo appropriately)","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Rules with abstractly-typed arguments may return incorrect answers when called with certain concrete types. A classic example is the matrix-matrix multiplication rule, a naive definition of which follows:","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"function rrule(::typeof(*), A::AbstractMatrix, B::AbstractMatrix)\n    function times_pullback(ȳ)\n        dA = ȳ * B'\n        dB = A' * ȳ\n        return NoTangent(), dA, dB\n    end\n    return A * B, times_pullback \nend","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"When computing *(A, B), where A isa Diagonal and B isa Matrix, the output will be a Matrix. As a result, ȳ in the pullback will be a Matrix, and consequently dA for a A isa Diagonal will be a Matrix, which is wrong. Not only is it the wrong type, but it can contain non-zeros off the diagonal, which is not possible, it is outside of the subspace. While a specialised rules can indeed be written for the Diagonal case, there are many other types and we don't want to be forced to write a rule for each of them. Instead, project_A = ProjectTo(A) can be used (outside the pullback) to extract an object that knows how to project onto the type of A (e.g. also knows the size of the array). This object can be called with a tangent ȳ * B', by doing project_A(ȳ * B'), to project it on the tangent space of A. The correct rule then looks like","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"function rrule(::typeof(*), A::AbstractMatrix, B::AbstractMatrix)\n    project_A = ProjectTo(A)\n    project_B = ProjectTo(B)\n    function times_pullback(ȳ)\n        dA = ȳ * B'\n        dB = A' * ȳ\n        return NoTangent(), project_A(dA), project_B(dB)\n    end\n    return A * B, times_pullback\nend","category":"page"},{"location":"writing_good_rules.html#Structs:-constructors-and-functors","page":"Writing Good Rules","title":"Structs: constructors and functors","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"To define an frule or rrule for a function foo we dispatch on the type of foo, which is typeof(foo). For example, the rrule signature would be like:","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"function rrule(::typeof(foo), args...; kwargs...)\n    ...\n    return y, foo_pullback\nend","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"For a struct Bar,","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"struct Bar\n    a::Float64\nend\n\n(bar::Bar)(x, y) = return bar.a + x + y # functor (i.e. callable object, overloading the call action)","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"we can define an frule/rrule for the Bar constructor(s), as well as any Bar functors.","category":"page"},{"location":"writing_good_rules.html#Constructors","page":"Writing Good Rules","title":"Constructors","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"To define an rrule for a constructor for a  type Bar we need to be careful to dispatch only on Type{Bar}. For example, the rrule signature for a Bar constructor would be like:","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"function ChainRulesCore.rrule(::Type{Bar}, a)\n    Bar_pullback(Δbar) = NoTangent(), Δbar.a\n    return Bar(a), Bar_pullback\nend","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Use Type{<:Bar} (with the <:) for non-concrete types, such that the rrule is defined for all subtypes. In particular, be careful not to use typeof(Bar) here. Because typeof(Bar) is DataType, using this to define an rrule/frule will define an rrule/frule for all constructors.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"You can check which to use with Core.Typeof:","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"julia> function foo end\nfoo (generic function with 0 methods)\n\njulia> typeof(foo)\ntypeof(foo)\n\njulia> Core.Typeof(foob)\ntypeof(foo)\n\njulia> typeof(Bar)\nDataType\n\njulia> Core.Typeof(Bar)\nType{Bar}\n\njulia> abstract type AbstractT end\n\njulia> typeof(AbstractT)\nDataType\n\njulia> Core.Typeof(AbstractT)\nType{AbstractT}","category":"page"},{"location":"writing_good_rules.html#Functors-(callable-objects)","page":"Writing Good Rules","title":"Functors (callable objects)","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"In contrast to defining a rule for a constructor, it is possible to define rules for calling an instance of an object. In that case, use bar::Bar, i.e.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"function ChainRulesCore.rrule(bar::Bar, x, y)\n    # Notice the first return is not `NoTangent()`\n    Bar_pullback(Δy) = Tangent{Bar}(;a=Δy), Δy, Δy\n    return bar(x, y), Bar_pullback\nend","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"to define the rules.","category":"page"},{"location":"writing_good_rules.html#Ensure-your-pullback-can-accept-the-right-types","page":"Writing Good Rules","title":"Ensure your pullback can accept the right types","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"As a rule the number of types you need to accept in a pullback is theoretically unlimitted, but practically highly constrained to inline with the primal return type. The three kinds of inputs you will practically need to accept one or more of: natural tangents, structural tangents, and thunks. You do not in general have to handle AbstractZeros as the AD system will not call the pullback if the input is a zero, since the output will also be. Some more background information on these types can be found in the design notes. In many cases these all all tangents can be treated the same: tangent types overload a bunch of linear-operators, and the majority of functions used inside a pullback are linear operators. If you find linear operators from Base/stdlibs that are not supported, consider openning an issue or PR on the ChainRulesCorejl repo.","category":"page"},{"location":"writing_good_rules.html#Natural-tangents","page":"Writing Good Rules","title":"Natural tangents","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Natural tangent types are the types you might feel the tangent should be. These are a purely human notion, they are the types the user wants to use because they make the math easy. There is currently no formal definition of what consistutes a natural tangent, but there are a few heuristics. For example, if a primal type P overloads subtraction (-(::P,::P)) then that generally returns a  natural tangent type for P; but this is not required to be defined.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Common cases for types that represent a vector-space (e.g. Float64, Array{Float64}) is that the natural tangent type is the same as the primal type. However, this is not always the case. For example for a PDiagMat a natural tangent is Diagonal since there is no requirement that a positive definite diagonal matrix has a positive definite tangent. Another example is for a DateTime, any Period subtype, such as Millisecond or Nanosecond is a natural differential. There are often many different natural tangent types for a given primal type. However, they are generally closely related and duck-type the same. For example, for most AbstractArray subtypes, most other AbstractArrays (of right size and element type) can be considered as natural tangent types.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Not all types have natural tangent types. For example there is no natural differential for a Tuple. It is not a Tuple since that doesn't have any method for +. Similar is true for many structs. For those cases there is only a structural differential.","category":"page"},{"location":"writing_good_rules.html#Structural-tangents","page":"Writing Good Rules","title":"Structural tangents","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Structural tangents are tangent types that shadow the structure of the primal type. They are represented by the Tangent type. They can represent any composite type, such as a tuple, or a structure (or a NamedTuple) etc.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"info: Do I have to support the structural tangents as well?\nTechnically, you might not actually have to write rules to accept structural tangents; if the AD system never has to decompose down to the level of getfield. This is common for types that don't support user getfield/getproperty access, and that have a lot of rules for the ways they are accessed (such cases include some AbstractArray subtypes). You really should support it just in case; especially if the primal type in question is not restricted to a well-tested concrete type. But if it is causing struggles, then you can leave it off til someone complains.","category":"page"},{"location":"writing_good_rules.html#Thunks","page":"Writing Good Rules","title":"Thunks","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"A thunk (either a Thunk, or a InplaceableThunk), represents a delayed computation. They can be thought of as a wrapper of the value the computation returns. In this sense they wrap either a natural or structural tangent.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"warning: You should to support AbstractThunk inputs even if you don't use thunks\nUnfortunately the AD sytems do not know what rules support thunks and what do not.  So all rules have to; at least if they want to play nice with arbitary AD systems.  Luckily it is not hard: much of the time they will duck-type as the object they wrap.  If not, then just add a unthunk after the start of your pullback.  (Even when they do duck-type, if they are used multiple times then unthunking at the start will prevent them from being recomputed.)  If you are using @thunk and the input is only needed for one of them then the unthunk should be in that one.  If not, and you have a bunch of pullbacks you might like to write a little helper unthunking(f) = x̄ -> f(unthunk(x̄)) that you can wrap your pullback function in before returning it from the rrule.  Yes, this is a bit of boiler-plate, and it is unfortunate.  Sadly, it is needed because if the AD wants to benefit it can't get that benifit unless things are not unthunked unnecessarily.  Which eventually allows them in some cases to never be unthunked at all.  There are two ways common things are never unthunked.  One is if the unthunking happens inside a @thunk which is never unthunked itself because it is the tangent for a primal input that never has it's tangent queried.  The second is if they are not unthunked because the rule does not need to know what is inside: consider the pullback for identity: x̄ -> (NoTangent(), x̄).","category":"page"},{"location":"writing_good_rules.html#Use-@not_implemented-appropriately","page":"Writing Good Rules","title":"Use @not_implemented appropriately","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"One can use @not_implemented to mark missing differentials. This is helpful if the function has multiple inputs or outputs, and one has worked out analytically and implemented some but not all differentials.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"It is recommended to include a link to a GitHub issue about the missing differential in the debugging information:","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"@not_implemented(\n\"\"\"\nderivatives of Bessel functions with respect to the order are not implemented:\nhttps://github.com/JuliaMath/SpecialFunctions.jl/issues/160\n\"\"\"\n)","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Do not use @not_implemented if the differential does not exist mathematically (use NoTangent() instead).","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"While this is more verbose, it ensures that if an error is thrown during the pullback the gensym name of the local function will include the name you gave it. This makes it a lot simpler to debug from the stacktrace.","category":"page"},{"location":"writing_good_rules.html#Use-rule-definition-tools","page":"Writing Good Rules","title":"Use rule definition tools","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Rule definition tools can help you write more frules and the rrules with less lines of code.","category":"page"},{"location":"writing_good_rules.html#[@non_differentiable](@ref)","page":"Writing Good Rules","title":"@non_differentiable","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"For non-differentiable functions the @non_differentiable macro can be used. For example, instead of manually defining the frule and the rrule for string concatenation *(String..), the macro call","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"@non_differentiable *(String...)","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"defines the following frule and rrule automatically","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"function ChainRulesCore.frule(var\"##_#1600\", ::Core.Typeof(*), String::Any...; kwargs...)\n    return (*(String...; kwargs...), NoTangent())\nend\nfunction ChainRulesCore.rrule(::Core.Typeof(*), String::Any...; kwargs...)\n    return (*(String...; kwargs...), function var\"*_pullback\"(_)\n        (ZeroTangent(), ntuple((_->NoTangent()), 0 + length(String))...)\n    end)\nend","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Note that the types of arguments are propagated to the frule and rrule definitions. This is needed in case the function differentiable for some but not for other types of arguments. For example *(1, 2, 3) is differentiable, and is not defined with the macro call above.","category":"page"},{"location":"writing_good_rules.html#[@scalar_rule](@ref)","page":"Writing Good Rules","title":"@scalar_rule","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"For functions involving only scalars, i.e. subtypes of Number (no structs, Strings...), both the frule and the rrule can be defined using a single @scalar_rule macro call. ","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Note that the function does not have to be mathbbR rightarrow mathbbR. In fact, any number of scalar arguments is supported, as is returning a tuple of scalars.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"See docstrings for the comprehensive usage instructions.","category":"page"},{"location":"writing_good_rules.html#Be-careful-about-pullback-closures-calling-other-methods-of-themselves","page":"Writing Good Rules","title":"Be careful about pullback closures calling other methods of themselves","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Due to JuliaLang/Julia#40990, a closure calling another (or the same) method of itself often comes out uninferable (and thus effectively type-unstable). This can be avoided by moving the pullback definition outside the function, so that it is no longer a closure. For example:","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"double_it(x::AbstractArray) = 2 .* x\n\nfunction ChainRulesCore.rrule(::typeof(double_it), x)\n    double_it_pullback(ȳ::AbstractArray) = (NoTangent(), 2 .* ȳ)\n    double_it_pullback(ȳ::AbstractThunk) = double_it_pullback(unthunk(ȳ))\n    return double_it(x), double_it_pullback\nend","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Ends up infering a return type of Any","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"julia> _, pullback = rrule(double_it, [2.0, 3.0])\n([4.0, 6.0], var\"#double_it_pullback#8\"(Core.Box(var\"#double_it_pullback#8\"(#= circular reference @-2 =#))))\n\njulia> @code_warntype pullback(@thunk([10.0, 10.0]))\nVariables\n  #self#::var\"#double_it_pullback#8\"\n  ȳ::Core.Const(Thunk(var\"#9#10\"()))\n  double_it_pullback::Union{}\n\nBody::Any\n1 ─ %1 = Core.getfield(#self#, :double_it_pullback)::Core.Box\n│   %2 = Core.isdefined(%1, :contents)::Bool\n└──      goto #3 if not %2\n2 ─      goto #4\n3 ─      Core.NewvarNode(:(double_it_pullback))\n└──      double_it_pullback\n4 ┄ %7 = Core.getfield(%1, :contents)::Any\n│   %8 = Main.unthunk(ȳ)::Vector{Float64}\n│   %9 = (%7)(%8)::Any\n└──      return %9","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"This can be solved by moving the pullbacks outside the function so they are not closures, and thus to not run into this upstream issue. In this case that is fairly simple, since this example doesn't close over anything (if it did then would need a closure calling an outside function that calls itself. See this example.).","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"_double_it_pullback(ȳ::AbstractArray) = (NoTangent(), 2 .* ȳ)\n_double_it_pullback(ȳ::AbstractThunk) = _double_it_pullback(unthunk(ȳ))\n\nfunction ChainRulesCore.rrule(::typeof(double_it), x)\n    return double_it(x), _double_it_pullback\nend","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"This infers just fine:","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"julia> _, pullback = rrule(double_it, [2.0, 3.0])\n([4.0, 6.0], _double_it_pullback)\n\njulia> @code_warntype pullback(@thunk([10.0, 10.0]))\nVariables\n  #self#::Core.Const(_double_it_pullback)\n  ȳ::Core.Const(Thunk(var\"#7#8\"()))\n\nBody::Tuple{NoTangent, Vector{Float64}}\n1 ─ %1 = Main.unthunk(ȳ)::Vector{Float64}\n│   %2 = Main._double_it_pullback(%1)::Core.PartialStruct(Tuple{NoTangent, Vector{Float64}}, Any[Core.Const(NoTangent()), Vector{Float64}])\n└──      return %2","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Though in this particular case, it can also be solved by taking advantage of duck-typing and just writing one method. Thus avoiding the call that confuses the compiler. Thunks duck-type as the type they wrap in most cases: including broadcast multiplication.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"function ChainRulesCore.rrule(::typeof(double_it), x)\n    double_it_pullback(ȳ) = (NoTangent(), 2 .* ȳ)\n    return double_it(x), double_it_pullback\nend","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"This infers perfectly.","category":"page"},{"location":"writing_good_rules.html#Write-tests","page":"Writing Good Rules","title":"Write tests","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"ChainRulesTestUtils.jl provides tools for writing tests based on FiniteDifferences.jl. Take a look at the documentation or the existing ChainRules.jl tests to see how to write the tests.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"warning: Warning\nDon't use analytical derivations for derivatives in the tests. Those are what you use to define the rules, and so can not be confidently used in the test. If you misread/misunderstood them, then your tests/implementation will have the same mistake. Use finite differencing methods instead, as they are based on the primal computation.","category":"page"},{"location":"writing_good_rules.html#CAS-systems-are-your-friends.","page":"Writing Good Rules","title":"CAS systems are your friends.","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"It is very easy to check gradients or derivatives with a computer algebra system (CAS) like WolframAlpha.","category":"page"},{"location":"writing_good_rules.html#Which-functions-need-rules?","page":"Writing Good Rules","title":"Which functions need rules?","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"In principle, a perfect AD system only needs rules for basic operations and can infer the rules for more complicated functions automatically. In practice, performance needs to be considered as well.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Some functions use ccall internally, for example ^. These functions can not be differentiated through by AD systems, and need custom rules.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Other functions can in principle be differentiated through by an AD system, but there exists a mathematical insight that can dramatically improve the computation of the derivative. An example is numerical integration, where writing a rule removes the need to perform AD through numerical integration.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Furthermore, AD systems make different trade-offs in performance due to their design. This means that a certain rule will help one AD system, but not improve (and also not harm) another. Below, we list some patterns relevant for the Zygote.jl AD system.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Rules for functions which mutate its arguments, e.g. sort!, should not be written at the moment. While technically they are supported, they would break Zygote.jl such that it would sometimes quietly return the wrong answer. This may be resolved in the future by allowing AD systems to opt-in or opt-out of certain types of rules.","category":"page"},{"location":"writing_good_rules.html#Patterns-that-need-rules-in-[Zygote.jl](https://github.com/FluxML/Zygote.jl)","page":"Writing Good Rules","title":"Patterns that need rules in Zygote.jl","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"There are a few classes of functions that Zygote can not differentiate through. Custom rules will need to be written for these to make AD work.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Other patterns can be AD'ed through, but the backward pass performance can be greatly improved by writing a rule.","category":"page"},{"location":"writing_good_rules.html#Functions-which-mutate-arrays","page":"Writing Good Rules","title":"Functions which mutate arrays","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"For example,","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"function addone!(array)\n    array .+= 1\n    return sum(array)\nend","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"complains that","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"julia> using Zygote\njulia> gradient(addone!, a)\nERROR: Mutating arrays is not supported","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"However, upon adding the rrule (restart the REPL after calling gradient)","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"function ChainRules.rrule(::typeof(addone!), a)\n    y = addone!(a)\n    function addone!_pullback(ȳ)\n        return NoTangent(), ones(length(a))\n    end\n    return y, addone!_pullback\nend","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"the gradient can be evaluated:","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"julia> gradient(addone!, a)\n([1.0, 1.0, 1.0],)","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"note: Why restarting REPL after calling `gradient`?\nWhen gradient is called in Zygote for a function with no rrule defined, a backward pass for the function call is generated and cached. When gradient is called for the second time on the same function signature, the backward pass is reused without checking whether an an rrule has been defined between the two calls to gradient.If an rrule is defined before the first call to gradient it should register the rule and use it, but that prevents comparing what happens before and after the rrule is defined. To compare both versions with and without an rrule in the REPL simultaneously, define a function f(x) = <body> (no rrule), another function f_cr(x) = f(x), and an rrule for f_cr.","category":"page"},{"location":"writing_good_rules.html#Exception-handling","page":"Writing Good Rules","title":"Exception handling","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Zygote does not support differentiating through try/catch statements. For example, differentiating through","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"function exception(x)\n    try\n        return x^2\n    catch e\n        println(\"could not square input\")\n        throw(e)\n    end\nend","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"does not work","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"julia> gradient(exception, 3.0)\nERROR: Compiling Tuple{typeof(exception),Int64}: try/catch is not supported.","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"without an rrule defined (restart the REPL after calling gradient)","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"function ChainRulesCore.rrule(::typeof(exception), x)\n    y = exception(x)\n    function exception_pullback(ȳ)\n        return NoTangent(), 2*x\n    end\n    return y, exception_pullback\nend","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"julia> gradient(exception, 3.0)\n(6.0,)","category":"page"},{"location":"writing_good_rules.html#Loops","page":"Writing Good Rules","title":"Loops","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Julia runs loops fast. Unfortunately Zygote differentiates through loops slowly. So, for example, computing the mean squared error by using a loop","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"function mse(y, ŷ)\n    N = length(y)\n    s = 0.0\n    for i in 1:N\n        s +=  (y[i] - ŷ[i])^2.0\n    end\n    return s/N\nend","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"takes a lot longer to AD through","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"julia> y = rand(30)\njulia> ŷ = rand(30)\njulia> @btime gradient(mse, $y, $ŷ)\n  38.180 μs (993 allocations: 65.00 KiB)","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"than if we supply an rrule, (restart the REPL after calling gradient)","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"function ChainRules.rrule(::typeof(mse), x, x̂)\n    output = mse(x, x̂)\n    function mse_pullback(ȳ)\n        N = length(x)\n        g = (2 ./ N) .* (x .- x̂) .* ȳ\n        return NoTangent(), g, -g\n    end\n    return output, mse_pullback\nend","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"which is much faster","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"julia> @btime gradient(mse, $y, $ŷ)\n  143.697 ns (2 allocations: 672 bytes)","category":"page"},{"location":"writing_good_rules.html#Inplace-accumulation","page":"Writing Good Rules","title":"Inplace accumulation","text":"","category":"section"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Inplace accumulation of gradients is slow in Zygote. The issue, demonstrated in the folowing example, is that the gradient of getindex allocates an array of zeros with a single non-zero element. ","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"function sum3(array)\n    x = array[1]\n    y = array[2]\n    z = array[3]\n    return x+y+z\nend","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"julia> @btime gradient(sum3, rand(30))\n  424.510 ns (9 allocations: 2.06 KiB)","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"Computing the gradient with only a single array allocation using an rrule (restart the REPL after calling gradient)","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"function ChainRulesCore.rrule(::typeof(sum3), a)\n    y = sum3(a)\n    function sum3_pullback(ȳ)\n        grad = zeros(length(a))\n        grad[1:3] .+= ȳ\n        return NoTangent(), grad\n    end\n    return y, sum3_pullback\nend","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"turns out to be significantly faster ","category":"page"},{"location":"writing_good_rules.html","page":"Writing Good Rules","title":"Writing Good Rules","text":"julia> @btime gradient(sum3, rand(30))\n  192.818 ns (3 allocations: 784 bytes)","category":"page"},{"location":"opting_out_of_rules.html#opt_out","page":"Opting out of rules","title":"Opting out of rules","text":"","category":"section"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"It is common to define rules fairly generically. Often matching (or exceeding) how generic the matching original primal method is. Sometimes this is not the correct behaviour. Sometimes the AD can do better than this human defined rule. If this is generally the case, then we should not have the rule defined at all. But if it is only the case for a particular set of types, then we want to opt-out just that one. This is done with the @opt_out macro.","category":"page"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"Consider one a rrule for sum (the following simplified from the one in ChainRules.jl itself)","category":"page"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"function rrule(::typeof(sum), x::AbstractArray{<:Number})\n    y = sum(x; dims=dims)\n    project = ProjectTo(x)\n    function sum_pullback(ȳ)\n        x̄ = project(fill(ȳ, size(x)))\n        return (NoTangent(), x̄)\n    end\n    return y, sum_pullback\nend","category":"page"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"That is a fairly reasonable rrule for the vast majority of cases. You might have a custom array type for which you could write a faster rule. In which case you would do that, by writing a faster, more specific, rrule. But sometimes, it is the case that ADing the (faster, more specific) primal for your custom array type would yeild the faster pullback without you having to write a rrule by hand.","category":"page"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"Consider a summing  SkewSymmetric (anti-symmetric) matrix. The skew symmetric matrix has structural zeros on the diagonal, and off-diagonals are paired with their negation. Thus the sum is always going to be zero. As such the author of that matrix type would probably have overloaded sum(x::SkewSymmetric{T}) where T = zero(T). ADing this would result in the tangent computed for x as ZeroTangent() and it would be very fast since AD can see that x is never used in the right-hand side. In contrast the generic method for AbstractArray defined above would have to allocate the fill array, and then compute the skew projection. Only to findout the output would be projected to SkewSymmetric(zeros(T)) anyway (slower, and a less useful type).","category":"page"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"To opt-out of using the generic rrule and to allow the AD system to do its own thing we use the @opt_out macro, to say to not use it for sum of SkewSymmetric.","category":"page"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"@opt_out rrule(::typeof(sum), ::SkewSymmetric)","category":"page"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"Perhaps we might not want to ever use rules for SkewSymmetric, because we have determined that it is always better to leave it to the AD, unless a verys specific rule has been written[1]. We could then opt-out for all 1 arg functions.","category":"page"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"@opt_out rrule(::Any, ::SkewSymmetric)","category":"page"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"Though this is likely to cause some method-ambiguities, if we do it for more, but we can resolve those.","category":"page"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"Similar can be done  @opt_out frule. It can also be done passing in a RuleConfig.","category":"page"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"warning: If the general rule uses a config, the opt-out must also\nFollowing the same principles as for rules with config, a rule with a RuleConfig argument will take precedence over one without, including if that one is a opt-out rule. But if the general rule does not use a config, then the opt-out rule can use a config. This allows, for example, you to use opt-out to avoid a particular AD system using a opt-out rule that takes that particular AD's config.","category":"page"},{"location":"opting_out_of_rules.html#How-to-support-this-(for-AD-implementers)","page":"Opting out of rules","title":"How to support this (for AD implementers)","text":"","category":"section"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"We provide two ways to know that a rule has been opted out of.","category":"page"},{"location":"opting_out_of_rules.html#rrule-/-frule-returns-nothing","page":"Opting out of rules","title":"rrule / frule returns nothing","text":"","category":"section"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"@opt_out defines a frule or rrule matching the signature that returns nothing.","category":"page"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"If you are in a position to generate code, in response to values returned by function calls then you can do something like:","category":"page"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"res = rrule(f, xs)\nif res === nothing\n    y, pullback = perform_ad_via_decomposition(r, xs)  # do AD without hitting the rrule\nelse\n    y, pullback = res\nend","category":"page"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"The Julia compiler will specialize based on inferring the return type of rrule, and so can remove that branch.","category":"page"},{"location":"opting_out_of_rules.html#no_rrule-/-no_frule-has-a-method","page":"Opting out of rules","title":"no_rrule / no_frule has a method","text":"","category":"section"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"@opt_out also defines a method for  ChainRulesCore.no_frule or ChainRulesCore.no_rrule. The body of this method doesn't matter, what matters is that it is a method-table. A simple thing you can do with this is not support opting out. To do this, filter all methods from the rrule/frule method table that also occur in the no_frule/no_rrule table. This will thus avoid ever hitting an rrule/frule that returns nothing (and thus prevents your library from erroring). This is easily done, though it does mean ignoring the user's stated desire to opt out of the rule.","category":"page"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"More complex you can use this to generate code that triggers your AD. If for a given signature there is a more specific method in the no_rrule/no_frule method-table, than the one that would be hit from the rrule/frule table (Excluding the one that exactly matches which will return nothing) then you know that the rule should not be used. You can, likely by looking at the primal method table, workout which method you would have it if the rule had not been defined, and then invoke it.","category":"page"},{"location":"opting_out_of_rules.html","page":"Opting out of rules","title":"Opting out of rules","text":"[1]: seems unlikely, but it is possible, there is a lot of structure that can be taken advantage of for some matrix types.","category":"page"},{"location":"debug_mode.html#Debug-Mode","page":"Debug Mode","title":"Debug Mode","text":"","category":"section"},{"location":"debug_mode.html","page":"Debug Mode","title":"Debug Mode","text":"ChainRulesCore supports a debug mode which you can use while writing new rules. It provides better error messages. If you are developing some new rules, and you get a weird error message, it is worth enabling debug mode.","category":"page"},{"location":"debug_mode.html","page":"Debug Mode","title":"Debug Mode","text":"There is some overhead to having it enabled, so it is disabled by default.","category":"page"},{"location":"debug_mode.html","page":"Debug Mode","title":"Debug Mode","text":"To enable, redefine the ChainRulesCore.debug_mode function to return true.","category":"page"},{"location":"debug_mode.html","page":"Debug Mode","title":"Debug Mode","text":"ChainRulesCore.debug_mode() = true","category":"page"},{"location":"debug_mode.html#Features-of-Debug-Mode:","page":"Debug Mode","title":"Features of Debug Mode:","text":"","category":"section"},{"location":"debug_mode.html","page":"Debug Mode","title":"Debug Mode","text":"If you add a Tangent to a primal value, and it was unable to construct a new primal values, then a better error message will be displayed detailing what overloads need to be written to fix this.\nduring add!!, if an InplaceThunk is used, and it runs the code that is supposed to run in place, but the return result is not the input (with updated values), then an error is thrown. Rather than silently using what ever values were returned.","category":"page"},{"location":"config.html#config","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"","category":"section"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"RuleConfig is a method for making rules conditionally defined based on the presence of certain features in the AD system. One key such feature is the ability to perform AD either in forwards or reverse mode or both.","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"This is done with a trait-like system (not Holy Traits), where the RuleConfig has a union of types as its only type-parameter. Where each type represents a particular special feature of this AD. To indicate that the AD system has a special property, its RuleConfig should be defined as:","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"struct MyADRuleConfig <: RuleConfig{Union{Feature1, Feature2}} end","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"And rules that should only be defined when an AD has a particular special property write:","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"rrule(::RuleConfig{>:Feature1}, f, args...) = # rrule that should only be define for ADs with `Feature1`\n\nfrule(::RuleConfig{>:Union{Feature1,Feature2}}, f, args...) = # frule that should only be define for ADs with both `Feature1` and `Feature2`","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"warning: Rules with Config always take precedence over rules without\nEven if the other arguments are more specific the rule with the config will always take precedence. For example of there is a rule rrule(::RuleConfig, ::typeof(foo), ::Any) and other rrule(foo, ::Float64), the first will always be selected. This is because the AD will always attempt to provide its config when checking for a rule, and only if that doesn't match, will the config-less rule be tried. In practice this doesn't happen often, but when it does the solution is a little ugly – though very similar to resolving method ambiguities.   You need to manually add methods that dispatch from a rule with config to the one without. See for example the rule for sum(abs2, xs) in ChainRules.jl.","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"A prominent use of this is in declaring that the AD system can, or cannot support being called from within the rule definitions.","category":"page"},{"location":"config.html#Declaring-support-for-calling-back-into-ADs","page":"Rule configurations and calling back into AD","title":"Declaring support for calling back into ADs","text":"","category":"section"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"To declare support or lack of support for forward and reverse-mode, use the two pairs of complementary types. For reverse mode: HasReverseMode, NoReverseMode. For forwards mode: HasForwardsMode, NoForwardsMode. AD systems that support any calling back into AD should have one from each set.","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"If an AD HasReverseMode, then it must define rrule_via_ad for that RuleConfig subtype. Similarly, if an AD HasForwardsMode then it must define frule_via_ad for that RuleConfig subtype.","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"For example:","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"struct MyReverseOnlyADRuleConfig <: RuleConfig{Union{HasReverseMode, NoForwardsMode}} end\n\nfunction ChainRulesCore.rrule_via_ad(::MyReverseOnlyADRuleConfig, f, args...)\n    ...\n    return y, pullback\nend","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"Note that it is not actually required that the same AD is used for forward and reverse. For example Nabla.jl is a reverse mode AD. It might declare that it HasForwardsMode, and then define a wrapper around ForwardDiff.jl in order to provide that capacity.","category":"page"},{"location":"config.html#Writing-rules-that-call-back-into-AD","page":"Rule configurations and calling back into AD","title":"Writing rules that call back into AD","text":"","category":"section"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"To define e.g. rules for higher order functions, it is useful to be able to call back into the AD system to get it to do some work for you.","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"For example the rule for reverse mode AD for map might like to use forward mode AD if one is available. Particularly for the case where only a single input collection is being mapped over. In that case we know the most efficient way to compute that sub-program is in forwards, as each call with-in the map only takes a single input.","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"Note: the following is not the most efficient rule for map via forward, but attempts to be clearer for demonstration purposes.","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"function rrule(config::RuleConfig{>:HasForwardsMode}, ::typeof(map), f::Function, x::Array{<:Real})\n    # real code would support functors/closures, but in interest of keeping example short we exclude it:\n    @assert (fieldcount(typeof(f)) == 0) \"Functors/Closures are not supported\"\n\n    y_and_ẏ = map(x) do xi\n        frule_via_ad(config, (NoTangent(), one(xi)), f, xi)\n    end\n    y = first.(y_and_ẏ)\n    ẏ = last.(y_and_ẏ)\n\n    pullback_map(ȳ) = NoTangent(), NoTangent(), ȳ .* ẏ\n    return y, pullback_map\nend","category":"page"},{"location":"config.html#Writing-rules-that-depend-on-other-special-requirements-of-the-AD.","page":"Rule configurations and calling back into AD","title":"Writing rules that depend on other special requirements of the AD.","text":"","category":"section"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"The >:HasReverseMode and >:HasForwardsMode are two examples of special properties that a RuleConfig could allow. Others could also exist, but right now they are the only two. It is likely that in the future such will be provided for e.g. mutation support.","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"Such a thing would look like:","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"struct SupportsMutation end\n\nfunction rrule(\n    ::RuleConfig{>:SupportsMutation}, typeof(push!), x::Vector\n)\n    y = push!(x)\n\n    function push!_pullback(ȳ)\n        pop!(x)  # undo change to primal incase it is used in another pullback we haven't called yet\n        pop!(ȳ)  # accumulate gradient via mutating ȳ, then return ZeroTangent\n        return NoTangent(), ZeroTangent()\n    end\n\n    return y, push!_pullback\nend","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"and it would be used in the AD e.g. as follows:","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"struct EnzymeRuleConfig <: RuleConfig{Union{SupportsMutation, HasReverseMode, NoForwardsMode}}","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"Note: you can only depend on the presence of a feature, not its absence. This means we may need to define features and their compliments, when one is not the obvious default (as in the fast of HasReverseMode/NoReverseMode and HasForwardsMode/NoForwardsMode.).","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"Such special properties generally should only be defines in ChainRulesCore. (Theoretically, they could be defined elsewhere, but the AD and the package containing the rule need to load them, and ChainRulesCore is the place for things like that.)","category":"page"},{"location":"config.html#Writing-rules-that-are-only-for-your-own-AD","page":"Rule configurations and calling back into AD","title":"Writing rules that are only for your own AD","text":"","category":"section"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"A special case of the above is writing rules that are defined only for your own AD. Rules which otherwise would be type-piracy, and would affect other AD systems. This could be done via making up a special property type and dispatching on it. But there is no need, as we can dispatch on the RuleConfig subtype directly.","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"For example in order to avoid mutation in nested AD situations, Zygote might want to have a rule for add!! that makes it just do +.","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"struct ZygoteConfig <: RuleConfig{Union{}} end\n\nrrule(::ZygoteConfig, typeof(ChainRulesCore.add!!), a, b) = a+b, Δ->(NoTangent(), Δ, Δ)","category":"page"},{"location":"config.html","page":"Rule configurations and calling back into AD","title":"Rule configurations and calling back into AD","text":"As an alternative to rules only for one AD, would be to add new special property definitions to ChainRulesCore (as described above) which would capture what makes that AD special.","category":"page"},{"location":"FAQ.html#FAQ","page":"FAQ","title":"FAQ","text":"","category":"section"},{"location":"FAQ.html#What-is-up-with-the-different-symbols?","page":"FAQ","title":"What is up with the different symbols?","text":"","category":"section"},{"location":"FAQ.html#Δx,-x,-dx","page":"FAQ","title":"Δx, ∂x, dx","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"ChainRules uses these perhaps atypically. As a notation that is the same across propagators, regardless of direction (in contrast see ẋ and x̄ below).","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"Δx is the input to a propagator, (i.e a seed for a pullback; or a perturbation for a pushforward).\n∂x is the output of a propagator.\ndx could be either input or output.","category":"page"},{"location":"FAQ.html#dots-and-bars:-\\dot{y}-\\dfrac{y}{x}-\\overline{x}","page":"FAQ","title":"dots and bars: doty = dfracyx = overlinex","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"v̇ is a derivative of the input moving forward: v = fracvx for input x, intermediate value v.\nv̄ is a derivative of the output moving backward: v = fracyv for output y, intermediate value v.","category":"page"},{"location":"FAQ.html#others","page":"FAQ","title":"others","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"Ω is often used as the return value of the function. Especially, but not exclusively, for scalar functions.\nΔΩ is thus a seed for the pullback.\n∂Ω is thus the output of a pushforward.","category":"page"},{"location":"FAQ.html#Why-does-rrule-return-the-primal-function-evaluation?","page":"FAQ","title":"Why does rrule return the primal function evaluation?","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"You might wonder why frule(f, x) returns f(x) and the derivative of f at x, and similarly for rrule returning f(x) and the pullback for f at x. Why not just return the pushforward/pullback, and let the user call f(x) to get the answer separately?","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"There are three reasons the rules also calculate the f(x).","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"For some rules an alternative way of calculating f(x) can give the same answer while also generating intermediate values that can be used in the calculations required to propagate the derivative.\nFor many rrules the output value is used in the definition of the pullback. For example tan, sigmoid etc.\nFor some frules there exists a single, non-separable operation that will compute both derivative and primal result. For example, this is the case for many of the methods for differential equation sensitivity analysis.","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"For more information and examples see the design notes on changing the primal.","category":"page"},{"location":"FAQ.html#Where-are-the-derivatives-for-keyword-arguments?","page":"FAQ","title":"Where are the derivatives for keyword arguments?","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"Pullbacks do not return a sensitivity for keyword arguments; similarly, pushforwards do not accept a perturbation for keyword arguments. This is because in practice functions are very rarely differentiable with respect to keyword arguments.","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"As a rule, keyword arguments tend to control side-effects, like logging verbosity, or to be functionality-changing to perform a different operation, e.g. dims=3, and thus not differentiable.","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"To the best of our knowledge no Julia AD system, with support for the definition of custom primitives, supports differentiating with respect to keyword arguments. At some point in the future ChainRules may support these. Maybe.","category":"page"},{"location":"FAQ.html#faq_abstract_zero","page":"FAQ","title":"What is the difference between ZeroTangent and NoTangent ?","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"ZeroTangent and NoTangent act almost exactly the same in practice: they result in no change whenever added to anything. Odds are if you write a rule that returns the wrong one everything will just work fine. We provide both to allow for clearer writing of rules, and easier debugging.","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"ZeroTangent() represents the fact that if one perturbs (adds a small change to) the matching primal, there will be no change in the behaviour of the primal function. For example, in fst(x, y) = x, the derivative of fst with respect to y is ZeroTangent(). fst(10, 5) == 10 and if we add 0.1 to 5 we still get fst(10, 5.1) == 10.","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"NoTangent() represents the fact that if one perturbs the matching primal, the primal function will now error. For example, in access(xs, n) = xs[n], the derivative of access with respect to n is NoTangent(). access([10, 20, 30], 2) == 20, but if we add 0.1 to 2 we get access([10, 20, 30], 2.1) which errors as indexing can't be applied at fractional indexes.","category":"page"},{"location":"FAQ.html#When-to-use-ChainRules-vs-ChainRulesCore?","page":"FAQ","title":"When to use ChainRules vs ChainRulesCore?","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"ChainRulesCore.jl is a light-weight dependency for defining rules for functions in your packages, without you needing to depend on ChainRules.jl itself. It has almost no dependencies of its own. If you only want to define rules, not use them, then you probably only want to load ChainRulesCore.jl.","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"ChainRules.jl provides the full functionality for AD systems. In particular, it has all the rules for Base Julia and the standard libraries. It is thus a much heavier package to load. AD systems making use of frules and rrules should load ChainRules.jl.","category":"page"},{"location":"FAQ.html#Where-should-I-put-my-rules?","page":"FAQ","title":"Where should I put my rules?","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"We recommend adding custom rules to your own packages with ChainRulesCore.jl. It is good to have them in the same package that defines the original function. This avoids type-piracy, and makes it easy to keep in-sync. ChainRulesCore is a very light-weight dependency.","category":"page"},{"location":"FAQ.html#How-do-I-test-my-rules?","page":"FAQ","title":"How do I test my rules?","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"You can use ChainRulesTestUtils.jl to test your custom rules. ChainRulesTestUtils.jl has some dependencies, so it is a separate package from ChainRulesCore.jl. This means your package can depend on the light-weight ChainRulesCore.jl, and make ChainRulesTestUtils.jl a test-only dependency.","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"Remember to read the section On writing good rrule / frule methods.","category":"page"},{"location":"FAQ.html#Where-can-I-learn-more-about-AD-?","page":"FAQ","title":"Where can I learn more about AD ?","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"There are not so many truly excellent learning resources for autodiff out there in the world, which is a bit sad. The list here is incomplete, but is vetted for quality.","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"Automatic Differentiation for Dummies keynote video by Simon Peyton Jones: particularly good if you like pure math type thinking.\n\"What types work with differentiation? comment on DexLang GitHub issue by Dan Zheng: summarizes several years of insights from the Swift AD work.\nMIT 18337 lecture notes 8-10 (by Christopher Rackauckas and David P. Sanders  : moves fast from basic to advanced, particularly good if you like applicable mathematics\nAutomatic Differentiation and Application: Good introduction\nForward-Mode AD via High Dimensional Algebras: actually part 2 of the introduction\nSolving Stiff Ordinary Differential Equations: ignore the ODE stuff, most of this is about Sparse AutoDiff, can skip/skim this one\nBasic Parameter Estimation, Reverse-Mode AD, and Inverse Problems: use in optimization, and details connections of other math.\nDiff-Zoo Jupyter Notebook Book  (by Mike Innes, has implementations and explanations.\n\"Evaluating Derivatives\" (by Griewank and Walther) is the best book at least for reverse-mode.","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"It also covers forward-mode though (by its own admission) not as well, it never mentioned dual numbers which is an unfortunate lack.","category":"page"},{"location":"FAQ.html#Is-removing-a-thunk-a-breaking-change?","page":"FAQ","title":"Is removing a thunk a breaking change?","text":"","category":"section"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"Removing thunks is not considered a breaking change. This is because (in principle) removing them changes the implementation of the values returned by an rrule, not the value that they represent. This is morally the same as similar issues discussed in ColPrac, such as details of floating point arithmetic changing.","category":"page"},{"location":"FAQ.html","page":"FAQ","title":"FAQ","text":"On a practical level, it's important that this is the case because thunks are a bit of a hack, and over time it is hoped that the need for them will reduce, as they increase code-complexity and place additional stress on the compiler.","category":"page"},{"location":"complex.html#complexfunctions","page":"Complex Numbers","title":"How do chain rules work for complex functions?","text":"","category":"section"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"ChainRules follows the convention that frule applied to a function f(x + i y) = u(xy) + i v(xy) with perturbation Delta x + i Delta y returns the value and","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"tfracpartial upartial x  Delta x + tfracpartial upartial y  Delta y + i  Bigl( tfracpartial vpartial x  Delta x + tfracpartial vpartial y  Delta y Bigr)\n","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"Similarly, rrule applied to the same function returns the value and a pullback function which, when applied to the adjoint Delta u + i Delta v, returns","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"Delta u  tfracpartial upartial x + Delta v  tfracpartial vpartial x + i  Bigl(Delta u  tfracpartial u partial y + Delta v  tfracpartial vpartial y Bigr)\n","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"If we interpret complex numbers as vectors in mathbbR^2, then frule (rrule) corresponds to multiplication with the (transposed) Jacobian of f(z), i.e. frule corresponds to","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"beginpmatrix\ntfracpartial upartial x  Delta x + tfracpartial upartial y  Delta y\n\ntfracpartial vpartial x  Delta x + tfracpartial vpartial y  Delta y\nendpmatrix\n=\nbeginpmatrix\ntfracpartial upartial x  tfracpartial upartial y \ntfracpartial vpartial x  tfracpartial vpartial y \nendpmatrix\nbeginpmatrix\nDelta x  Delta y\nendpmatrix\n","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"and rrule corresponds to","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"beginpmatrix\ntfracpartial upartial x  Delta u + tfracpartial vpartial x  Delta v\n\ntfracpartial upartial y  Delta u + tfracpartial vpartial y  Delta v\nendpmatrix\n=\nbeginpmatrix\ntfracpartial upartial x  tfracpartial upartial y \ntfracpartial vpartial x  tfracpartial vpartial y \nendpmatrix^mathsfT\nbeginpmatrix\nDelta u  Delta v\nendpmatrix\n","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"The Jacobian of fmathbbC to mathbbC interpreted as a function mathbbR^2 to mathbbR^2 can hence be evaluated using either of the following functions.","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"function jacobian_via_frule(f,z)\n    du_dx, dv_dx = reim(frule((ZeroTangent(), 1),f,z)[2])\n    du_dy, dv_dy = reim(frule((ZeroTangent(),im),f,z)[2])\n    return [\n        du_dx  du_dy\n        dv_dx  dv_dy\n    ]\nend","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"function jacobian_via_rrule(f,z)\n    _, pullback = rrule(f,z)\n    du_dx, du_dy = reim(pullback( 1)[2])\n    dv_dx, dv_dy = reim(pullback(im)[2])\n    return [\n        du_dx  du_dy\n        dv_dx  dv_dy\n    ]\nend","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"If f(z) is holomorphic, then the derivative part of frule can be implemented as f(z)  Delta z and the derivative part of rrule can be implemented as bigl(f(z)bigr)^*  Delta f, where cdot^* is the complex conjugate. Consequently, holomorphic derivatives can be evaluated using either of the following functions.","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"function holomorphic_derivative_via_frule(f,z)\n    fz,df_dz = frule((ZeroTangent(),1),f,z)\n    return df_dz\nend","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"function holomorphic_derivative_via_rrule(f,z)\n    fz, pullback = rrule(f,z)\n    dself, conj_df_dz = pullback(1)\n    return conj(conj_df_dz)\nend","category":"page"},{"location":"complex.html","page":"Complex Numbers","title":"Complex Numbers","text":"note: Note\nThere are various notions of complex derivatives (holomorphic and Wirtinger derivatives, Jacobians, gradients, etc.) which differ in subtle but important ways. The goal of ChainRules is to provide the basic differentiation rules upon which these derivatives can be implemented, but it does not implement these derivatives itself. It is recommended that you carefully check how the above definitions of frule and rrule translate into your specific notion of complex derivative, since getting this wrong will quietly give you wrong results.","category":"page"},{"location":"api.html#API-Documentation","page":"API","title":"API Documentation","text":"","category":"section"},{"location":"api.html#Rules","page":"API","title":"Rules","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"Modules = [ChainRulesCore]\nPages = [\"rules.jl\"]\nPrivate = false","category":"page"},{"location":"api.html#ChainRulesCore.frule-Tuple{Any, Any, Vararg{Any, N} where N}","page":"API","title":"ChainRulesCore.frule","text":"frule([::RuleConfig,] (Δf, Δx...), f, x...)\n\nExpressing the output of f(x...) as Ω, return the tuple:\n\n(Ω, ΔΩ)\n\nThe second return value is the differential w.r.t. the output.\n\nIf no method matching frule((Δf, Δx...), f, x...) has been defined, then return nothing.\n\nExamples:\n\nunary input, unary output scalar function:\n\njulia> dself = NoTangent();\n\njulia> x = rand()\n0.8236475079774124\n\njulia> sinx, Δsinx = frule((dself, 1), sin, x)\n(0.7336293678134624, 0.6795498147167869)\n\njulia> sinx == sin(x)\ntrue\n\njulia> Δsinx == cos(x)\ntrue\n\nUnary input, binary output scalar function:\n\njulia> sincosx, Δsincosx = frule((dself, 1), sincos, x);\n\njulia> sincosx == sincos(x)\ntrue\n\njulia> Δsincosx[1] == cos(x)\ntrue\n\njulia> Δsincosx[2] == -sin(x)\ntrue\n\nNote that techically speaking julia does not have multiple output functions, just functions that return a single output that is iterable, like a Tuple. So this is actually a Tangent:\n\njulia> Δsincosx\nTangent{Tuple{Float64, Float64}}(0.6795498147167869, -0.7336293678134624)\n\nThe optional RuleConfig option allows specifying frules only for AD systems that support given features. If not needed, then it can be omitted and the frule without it will be hit as a fallback. This is the case for most rules.\n\nSee also: rrule, @scalar_rule, RuleConfig\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.rrule-Tuple{Any, Vararg{Any, N} where N}","page":"API","title":"ChainRulesCore.rrule","text":"rrule([::RuleConfig,] f, x...)\n\nExpressing x as the tuple (x₁, x₂, ...) and the output tuple of f(x...) as Ω, return the tuple:\n\n(Ω, (Ω̄₁, Ω̄₂, ...) -> (s̄elf, x̄₁, x̄₂, ...))\n\nWhere the second return value is the the propagation rule or pullback. It takes in differentials corresponding to the outputs (x̄₁, x̄₂, ...), and s̄elf, the internal values of the function itself (for closures)\n\nIf no method matching rrule(f, xs...) has been defined, then return nothing.\n\nExamples:\n\nunary input, unary output scalar function:\n\njulia> x = rand();\n\njulia> sinx, sin_pullback = rrule(sin, x);\n\njulia> sinx == sin(x)\ntrue\n\njulia> sin_pullback(1) == (NoTangent(), cos(x))\ntrue\n\nbinary input, unary output scalar function:\n\njulia> x, y = rand(2);\n\njulia> hypotxy, hypot_pullback = rrule(hypot, x, y);\n\njulia> hypotxy == hypot(x, y)\ntrue\n\njulia> hypot_pullback(1) == (NoTangent(), (x / hypot(x, y)), (y / hypot(x, y)))\ntrue\n\nThe optional RuleConfig option allows specifying rrules only for AD systems that support given features. If not needed, then it can be omitted and the rrule without it will be hit as a fallback. This is the case for most rules.\n\nSee also: frule, @scalar_rule, RuleConfig\n\n\n\n\n\n","category":"method"},{"location":"api.html#Rule-Definition-Tools","page":"API","title":"Rule Definition Tools","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"Modules = [ChainRulesCore]\nPages = [\"rule_definition_tools.jl\"]\nPrivate = false","category":"page"},{"location":"api.html#ChainRulesCore.@non_differentiable-Tuple{Any}","page":"API","title":"ChainRulesCore.@non_differentiable","text":"@non_differentiable(signature_expression)\n\nA helper to make it easier to declare that a method is not differentiable. This is a short-hand for defining an frule and rrule that return NoTangent() for all partials (even for the function s̄elf-partial itself)\n\nKeyword arguments should not be included.\n\njulia> @non_differentiable Base.:(==)(a, b)\n\njulia> _, pullback = rrule(==, 2.0, 3.0);\n\njulia> pullback(1.0)\n(NoTangent(), NoTangent(), NoTangent())\n\nYou can place type-constraints in the signature:\n\njulia> @non_differentiable Base.length(xs::Union{Number, Array})\n\njulia> frule((ZeroTangent(), 1), length, [2.0, 3.0])\n(2, NoTangent())\n\nwarning: Warning\nThis helper macro covers only the simple common cases. It does not support where-clauses. For these you can declare the rrule and frule directly\n\n\n\n\n\n","category":"macro"},{"location":"api.html#ChainRulesCore.@opt_out-Tuple{Any}","page":"API","title":"ChainRulesCore.@opt_out","text":"@opt_out frule([config], _, f, args...)\n@opt_out rrule([config], f, args...)\n\nThis allows you to opt-out of an frule or an rrule by providing a more specific method, that says to use the AD system to differentiate it.\n\nFor example, consider some function foo(x::AbtractArray). In general, you know an efficient and generic way to implement its rrule. You do so, (likely making use of ProjectTo). But it actually turns out that for some FancyArray type it is better to let the AD do its thing.\n\nThen you would write something like:\n\nfunction rrule(::typeof(foo), x::AbstractArray)\n    foo_pullback(ȳ) = ...\n    return foo(x), foo_pullback\nend\n\n@opt_out rrule(::typeof(foo), ::FancyArray)\n\nThis will generate an rrule that returns nothing, and will also add a similar entry to ChainRulesCore.no_rrule.\n\nSimilar applies for frule and ChainRulesCore.no_frule\n\nFor more information see the documentation on opting out of rules.\n\n\n\n\n\n","category":"macro"},{"location":"api.html#ChainRulesCore.@scalar_rule-Tuple{Any, Any, Vararg{Any, N} where N}","page":"API","title":"ChainRulesCore.@scalar_rule","text":"@scalar_rule(f(x₁, x₂, ...),\n             @setup(statement₁, statement₂, ...),\n             (∂f₁_∂x₁, ∂f₁_∂x₂, ...),\n             (∂f₂_∂x₁, ∂f₂_∂x₂, ...),\n             ...)\n\nA convenience macro that generates simple scalar forward or reverse rules using the provided partial derivatives. Specifically, generates the corresponding methods for frule and rrule:\n\nfunction ChainRulesCore.frule((NoTangent(), Δx₁, Δx₂, ...), ::typeof(f), x₁::Number, x₂::Number, ...)\n    Ω = f(x₁, x₂, ...)\n    $(statement₁, statement₂, ...)\n    return Ω, (\n            (∂f₁_∂x₁ * Δx₁ + ∂f₁_∂x₂ * Δx₂ + ...),\n            (∂f₂_∂x₁ * Δx₁ + ∂f₂_∂x₂ * Δx₂ + ...),\n            ...\n        )\nend\n\nfunction ChainRulesCore.rrule(::typeof(f), x₁::Number, x₂::Number, ...)\n    Ω = f(x₁, x₂, ...)\n    $(statement₁, statement₂, ...)\n    return Ω, ((ΔΩ₁, ΔΩ₂, ...)) -> (\n            NoTangent(),\n            ∂f₁_∂x₁ * ΔΩ₁ + ∂f₂_∂x₁ * ΔΩ₂ + ...),\n            ∂f₁_∂x₂ * ΔΩ₁ + ∂f₂_∂x₂ * ΔΩ₂ + ...),\n            ...\n        )\nend\n\nIf no type constraints in f(x₁, x₂, ...) within the call to @scalar_rule are provided, each parameter in the resulting frule/rrule definition is given a type constraint of Number. Constraints may also be explicitly be provided to override the Number constraint, e.g. f(x₁::Complex, x₂), which will constrain x₁ to Complex and x₂ to Number.\n\nAt present this does not support defining for closures/functors. Thus in reverse-mode, the first returned partial, representing the derivative with respect to the function itself, is always NoTangent(). And in forward-mode, the first input to the returned propagator is always ignored.\n\nThe result of f(x₁, x₂, ...) is automatically bound to Ω. This allows the primal result to be conveniently referenced (as Ω) within the derivative/setup expressions.\n\nThis macro assumes complex functions are holomorphic. In general, for non-holomorphic functions, the frule and rrule must be defined manually.\n\nIf the derivative is one, (e.g. for identity functions) true can be used as the most general multiplicative identity.\n\nThe @setup argument can be elided if no setup code is need. In other words:\n\n@scalar_rule(f(x₁, x₂, ...),\n             (∂f₁_∂x₁, ∂f₁_∂x₂, ...),\n             (∂f₂_∂x₁, ∂f₂_∂x₂, ...),\n             ...)\n\nis equivalent to:\n\n@scalar_rule(f(x₁, x₂, ...),\n             @setup(nothing),\n             (∂f₁_∂x₁, ∂f₁_∂x₂, ...),\n             (∂f₂_∂x₁, ∂f₂_∂x₂, ...),\n             ...)\n\nFor examples, see ChainRules' rulesets directory.\n\nSee also: frule, rrule.\n\n\n\n\n\n","category":"macro"},{"location":"api.html#Differentials","page":"API","title":"Differentials","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"Modules = [ChainRulesCore]\nPages = [\n    \"differentials/abstract_zero.jl\",\n    \"differentials/one.jl\",\n    \"differentials/composite.jl\",\n    \"differentials/thunks.jl\",\n    \"differentials/abstract_differential.jl\",\n    \"differentials/notimplemented.jl\",\n]\nPrivate = false","category":"page"},{"location":"api.html#ChainRulesCore.AbstractZero","page":"API","title":"ChainRulesCore.AbstractZero","text":"AbstractZero <: AbstractTangent\n\nSupertype for zero-like differentials—i.e., differentials that act like zero when added or multiplied to other values. If an AD system encounters a propagator that takes as input only subtypes of AbstractZero, then it can stop performing AD operations. All propagators are linear functions, and thus the final result will be zero.\n\nAll AbstractZero subtypes are singleton types. There are two of them: ZeroTangent() and NoTangent().\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.NoTangent","page":"API","title":"ChainRulesCore.NoTangent","text":"NoTangent() <: AbstractZero\n\nThis differential indicates that the derivative does not exist. It is the differential for primal types that are not differentiable, such as integers or booleans (when they are not being used to represent floating-point values). The only valid way to perturb such values is to not change them at all. As a consequence, NoTangent is functionally identical to ZeroTangent(), but it provides additional semantic information.\n\nAdding this differential to a primal is generally wrong: gradient-based methods cannot be used to optimize over discrete variables. An optimization package making use of this might want to check for such a case.\n\nnote: Note\nThis does not indicate that the derivative is not implemented, but rather that mathematically it is not defined.\n\nThis mostly shows up as the derivative with respect to dimension, index, or size arguments.\n\n    function rrule(fill, x, len::Int)\n        y = fill(x, len)\n        fill_pullback(ȳ) = (NoTangent(), @thunk(sum(Ȳ)), NoTangent())\n        return y, fill_pullback\n    end\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.ZeroTangent","page":"API","title":"ChainRulesCore.ZeroTangent","text":"ZeroTangent() <: AbstractZero\n\nThe additive identity for differentials. This is basically the same as 0. A derivative of ZeroTangent() does not propagate through the primal function.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.Tangent","page":"API","title":"ChainRulesCore.Tangent","text":"Tangent{P, T} <: AbstractTangent\n\nThis type represents the differential for a struct/NamedTuple, or Tuple. P is the the corresponding primal type that this is a differential for.\n\nTangent{P} should have fields (technically properties), that match to a subset of the fields of the primal type; and each should be a differential type matching to the primal type of that field. Fields of the P that are not present in the Tangent are treated as Zero.\n\nT is an implementation detail representing the backing data structure. For Tuple it will be a Tuple, and for everything else it will be a NamedTuple. It should not be passed in by user.\n\nFor Tangents of Tuples, iterate and getindex are overloaded to behave similarly to for a tuple. For Tangents of structs, getproperty is overloaded to allow for accessing values via comp.fieldname. Any fields not explictly present in the Tangent are treated as being set to ZeroTangent(). To make a Tangent have all the fields of the primal the canonicalize function is provided.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.canonicalize-Union{Tuple{Tangent{P, var\"#s3\"} where var\"#s3\"<:(NamedTuple{L, T} where T<:Tuple)}, Tuple{L}, Tuple{P}} where {P, L}","page":"API","title":"ChainRulesCore.canonicalize","text":"canonicalize(comp::Tangent{P}) -> Tangent{P}\n\nReturn the canonical Tangent for the primal type P. The property names of the returned Tangent match the field names of the primal, and all fields of P not present in the input comp are explictly set to ZeroTangent().\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.InplaceableThunk","page":"API","title":"ChainRulesCore.InplaceableThunk","text":"InplaceableThunk(add!::Function, val::Thunk)\n\nA wrapper for a Thunk, that allows it to define an inplace add! function.\n\nadd! should be defined such that: ithunk.add!(Δ) = Δ .+= ithunk.val but it should do this more efficently than simply doing this directly. (Otherwise one can just use a normal Thunk).\n\nMost operations on an InplaceableThunk treat it just like a normal Thunk; and destroy its inplacability.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.Thunk","page":"API","title":"ChainRulesCore.Thunk","text":"Thunk(()->v)\n\nA thunk is a deferred computation. It wraps a zero argument closure that when invoked returns a differential. @thunk(v) is a macro that expands into Thunk(()->v).\n\nTo evaluate the wrapped closure, call unthunk which is a no-op when the argument is not a Thunk.\n\njulia> t = @thunk(3)\nThunk(var\"#4#5\"())\n\njulia> unthunk(t)\n3\n\nWhen to @thunk?\n\nWhen writing rrules (and to a lesser exent frules), it is important to @thunk appropriately. Propagation rules that return multiple derivatives may not have all deriviatives used.  By @thunking the work required for each derivative, they then compute only what is needed.\n\nHow do thunks prevent work?\n\nIf we have res = pullback(...) = @thunk(f(x)), @thunk(g(x)) then if we did dx + res[1] then only f(x) would be evaluated, not g(x). Also if we did ZeroTangent() * res[1] then the result would be ZeroTangent() and f(x) would not be evaluated.\n\nSo why not thunk everything?\n\n@thunk creates a closure over the expression, which (effectively) creates a struct with a field for each variable used in the expression, and call overloaded.\n\nDo not use @thunk if this would be equal or more work than actually evaluating the expression itself. This is commonly the case for scalar operators.\n\nFor more details see the manual section on using thunks effectively\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.unthunk-Tuple{Any}","page":"API","title":"ChainRulesCore.unthunk","text":"unthunk(x)\n\nOn AbstractThunks this removes 1 layer of thunking. On any other type, it is the identity operation.\n\n\n\n\n\n","category":"method"},{"location":"api.html#ChainRulesCore.@thunk-Tuple{Any}","page":"API","title":"ChainRulesCore.@thunk","text":"@thunk expr\n\nDefine a Thunk wrapping the expr, to lazily defer its evaluation.\n\n\n\n\n\n","category":"macro"},{"location":"api.html#ChainRulesCore.@not_implemented-Tuple{Any}","page":"API","title":"ChainRulesCore.@not_implemented","text":"@not_implemented(info)\n\nCreate a differential that indicates that the derivative is not implemented.\n\nThe info should be useful information about the missing differential for debugging.\n\nnote: Note\nThis macro should be used only if the automatic differentiation would error otherwise. It is mostly useful if the function has multiple inputs or outputs, and one has worked out analytically and implemented some but not all differentials.\n\nnote: Note\nIt is good practice to include a link to a GitHub issue about the missing differential in the debugging information.\n\n\n\n\n\n","category":"macro"},{"location":"api.html#Accumulation","page":"API","title":"Accumulation","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"add!!\nChainRulesCore.is_inplaceable_destination","category":"page"},{"location":"api.html#ChainRulesCore.add!!","page":"API","title":"ChainRulesCore.add!!","text":"add!!(x, y)\n\nReturns x+y, potentially mutating x in-place to hold this value. This avoids allocations when x can be mutated in this way.\n\n\n\n\n\nadd!!(x, t::InplacableThunk)\n\nThe specialization of add!! for InplaceableThunk promises to only call t.add! on x if x is suitably mutable; otherwise it will be out of place.\n\n\n\n\n\n","category":"function"},{"location":"api.html#ChainRulesCore.is_inplaceable_destination","page":"API","title":"ChainRulesCore.is_inplaceable_destination","text":"is_inplaceable_destination(x) -> Bool\n\nReturns true if x is suitable for for storing inplace accumulation of gradients. For arrays this boils down x .= y if will work to mutate x, if y is an appropriate differential. Wrapper array types do not need to overload this if they overload Base.parent, and are is_inplaceable_destination if and only if their parent array is. Other types should overload this, as it defaults to false.\n\n\n\n\n\n","category":"function"},{"location":"api.html#RuleConfig","page":"API","title":"RuleConfig","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"Modules = [ChainRulesCore]\nPages = [\"config.jl\"]\nPrivate = false","category":"page"},{"location":"api.html#ChainRulesCore.HasForwardsMode","page":"API","title":"ChainRulesCore.HasForwardsMode","text":"HasForwardsMode\n\nThis trait indicates that a RuleConfig{>:HasForwardsMode} can perform forward mode AD. If it is set then frule_via_ad must be implemented.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.HasReverseMode","page":"API","title":"ChainRulesCore.HasReverseMode","text":"HasReverseMode\n\nThis trait indicates that a RuleConfig{>:HasReverseMode} can perform reverse mode AD. If it is set then rrule_via_ad must be implemented.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.NoForwardsMode","page":"API","title":"ChainRulesCore.NoForwardsMode","text":"NoForwardsMode\n\nThis is the complement to HasForwardsMode. To avoid ambiguities [RuleConfig]s that do not support performing forwards mode AD should be RuleConfig{>:NoForwardsMode}.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.NoReverseMode","page":"API","title":"ChainRulesCore.NoReverseMode","text":"NoReverseMode\n\nThis is the complement to HasReverseMode. To avoid ambiguities [RuleConfig]s that do not support performing reverse mode AD should be RuleConfig{>:NoReverseMode}.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.RuleConfig","page":"API","title":"ChainRulesCore.RuleConfig","text":"RuleConfig{T}\n\nThe configuration for what rules to use. T: traits. This should be a Union of all special traits needed for rules to be allowed to be defined for your AD. If nothing special this should be set to Union{}.\n\nAD authors should define a subtype of RuleConfig to use when calling frule/rrule.\n\nRule authors can dispatch on this config when defining rules. For example:\n\n# only define rrule for `pop!` on AD systems where mutation is supported.\nrrule(::RuleConfig{>:SupportsMutation}, typeof(pop!), ::Vector) = ...\n\n# this definition of map is for any AD that defines a forwards mode\nrrule(conf::RuleConfig{>:HasForwardsMode}, typeof(map), ::Vector) = ...\n\n# this definition of map is for any AD that only defines a reverse mode.\n# It is not as good as the rrule that can be used if the AD defines a forward-mode as well.\nrrule(conf::RuleConfig{>:Union{NoForwardsMode, HasReverseMode}}, typeof(map), ::Vector) = ...\n\nFor more details see rule configurations and calling back into AD.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.frule_via_ad","page":"API","title":"ChainRulesCore.frule_via_ad","text":"frule_via_ad(::RuleConfig{>:HasForwardsMode}, ȧrgs, f, args...; kwargs...)\n\nThis function has the same API as frule, but operates via performing forwards mode automatic differentiation. Any RuleConfig subtype that supports the HasForwardsMode special feature must provide an implementation of it.\n\nSee also: rrule_via_ad, RuleConfig and the documentation on rule configurations and calling back into AD\n\n\n\n\n\n","category":"function"},{"location":"api.html#ChainRulesCore.rrule_via_ad","page":"API","title":"ChainRulesCore.rrule_via_ad","text":"rrule_via_ad(::RuleConfig{>:HasReverseMode}, f, args...; kwargs...)\n\nThis function has the same API as rrule, but operates via performing reverse mode automatic differentiation. Any RuleConfig subtype that supports the HasReverseMode special feature must provide an implementation of it.\n\nSee also: frule_via_ad, RuleConfig and the documentation on rule configurations and calling back into AD\n\n\n\n\n\n","category":"function"},{"location":"api.html#ProjectTo","page":"API","title":"ProjectTo","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"ProjectTo","category":"page"},{"location":"api.html#ChainRulesCore.ProjectTo","page":"API","title":"ChainRulesCore.ProjectTo","text":"(p::ProjectTo{T})(dx)\n\nProjects the differential dx onto a specific tangent space.\n\nThe type T is meant to encode the largest acceptable space, so usually this enforces p(dx)::T. But some subspaces which aren't subtypes of T may be allowed, and in particular dx::AbstractZero always passes through.\n\nUsually T is the \"outermost\" part of the type, and p stores additional properties such as projectors for each constituent field. Arrays have either one projector p.element expressing the element type for an array of numbers, or else an array of projectors p.elements. These properties can be supplied as keyword arguments on construction, p = ProjectTo{T}(; field=data, element=Projector(x)). For each T in use, corresponding methods should be written for ProjectTo{T}(dx) with nonzero dx.\n\nWhen called on dx::Thunk, the projection is inserted into the thunk.\n\n\n\n\n\n","category":"type"},{"location":"api.html#Internal","page":"API","title":"Internal","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"ChainRulesCore.AbstractTangent\nChainRulesCore.debug_mode\nChainRulesCore.no_rrule\nChainRulesCore.no_frule","category":"page"},{"location":"api.html#ChainRulesCore.AbstractTangent","page":"API","title":"ChainRulesCore.AbstractTangent","text":"The subtypes of AbstractTangent define a custom \"algebra\" for chain rule evaluation that attempts to factor various features like complex derivative support, broadcast fusion, zero-elision, etc. into nicely separated parts.\n\nIn general a differential type is the type of a derivative of a value. The type of the value is for contrast called the primal type. Differential types correspond to primal types, although the relation is not one-to-one. Subtypes of  AbstractTangent are not the only differential types. In fact for the most common primal types, such as Real or AbstractArray{Real} the the differential type is the same as the primal type.\n\nIn a circular definition: the most important property of a differential is that it should be able to be added (by defining +) to another differential of the same primal type. That allows for gradients to be accumulated.\n\nIt generally also should be able to be added to a primal to give back another primal, as this facilitates gradient descent.\n\nAll subtypes of AbstractTangent implement the following operations:\n\n+(a, b): linearly combine differential a and differential b\n*(a, b): multiply the differential b by the scaling factor a\nBase.zero(x) = ZeroTangent(): a zero.\n\nFurther, they often implement other linear operators, such as conj, adjoint, dot. Pullbacks/pushforwards are linear operators, and their inputs are often AbstractTangent subtypes. Pullbacks/pushforwards in-turn call other linear operators on those inputs. Thus it is desirable to have all common linear operators work on AbstractTangents.\n\n\n\n\n\n","category":"type"},{"location":"api.html#ChainRulesCore.debug_mode","page":"API","title":"ChainRulesCore.debug_mode","text":"debug_mode() -> Bool\n\nDetermines if ChainRulesCore is in debug_mode. Defaults to false, but if the user redefines it to return true then extra information will be shown when errors occur.\n\nEnable via:\n\nChainRulesCore.debug_mode() = true\n\n\n\n\n\n","category":"function"},{"location":"api.html#ChainRulesCore.no_rrule","page":"API","title":"ChainRulesCore.no_rrule","text":"no_rrule\n\nThis is an piece of infastructure supporting opting out of rrule. It follows the signature for rrule exactly. A collection of type-tuples is stored in its method-table. If something has this defined, it means that it must having a must also have a rrule,  defined that returns nothing.\n\n!!! warning \"do not overload norrule directly     It is fine and intended to query the method table of `norrule.     It is not safe to add to that directly, as corresponding changes also need to be made torrule.     The [@optout](@ref) macro does both these things, and so should almost always be used     rather than defining a method ofnorrule` directly.\n\nMechanics\n\nnote: when the text below says methods == it actually means: parameters(m.sig)[2:end] (i.e. the signature type tuple) rather than the method object m itself.\n\nTo decide if should opt-out using this mechanism.\n\nfind the most specific method of rrule and no_rule e.g with Base.which\nif the method of no_rrule == the method of rrule, then should opt-out\n\nTo just ignore the fact that rules can be opted-out from, and that some rules thus return nothing, then filter the list of methods of rrule to remove those that are == to ones that occur in the method table of no_rrule.\n\nNote also when doing this you must still also handle falling back from rule with config, to rule without config.\n\nOn the other-hand if your AD can work with rrules that return nothing, then it is simpler to just use that mechanism for opting out; and you don't need to worry about this at all.\n\nFor more information see the documentation on opting out of rules\n\nSee also ChainRulesCore.no_frule.\n\n\n\n\n\n","category":"function"},{"location":"api.html#ChainRulesCore.no_frule","page":"API","title":"ChainRulesCore.no_frule","text":"no_frule\n\nThis is an piece of infastructure supporting opting out of frule. It follows the signature for frule exactly. A collection of type-tuples is stored in its method-table. If something has this defined, it means that it must having a must also have a frule,  defined that returns nothing.\n\n!!! warning \"do not overload nofrule directly     It is fine and intended to query the method table of `nofrule.     It is not safe to add to that directly, as corresponding changes also need to be made tofrule.     The [@optout](@ref) macro does both these things, and so should almost always be used     rather than defining a method ofnofrule` directly.\n\nMechanics\n\nnote: when the text below says methods == it actually means: parameters(m.sig)[2:end] (i.e. the signature type tuple) rather than the method object m itself.\n\nTo decide if should opt-out using this mechanism.\n\nfind the most specific method of frule and no_rule e.g with Base.which\nif the method of no_frule == the method of frule, then should opt-out\n\nTo just ignore the fact that rules can be opted-out from, and that some rules thus return nothing, then filter the list of methods of frule to remove those that are == to ones that occur in the method table of no_frule.\n\nNote also when doing this you must still also handle falling back from rule with config, to rule without config.\n\nOn the other-hand if your AD can work with frules that return nothing, then it is simpler to just use that mechanism for opting out; and you don't need to worry about this at all.\n\nFor more information see the documentation on opting out of rules\n\nSee also ChainRulesCore.no_rrule.\n\n\n\n\n\n","category":"function"},{"location":"converting_zygoterules.html#Converting-ZygoteRules.@adjoint-to-rrules","page":"Converting ZygoteRules","title":"Converting ZygoteRules.@adjoint to rrules","text":"","category":"section"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"ZygoteRules.jl is a legacy package similar to ChainRulesCore but supporting Zygote.jl only.","category":"page"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"If you have some rules written with ZygoteRules it is a good idea to upgrade them to use ChainRules instead. Zygote will still be able to use them, but so will other AD systems, and you will get access to some more advanced features. Some of these features are currently ignored by Zygote, but could be supported in the future.","category":"page"},{"location":"converting_zygoterules.html#Example","page":"Converting ZygoteRules","title":"Example","text":"","category":"section"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"Consider the function","category":"page"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"struct Foo\n    a::Float64,\n    b::Float64\nend\n\nf(x, y::Foo, z) = 2*x + y.a","category":"page"},{"location":"converting_zygoterules.html#ZygoteRules","page":"Converting ZygoteRules","title":"ZygoteRules","text":"","category":"section"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"@adjoint function f(x, y::Foo, z)\n    f_pullback(Ω̄) = (2Ω̄, NamedTuple(;a=Ω̄, b=nothing), nothing)\n    return f(x, y, z), f_pullback\nend","category":"page"},{"location":"converting_zygoterules.html#ChainRules","page":"Converting ZygoteRules","title":"ChainRules","text":"","category":"section"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"function rrule(::typeof(f), x, y::Foo, z)\n    f_pullback(Ω̄) = (NoTangent(), 2Ω̄, Tangent{Foo}(;a=Ω̄), ZeroTangent())\n    return f(x, y, z), f_pullback\nend","category":"page"},{"location":"converting_zygoterules.html#Write-as-a-rrule(::typeof(f),-...)","page":"Converting ZygoteRules","title":"Write as a rrule(::typeof(f), ...)","text":"","category":"section"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"No magic macro here, rrule is the function that it is. The function it is the rule for is the first argument, or second argument if you need to take a RuleConfig.","category":"page"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"Note that when writing the rule for constructor you will need to use ::Type{Foo}, not typeof(Foo). See docs on Constructors.","category":"page"},{"location":"converting_zygoterules.html#Include-the-derivative-with-respect-to-the-function-object-itself","page":"Converting ZygoteRules","title":"Include the derivative with respect to the function object itself","text":"","category":"section"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"The ZygoteRules.@adjoint macro automagically[1] inserts an extra nothing in the return for the function it generates to represent the derivative of output with respect to the function object. ChainRules as a philosophy avoids magic as much as possible, and thus require you to return it explicitly. If it is a plain function (like typeof(sin)), then the differential will be NoTangent.","category":"page"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"[1]: unless you write it in functor form (i.e. @adjoint (f::MyType)(args...)=...), in that case like for rrule you need to include it explictly.","category":"page"},{"location":"converting_zygoterules.html#Tangent-Type-changes","page":"Converting ZygoteRules","title":"Tangent Type changes","text":"","category":"section"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"ChainRules uses tangent types that must represent vector spaces (i.e. tangent spaces). They need to have things like + defined on them. ZygoteRules takes a more adhoc approach to this.","category":"page"},{"location":"converting_zygoterules.html#nothing-becomes-an-AbstractZero","page":"Converting ZygoteRules","title":"nothing becomes an AbstractZero","text":"","category":"section"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"ZygoteRules uses nothing to represent some sense of zero, in a primal type agnostic way. There are many senses of zero. ChainRules represents two of them, as subtypes of AbstractZero.","category":"page"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"ZeroTangent for the case that there is no relationship between the primal output and the primal input. NoTangent for the case where conceptually the tangent space doesn't exist. e.g. what is the Tangent to a String or an index: those can't be perturbed.","category":"page"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"See FAQ on the difference between ZeroTangent and NoTangent. At the end of the day it doesn't matter too much if you get them wrong. NoTangent and ZeroTangent more or less act identically.","category":"page"},{"location":"converting_zygoterules.html#Tuples-and-NamedTuples-become-Tangent{T}s","page":"Converting ZygoteRules","title":"Tuples and NamedTuples become Tangent{T}s","text":"","category":"section"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"Zygote uses Tuples and NamedTuples to represent the structural tangents for Tuples and structs respectively. ChainRules core provides a generic Tangent{T} to represent the structural tangent of a primal type T. It takes positional arguments if representing tangent for a Tuple. Or keyword argument to represent the tangent for a struct or a NamedTuple. When representing a struct you only need to list the nonzero fields – any not given are implicit considered to be ZeroTangent.","category":"page"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"When we say structural tangent we mean tangent types that are based only on the structure of the primal. This is in contrast to a natural tangent which captures some knowledge based on what the primal type represents. (E.g. for arrays a natural tangent is often the same kind of array). For more details see the the design docs on the many tangent types","category":"page"},{"location":"converting_zygoterules.html#Calling-back-into-AD-(ZygoteRules.pullback)","page":"Converting ZygoteRules","title":"Calling back into AD (ZygoteRules.pullback)","text":"","category":"section"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"Rules that need to call back into the AD system, e.g, for higher order functions like map(f, xs), need to be changed. In ZygoteRules you can use ZygoteRules.pullback or ZygoteRules._pullback, which will always result in calling into Zygote. Since ChainRules is AD agnostic, you can't do that. Instead you use a RuleConfig to specify requirements of an AD system e.g ::RuleConfig{>:HasReverseMode} work for Zygote, and then use rrule_via_ad.","category":"page"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"See the docs on calling back into AD for more details.","category":"page"},{"location":"converting_zygoterules.html#Consider-adding-some-thunks","page":"Converting ZygoteRules","title":"Consider adding some thunks","text":"","category":"section"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"A feature ChainRulesCore offers that ZygoteRules doesn't is support for thunks. Thunks delay work until it is needed, and avoid it if it never is. See docs on @thunk, Thunk, InplaceableThunk.","category":"page"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"You don't have to use thunks, though. It is easy to go overboard with using thunks.","category":"page"},{"location":"converting_zygoterules.html#Testing-Changes","page":"Converting ZygoteRules","title":"Testing Changes","text":"","category":"section"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"One of the advantages of using ChainRules is that you can easily and robustly test your rules with ChainRulesTestUtils.jl. This uses finite differencing to test the accuracy of derivative, as well as checks the correctness of the API. It should catch anything you might have gotten wrong referred to in this page.","category":"page"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"The test for the above example is test_rrule(f, 2.5, Foo(9.9, 7.2), 31.0). You can see it looks a lot like an example call to rrule, just with the prefix test_ added to the start.","category":"page"},{"location":"converting_zygoterules.html#@nograd-becomes-@non_differentiable","page":"Converting ZygoteRules","title":"@nograd becomes @non_differentiable","text":"","category":"section"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"Probably more or less with no changes. @non_differentiable also lets you specify a signature in case you want to restrict non-differentiability to a certain subset of argument types.","category":"page"},{"location":"converting_zygoterules.html#No-such-thing-a-literal_getproperty","page":"Converting ZygoteRules","title":"No such thing a literal_getproperty","text":"","category":"section"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"That is just getproperty, it takes Symbol. It should constant-fold. It likely doesn't though as Zygote doesn't play nice with the optimizer.","category":"page"},{"location":"converting_zygoterules.html#Take-embedded-spaces-and-types-seriously","page":"Converting ZygoteRules","title":"Take embedded spaces and types seriously","text":"","category":"section"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"Traditionally Zygote has taken a very laissez-faire attitude towards types and mathematical spaces. Sometimes treating Reals as embedded in the Complex plane; sometimes not. Sometimes treating sparse and structuredly-sparse matrix as embedded in the space of dense matrices. Writing rules that apply to any Array{T} which perhaps are only applicable for Array{<:Real} and not so much for Array{Quaternion}. Traditionally ChainRules takes a much more considered approach.","category":"page"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"See for example our docs on how to handle complex numbers correctly. (The outcome of several long long long discussions with a number of experts in our community)","category":"page"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"Now, I am not here to tell you what to do in your package, but this is a good time to reconsider how seriously you take these things in the rules you are converting.","category":"page"},{"location":"converting_zygoterules.html#What-if-I-miss-something","page":"Converting ZygoteRules","title":"What if I miss something","text":"","category":"section"},{"location":"converting_zygoterules.html","page":"Converting ZygoteRules","title":"Converting ZygoteRules","text":"It is not great, but it probably OK. Zygote's ChainRules interface is fairly forgiving. Other AD systems may not be. If you test with ChainRulesTestUtils.jl then you can be confident that you didn't miss anything.","category":"page"},{"location":"use_in_ad_system.html#Using-ChainRules-in-your-AD-system","page":"Usage in AD","title":"Using ChainRules in your AD system","text":"","category":"section"},{"location":"use_in_ad_system.html","page":"Usage in AD","title":"Usage in AD","text":"This section is for authors of AD systems. It assumes a pretty solid understanding of both Julia and automatic differentiation. It explains how to make use of ChainRule's \"rulesets\" (frules, rrules,) to avoid having to code all your own AD primitives / custom sensitives.","category":"page"},{"location":"use_in_ad_system.html","page":"Usage in AD","title":"Usage in AD","text":"There are 3 main ways to access ChainRules rule sets in your AutoDiff system.","category":"page"},{"location":"use_in_ad_system.html","page":"Usage in AD","title":"Usage in AD","text":"Operator Overloading Generation\nUse ChainRulesOverloadGeneration.jl.\nThis is primarily intended for operator overloading based AD systems which will generate overloads for primal functions based for their overloaded types based on the existence of an rrule/frule.\nA source code generation based AD can also use this by overloading their transform generating function directly so as not to recursively generate a transform but to just return the rule.\nThis does not play nice with Revise.jl, adding or modifying rules in loaded files will not be reflected until a manual refresh, and deleting rules will not be reflected at all.\nSource code tranform based on inserting branches that check of rrule/frule return nothing\nIf the rrule/frule returns a rule result then use it, if it returns nothing then do normal AD path.\nIn theory type inference optimizes these branchs out; in practice it may not.\nThis is a fairly simple Cassette overdub (or similar) of all calls, and is suitable for overloading based AD or source code transformation.\nSource code transform based on rrule/frule method-table\nIf an applicable rrule/frule exists in the method table then use it, else generate normal AD path.\nThis avoids having branches in your generated code.\nThis requires maintaining your own back-edges.\nThis is pretty hardcore even by the standard of source code tranformations.","category":"page"},{"location":"arrays.html#Deriving-Array-Rules","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"One of the goals of the ChainRules interface is to make it easy to define your own rules for a function. This tutorial attempts to demystify deriving and implementing custom rules for arrays with real and complex entries, with examples. The approach we use is similar to the one succinctly explained and demonstrated in [Giles2008] and its extended work [Giles2008ext], but we generalize it to support functions of multidimensional arrays with both real and complex entries.","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Throughout this tutorial, we will use the following type alias:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"const RealOrComplex = Union{Real,Complex}","category":"page"},{"location":"arrays.html#Forward-mode-rules","page":"Deriving Array Rules","title":"Forward-mode rules","text":"","category":"section"},{"location":"arrays.html#Approach","page":"Deriving Array Rules","title":"Approach","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Consider a function","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Ω = f(X::Array{<:RealOrComplex}...)::Array{<:RealOrComplex}","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"or in math notation","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"f (ldots X_m ldots) mapsto Omega","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"where the components of X_m are written as (X_m)_ildotsj. The variables X_m and Omega are intermediates in a larger program (function) that, by considering only a single real input t and real output s can always be written as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"t mapsto (ldots X_m ldots) mapsto Omega mapsto s","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"where t and s are real numbers. If we know the partial derivatives of X_m with respect to t, fracdX_mdt = dotX_m, the chain rule gives the pushforward of f as:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginequation labelpf\ndotOmega\n    = f_*(ldots dotX_m ldots)\n    = sum_m sum_i ldots j\n        fracpartial Omega partial (X_m)_ildotsj  (dotX_m)_ildotsj\nendequation","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"That's ugly, but in practice we can often write it more simply by using forward mode rules for simpler functions, as we'll see below. The forward-mode rules for arrays follow directly from the usual scalar chain rules.","category":"page"},{"location":"arrays.html#Array-addition","page":"Deriving Array Rules","title":"Array addition","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Ω = A + B","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"This one is easy:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Omega = A + B","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"dotOmega = dotA + dotB","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We can implement the frule in ChainRules's notation:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function frule(\n    (_, ΔA, ΔB),\n    ::typeof(+),\n    A::Array{<:RealOrComplex},\n    B::Array{<:RealOrComplex},\n)\n    Ω = A + B\n    ∂Ω = ΔA + ΔB\n    return (Ω, ∂Ω)\nend","category":"page"},{"location":"arrays.html#Matrix-multiplication","page":"Deriving Array Rules","title":"Matrix multiplication","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Ω = A * B","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Omega = A B","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"First we write in component form:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Omega_ij = sum_k A_ik B_kj","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Then we use the product rule to get the pushforward for each scalar entry:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\ndotOmega_ij\n    = sum_k left( dotA_ik B_kj + A_ik dotB_kj right)\n         textapply scalar product rule \n            fracddt(x y) = fracdxdt y + x fracdydt \n    = sum_k dotA_ik B_kj + sum_k A_ik dotB_kj\n         textsplit sum\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"But the last expression is just the component form of a sum of matrix products:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginequationlabeldiffprod\ndotOmega = dotA B + A dotB\nendequation","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"This is the matrix product rule, and we write its frule as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function frule(\n    (_, ΔA, ΔB),\n    ::typeof(*),\n    A::Matrix{<:RealOrComplex},\n    B::Matrix{<:RealOrComplex},\n)\n    Ω = A * B\n    ∂Ω = ΔA * B + A * ΔB\n    return (Ω, ∂Ω)\nend","category":"page"},{"location":"arrays.html#Matrix-inversion","page":"Deriving Array Rules","title":"Matrix inversion","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Ω = inv(A)","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Omega = A^-1","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"It's easiest to derive this rule from either of the two constraints:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\nOmega A = A^-1 A = I\nA Omega = A A^-1 = I\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"where I is the identity matrix.","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We use the matrix product rule to differentiate the first constraint:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"dotOmega A + Omega dotA = 0","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Then, right-multiply both sides by A^-1 to isolate dotOmega:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign\n0  = dotOmega A A^-1 + Omega dotA A^-1 nonumber\n   = dotOmega I + Omega dotA A^-1\n        textuse  A A^-1 = I nonumber\n   = dotOmega + Omega dotA Omega\n        textsubstitute  A^-1 = Omega nonumber\ndotOmega\n   = -Omega dotA Omega\n        textsolve for  dotOmega labelinvdiff\nendalign","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We write the frule as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function frule((_, ΔA), ::typeof(inv), A::Matrix{<:RealOrComplex})\n    Ω = inv(A)\n    ∂Ω = -Ω * ΔA * Ω\n    return (Ω, ∂Ω)\nend","category":"page"},{"location":"arrays.html#Other-useful-identities","page":"Deriving Array Rules","title":"Other useful identities","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"These identities are particularly useful:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\nfracddt left( Re(A) right) = Re(dotA)\nfracddt left( A^* right) = dotA^*\nfracddt left( A^mathsfT right) = dotA^mathsfT\nfracddt left( A^mathsfH right) = dotA^mathsfH\nfracddt left( sum_j  A_i ldots j ldots k right) =\n    sum_j dotA_i ldots j ldots k\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"where cdot^* is the complex conjugate (conj), and cdot^mathsfH = left(cdot^mathsfTright)^* is the conjugate transpose (the adjoint function).","category":"page"},{"location":"arrays.html#Reverse-mode-rules","page":"Deriving Array Rules","title":"Reverse-mode rules","text":"","category":"section"},{"location":"arrays.html#Approach-2","page":"Deriving Array Rules","title":"Approach","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Reverse-mode rules are a little less intuitive, but we can re-use our pushforwards to simplify their derivation. Recall our program:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"t mapsto (ldots X_m ldots) mapsto Omega mapsto s","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"At any step in the program, if we have intermediates X_m, we can write down the derivative fracdsdt in terms of the tangents dotX_m = fracdX_mdt and adjoints overlineX_m = fracpartial spartial X_m","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\nfracdsdt\n    = sum_m Releft( sum_ildotsj\n           left( fracpartial spartial (X_m)_ildotsj right)^*\n           fracd (X_m)_ildotsjdt\n       right)\n    = sum_m Releft( sum_ildotsj\n           (overlineX_m)_ildotsj^*\n           (dotX_m)_ildotsj\n       right)\n    = sum_m Reip overlineX_m  dotX_m \nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"where Re(cdot) is the real part of a number (real), and ipcdotcdot is the Frobenius inner product (LinearAlgebra.dot). Because this equation follows at any step of the program, we can equivalently write ","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"fracdsdt = Reip overlineOmega  dotOmega ","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"which gives the identity","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginequation labelpbident\nReip overlineOmega  dotOmega  = sum_m Reip overlineX_m  dotX_m \nendequation","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"For matrices and vectors, ipAB = tr(A^mathsfH B), and the identity simplifies to:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginequation labelpbidentmat\nReleft( trleft(\n    overlineOmega^mathsfH dotOmega\nright) right) =\nsum_m Re left( tr left(\n    overlineX_m^mathsfH dotX_m\nright) right)\nendequation","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"where tr(cdot) is the matrix trace (LinearAlgebra.tr) function. However, it is often cleaner and more general to work with the inner product.","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Our approach for deriving the adjoints overlineX_m is then:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Derive the pushforward (dotOmega in terms of dotX_m) using \\eqref{pf}.\nSubstitute this expression for dotOmega into the left-hand side of \\eqref{pbident}.\nManipulate until it looks like the right-hand side of \\eqref{pbident}.\nSolve for each overlineX_m.","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Note that the final expressions for the adjoints will not contain any dotX_m terms.","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"note: Note\nWhy do we conjugate, and why do we only use the real part of the dot product in \\eqref{pbident}? Recall from Complex Numbers that we treat a complex number as a pair of real numbers. These identities are a direct consequence of this convention. Consider fracdsdt for a scalar function f (x + i y) mapsto (u + i v):beginalign*\nfracdsdt\n    = Reip overlinex + i overliney  dotx + i doty  \n    = Releft(\n           left( overlinex + i overliney right)^*\n           left( dotx + i doty right)\n       right) \n    = Releft(\n           left( overlinex - i overliney right)\n           left( dotx + i doty right)\n       right) \n    = Releft(\n           left( overlinex dotx + overliney doty right) +\n           i left( overlinex doty - overliney dotx right)\n       right)\n    = overlinex dotx + overliney doty\nendalign*which is exactly what the identity would produce if we had written the function as f (x y) mapsto (u v).","category":"page"},{"location":"arrays.html#Useful-properties-of-the-inner-product","page":"Deriving Array Rules","title":"Useful properties of the inner product","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Several properties of the Frobenius inner product come in handy. First, it is linear in its second argument and conjugate linear in its first. That is, for arrays A B C D and scalars a and b,","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign\nipA+BC+D = ipAC + ipBC + ipAD + ipBD labeliplinear\nipaAbB = a^* b ipAB nonumber\nendalign","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Second, swapping arguments is equivalent to conjugating the inner product:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginequation\nipAB = ipBA^* labelipconj\nendequation","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Third, for matrices and vectors A, B, and C, we can move arguments from the left or right of one side to the other using the matrix adjoint:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginequation\nipABCD = ipB^mathsfH ACD = ipB^mathsfH A D^mathsfHC labelipperm\nendequation","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Fourth, the inner product of two arrays A and B is equivalent to the sum of the elementwise inner products of the two arrays:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginequation\nipAB = sum_ildotsk ipA_ildotskB_ildotsk = sum_ildotsk A_ildotsk^* B_ildotsk\nendequation","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"As a result, only elements that are nonzero on both sides contribute to the inner product. This property is especially useful when deriving rules involving structurally sparse arrays.","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Now let's derive a few pullbacks using this approach.","category":"page"},{"location":"arrays.html#Matrix-multiplication-2","page":"Deriving Array Rules","title":"Matrix multiplication","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Ω = A * B","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We above derived in \\eqref{diffprod} the pushforward","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"dotOmega = dotA B + A dotB","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Using \\eqref{pbidentmat}, we now multiply by overlineOmega^mathsfH and take the real trace:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\nReipoverlineOmegadotOmega\n    = Re ipoverline OmegadotA B + A dotB\n            textsubstitute  dotOmega text from  eqrefdiffprod\n    = Re ipoverline OmegadotA B + Re ipoverline OmegaA dotB\n            textexpand using  eqrefiplinear \n    = Re ipoverline Omega B^mathsfHdotA + Re ipA^mathsfH overline OmegadotB\n            textrearrange the left term using  eqrefipperm\n    = Re ipoverline AdotA + Re ipoverline BdotB\n            textright-hand side of  eqrefpbidentmat\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"That's it! The expression is in the desired form to solve for the adjoints by comparing the last two lines:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"overline A = overline Omega B^mathsfH qquad overline B = A^mathsfH overline Omega","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Using ChainRules's notation, we would implement the rrule as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function rrule(::typeof(*), A::Matrix{<:RealOrComplex}, B::Matrix{<:RealOrComplex})\n    function times_pullback(ΔΩ)\n        ∂A = @thunk(ΔΩ * B')\n        ∂B = @thunk(A' * ΔΩ)\n        return (NoTangent(), ∂A, ∂B)\n    end\n    return A * B, times_pullback\nend","category":"page"},{"location":"arrays.html#Matrix-inversion-2","page":"Deriving Array Rules","title":"Matrix inversion","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Ω = inv(A)","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"In \\eqref{invdiff}, we derived the pushforward as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"dotOmega = -Omega dotA Omega","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Using \\eqref{pbidentmat},","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\nReipoverlineOmegadotOmega\n    = ReipoverlineOmega-Omega dotA Omega\n            textsubstitute  eqrefinvdiff\n    = Reip-Omega^mathsfH overlineOmega Omega^mathsfHdotA\n            textrearrange using  eqrefipperm\n    = ReipoverlineAdotA\n            textright-hand side of  eqrefpbidentmat\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"we can now solve for overlineA:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"overlineA = -Omega^mathsfH overlineOmega Omega^mathsfH","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We can implement the resulting rrule as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function rrule(::typeof(inv), A::Matrix{<:RealOrComplex})\n    Ω = inv(A)\n    function inv_pullback(ΔΩ)\n        ∂A = -Ω' * ΔΩ * Ω'\n        return (NoTangent(), ∂A)\n    end\n    return Ω, inv_pullback\nend","category":"page"},{"location":"arrays.html#A-multidimensional-array-example","page":"Deriving Array Rules","title":"A multidimensional array example","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We presented the approach for deriving pushforwards and pullbacks for arrays of arbitrary dimensions, so let's cover an example. For multidimensional arrays, it's often easier to work in component form. Consider the following function:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Ω = sum(abs2, X::Array{<:RealOrComplex,3}; dims=2)::Array{<:Real,3}","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"which we write as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Omega_i1k = sum_j X_ijk^2\n             = sum_j Re ipX_ijkX_ijk","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"The pushforward from \\eqref{pf} is","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign\ndotOmega_i1k\n    = sum_j ReipdotX_ijkX_ijk + ipX_ijkdotX_ijk nonumber\n    = sum_j ReipX_ijkdotX_ijk^* + ipX_ijkdotX_ijk nonumber\n    = sum_j 2 ReipX_ijkdotX_ijk labelsumabspf\nendalign","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"where in the last step we have used the fact that for all real a and b,","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"(a + i b) + (a + i b)^*\n    = (a + i b) + (a - i b)\n    = 2 a\n    = 2 Re (a + i b)","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Because none of this derivation depended on the index (or indices), we implement frule generically as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function frule(\n    (_, _, ΔX),\n    ::typeof(sum),\n    ::typeof(abs2),\n    X::Array{<:RealOrComplex};\n    dims = :,\n)\n    Ω = sum(abs2, X; dims = dims)\n    ∂Ω = sum(2 .* real.(conj.(X) .* ΔX); dims = dims)\n    return (Ω, ∂Ω)\nend","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We can now derive the reverse-mode rule. The elementwise form of \\eqref{pbident} is","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\nReip overlineOmega  dotOmega \n    = Re left( sum_ik overlineOmega_i1k^*\n           dotOmega_i1k right)\n            textexpand left-hand side of  eqrefpbident\n    = Re left(sum_ijk overlineOmega_i1k^*\n           2 Releft( X_ijk^* dotX_ijk right)\n       right)\n            textsubstitute  eqrefsumabspf\n    = Re left( sum_ijk\n           left(\n               2 Re left( overlineOmega_i1k right)\n               X_ijk^*\n           right) dotX_ijk\n       right)\n            textbring  dotX_ijk text outside of  Re\n    = sum_ijk Reip2 Re left( overlineOmega_i1k right) X_ijkdotX_ijk\n            textrewrite as an inner product\n    = sum_ijk ReipoverlineX_ijkdotX_i1k\n            textright-hand side of  eqrefpbident\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We now solve for overlineX:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"overlineX_ijk = 2Re left( overlineOmega_i1k right) X_ijk","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Like the frule, this rrule can be implemented generically:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function rrule(::typeof(sum), ::typeof(abs2), X::Array{<:RealOrComplex}; dims = :)\n    function sum_abs2_pullback(ΔΩ)\n        ∂abs2 = NoTangent()\n        ∂X = @thunk(2 .* real.(ΔΩ) .* X)\n        return (NoTangent(), ∂abs2, ∂X)\n    end\n    return sum(abs2, X; dims = dims), sum_abs2_pullback\nend","category":"page"},{"location":"arrays.html#Functions-that-return-a-tuple","page":"Deriving Array Rules","title":"Functions that return a tuple","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Every Julia function returns a single output. For example, let's look at LinearAlgebra.logabsdet, the logarithm of the absolute value of the determinant of a matrix, which returns log det(A) and operatornamesign(det A) = fracdet A det A :","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"(l, s) = logabsdet(A)","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"The return type is actually a single output, a tuple of scalars, but when deriving, we treat them as multiple outputs. The left-hand side of \\eqref{pbident} then becomes a sum over terms, just like the right-hand side.","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Let's derive the forward- and reverse-mode rules for logabsdet.","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\nl = log det(A)\ns = operatornamesign(det(A))\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"where operatornamesign(x) = fracxx.","category":"page"},{"location":"arrays.html#Forward-mode-rule","page":"Deriving Array Rules","title":"Forward-mode rule","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"To make this easier, let's break the computation into more manageable steps:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\nd = det(A)\na = d = sqrtRe left( d^* d right)\nl = log a\ns = fracda\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We'll make frequent use of the identities:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"d = a s","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"s^* s = fracd^* da^2 = fraca^2a^2 = 1","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"It will also be useful to define b = trleft( A^-1 dotA right).","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"For dotd, we use the pushforward for the determinant given in section 2.2.4 of [Giles2008ext]:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"dotd = d b","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Now we'll compute the pushforwards for the remaining steps.","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\ndota = frac12 a fracddt\n                         Releft( d^* d right)\n        = frac22 a Re left( d^* dotd right)\n        = Re left( s^* dotd right)\n             textuse  d = a s \n        = Re left( s^* d b right)\n             textsubstitute  dotd \ndotl = a^-1 dota\n        = a^-1 Re left( s^* d b right)\n             textsubstitute  dota\n        = Re left( s^* s b right)\n             textuse  d = a s \n        = Re left(b right)\n             textuse  s^* s = 1\ndots = a^-1 dotd - a^-2 d dota\n        = a^-1 left( dotd - dota s right)\n             textuse  d = a s \n        = a^-1 left(\n               dotd - Re left( s^* dotd right) s\n           right)\n             textsubstitute  dota\n        = a^-1 left(\n               dotd - left(\n                   s^* dotd -\n                   i Im left( s^* dotd right)\n               right) s\n           right)\n             textuse  Re(x) = x - i Im(x)\n        = a^-1 left(\n               dotd - left( s^* s right) dotd +\n               i Im left( s^* dotd right) s \n               right)\n        = i a^-1 Im left( s^* dotd right) s\n             textuse  s^* s = 1\n        = i a^-1 Im left( s^* d b right) s\n             textsubstitute  dotd\n        = i Im left( s^* s b right) s\n             textuse  d = a s \n        = i Im(b) s\n             textuse  s^* s = 1\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Note that the term b is reused. In summary, after all of that work, the final pushforward is quite simple:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign\nb = tr left( A^-1 dotA right) labellogabsdet_b \ndotl = Re(b) labellogabsdet_ldot\ndots = i Im(b) s labellogabsdet_sdot\nendalign","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We can define the frule as:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function frule((_, ΔA), ::typeof(logabsdet), A::Matrix{<:RealOrComplex})\n    # The primal function uses the lu decomposition to compute logabsdet\n    # we reuse this decomposition to compute inv(A) * ΔA\n    F = lu(A, check = false)\n    Ω = logabsdet(F)  # == logabsdet(A)\n    b = tr(F \\ ΔA)  # == tr(inv(A) * ΔA)\n    s = last(Ω)\n    ∂l = real(b)\n    # for real A, ∂s will always be zero (because imag(b) = 0)\n    # this is type-stable because the eltype is known\n    ∂s = eltype(A) <: Real ? ZeroTangent() : im * imag(b) * s\n    # tangents of tuples are of type Tangent{<:Tuple}\n    ∂Ω = Tangent{typeof(Ω)}(∂l, ∂s)\n    return (Ω, ∂Ω)\nend","category":"page"},{"location":"arrays.html#Reverse-mode-rule","page":"Deriving Array Rules","title":"Reverse-mode rule","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\nReipoverlineldotl + Reipoverlinesdots\n     textleft-hand side of  eqrefpbidentmat\n= Releft( overlinel^* dotl + overlines^* dots right) \n= Releft( \n       overlinel^* Re(b) + i overlines^* s Im(b)\n   right)\n        textsubstitute  eqreflogabsdet_ldot text and  eqreflogabsdet_sdot \n= Releft( \n       Releft( overlinel right) Re(b) -\n       Im left( overlines^* s right) Im(b)\n   right)\n        textdiscard imaginary parts \n= Releft(\n       left(\n           Re left( overlinel right) +\n           i Im left( overlines^* s right)\n       right) b\n   right)\n        textgather parts of  b \n= Releft(\n       left(\n           Re left( overlinel right) +\n           i Im left( overlines^* s right)\n       right)\n       tr(A^-1 dotA)\n   right)\n        textsubstitute  b text from  eqreflogabsdet_b \n= Releft( tr left(\n       left(\n           Re left( overlinel right) +\n           i Im left( overlines^* s right)\n       right)\n       A^-1 dotA\n   right) right)\n        textbring scalar within  tr \n= Reip\n        left(\n            Re left( overlinel right) + i Im left( s^* overlines right)\n        right) A^-mathsfH\n    dotA  textrewrite as inner product\n= ReipoverlineAdotA  textright-hand side of  eqrefpbidentmat\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Now we solve for overlineA:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\noverlineA = left(\n    Re left( overlinel right) +\n    i Im left( s^* overlines right)\nright) A^-mathsfH\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"The rrule can be implemented as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function rrule(::typeof(logabsdet), A::Matrix{<:RealOrComplex})\n    # The primal function uses the lu decomposition to compute logabsdet\n    # we reuse this decomposition to compute inv(A)\n    F = lu(A, check = false)\n    Ω = logabsdet(F)  # == logabsdet(A)\n    s = last(Ω)\n    function logabsdet_pullback(ΔΩ)\n        (Δl, Δs) = ΔΩ\n        f = conj(s) * Δs\n        imagf = f - real(f)  # 0 for real A and Δs, im * imag(f) for complex A and/or Δs\n        g = real(Δl) + imagf\n        ∂A = g * inv(F)'  # == g * inv(A)'\n        return (NoTangent(), ∂A)\n    end\n    return (Ω, logabsdet_pullback)\nend","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"note: Note\nIt's a good idea when deriving pushforwards and pullbacks to verify that they make sense. For the pushforward, since l is real, it follows that dotl is too.What about dots? Well, s = fracdd is point on the unit circle in the complex plane. Multiplying a complex number by i rotates it counter-clockwise by 90°. So the expression for dots takes a real number, Im(b), multiplies by s to make it parallel to s, then multiplies by i to make it perpendicular to s, that is, perfectly tangent to the unit complex circle at s.For the pullback, it again follows that only the real part of overlinel is pulled back.s^* rotates a number parallel to s to the real line. So s^* overlines rotates overlines so that its imaginary part is the part that was tangent to the complex circle at s, while the real part is the part that was not tangent. Then the pullback isolates the imaginary part, which effectively is a projection. That is, any part of the adjoint overlines that is not tangent to the complex circle at s will not contribute to overlineA.","category":"page"},{"location":"arrays.html#Implicit-functions","page":"Deriving Array Rules","title":"Implicit functions","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Sometimes a function is only defined implicitly, and internally some solver or iterative algorithm is used to compute the result. We can still in some cases derive rules by considering only the implicit functions and not the internals. One example is the solution X to the Sylvester equation","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"A X + X B = -C","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"for inputs A, B, and C. We can also write this solution as X = operatornamesylvester(A B C), which in Julia is computed using LinearAlgebra.sylvester(A, B, C).","category":"page"},{"location":"arrays.html#Forward-mode-Rule","page":"Deriving Array Rules","title":"Forward-mode Rule","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"We start by differentiating the implicit function:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"dotA X + A dotX + dotX B + X dotB = -dotC","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Then we isolate the terms with dotX on one side:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign\nA dotX + dotX B\n    = -dotC - dotA X - X dotB labelsylpfimplicit\n    = -(dotC + dotA X + X dotB) nonumber\nendalign","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"So the pushforward is the solution to a different Sylvester equation:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"dotX = operatornamesylvester(A B dotC + dotA X + X dotB)","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"The frule can be implemented as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function frule((_, ΔA, ΔB, ΔC), ::typeof(sylvester), A, B, C)\n    X = sylvester(A, B, C)\n    return X, sylvester(A, B, ΔC + ΔA * X + X * ΔB)\nend","category":"page"},{"location":"arrays.html#Reverse-mode-Rule","page":"Deriving Array Rules","title":"Reverse-mode Rule","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Like with the pushforward, it's easiest to work with the implicit function. We start by introducing some dummy -Z and taking its inner product with both sides of \\eqref{sylpfimplicit}:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"ip-ZA dotX + dotX B = ip-Z-dotC - dotA X - X dotB","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Then we expand","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"ip-ZA dotX + ip-ZdotX B = ipZdotC + ipZdotA X + ipZX dotB","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Now permute:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"ip-A^mathsfH ZdotX + ip-Z B^mathsfHdotX = ipZdotC + ipZ X^mathsfHdotA + ipX^mathsfH ZX dotB","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Then combine:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"ip-(A^mathsfH Z + Z B^mathsfH)dotX = ipZ X^mathsfHdotA + ipX^mathsfH ZX dotB + ipZdotC","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"This is almost exactly the identity we need to solve for overlineA, overlineB, and overlineC. To manipulate it to the right form, we need only define A^mathsfH Z + Z B^mathsfH = -overlineX. This yet another Sylvester equation, so letting Z = overlineC, our final pullback is:","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"beginalign*\noverlineC = operatornamesylvester(A^mathsfH B^mathsfH overlineX)\n             = operatornamesylvester(B A overlineX^mathsfH)^mathsfH\noverlineA = overlineC X^mathsfH\noverlineB = X^mathsfH overlineC\nendalign*","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"The rrule can be implemented as","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"function rrule(::typeof(sylvester), A, B, C)\n    X = sylvester(A, B, C)\n    function sylvester_pullback(ΔX)\n        ∂C = copy(sylvester(B, A, copy(ΔX'))')\n        return NoTangent(), @thunk(∂C * X'), @thunk(X' * ∂C), ∂C\n    end\n    return X, sylvester_pullback\nend","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"Note, however, that the Sylvester equation is usually solved using the Schur decomposition of A and B. These Schur decompositions can be reused to solve the Sylvester equations in the pushforward and pullback. See the implementation in ChainRules for details.","category":"page"},{"location":"arrays.html#More-examples","page":"Deriving Array Rules","title":"More examples","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"For more instructive examples of array rules, see [Giles2008ext] (real vector and matrix rules) and the LinearAlgebra rules in ChainRules. For differentiating the LU decomposition, see this blog post by Seth Axen.","category":"page"},{"location":"arrays.html#References","page":"Deriving Array Rules","title":"References","text":"","category":"section"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"[Giles2008]: Giles M. B. Collected Matrix Derivative Results for Forward and Reverse Mode Algorithmic Differentiation. In: Advances in Automatic Differentiation. Lecture Notes in Computational Science and Engineering, vol 64: pp 35-44. Springer, Berlin (2008). doi: 10.1007/978-3-540-68942-3_4. pdf","category":"page"},{"location":"arrays.html","page":"Deriving Array Rules","title":"Deriving Array Rules","text":"[Giles2008ext]: Giles M. B. An Extended Collection of Matrix Derivative Results for Forward and Reverse Mode Algorithmic Differentiation. (unpublished). pdf","category":"page"},{"location":"gradient_accumulation.html#Gradient-Accumulation","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"","category":"section"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"Consider some function f(x) = g(x) + h(x). If we would like the derivative of f with respect to x we must compute it for each part and then sum them, i.e. fracpartial fpartial x = fracpartial gpartial x + fracpartial hpartial x. In general, we must accumulate (sum) gradients from each sub-part of a program where a variable is used.","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"Consider for example:","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"function sum_first_and_second(X::Array{Float64})\n    a = X[1]\n    b = X[2]\n    y = a + b\n    return y\nend","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"The AD software must transform that into something which repeatedly sums up the gradient of each part: X̄ = ā + b̄.","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"This requires that all differential types D must implement +: +(::D, ::D)::D.","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"We can note that in this particular case ā and b̄ will both be arrays. This operation (X̄ = ā + b̄) will allocate one array to hold ā, another one to hold b̄, and a third one to hold ā + b̄. This is three allocations. Allocations are not free, they increase the time the program takes to run by a nontrivial amount, even with a good allocator and a good garbage collector.","category":"page"},{"location":"gradient_accumulation.html#Maybe-mutating-accumulation-(add!!)","page":"Gradient Accumulation","title":"Maybe-mutating accumulation (add!!)","text":"","category":"section"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"We can note that in the above that neither ā nor b̄ are ever used again after accumulating to get X̄. Furthermore, Arrays are mutable. That means we could over-write either ā or b̄ and use the result as X̄:","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"ā .+= b̄\nX̄ = ā","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"This cuts our allocations down to 2, just ā and b̄.","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"However, we have a bit of a problem that not all types are mutable, so this pattern is hard to apply in general. To deal with that ChainRulesCore provides add!!. Per the BangBang.jl convention, this is a maybe mutating addition. It may mutate its first argument (if it is mutable), but it will definitely return the correct result. We would write using that as X̄ = add!!(ā, b̄): which would in this case give us just 2 allocations. AD systems can generate add!! instead of + when accumulating gradient to take advantage of this.","category":"page"},{"location":"gradient_accumulation.html#Inplaceable-Thunks-(InplaceableThunks)-avoid-allocating-values-in-the-first-place.","page":"Gradient Accumulation","title":"Inplaceable Thunks (InplaceableThunks) avoid allocating values in the first place.","text":"","category":"section"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"We got down to two allocations from using add!!, but can we do better? We can think of having a differential type which acts on a partially accumulated result, to mutate it to contain its current value plus the partial derivative being accumulated. Rather than having an actual computed value, we can just have a thing that will act on a value to perform the addition. Let's illustrate it with our example.","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"b̄ is the partial for X[2] and its value can be computed by:","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"b̄ = zeros(size(X))\nb̄[2] = ȳ  # the scalar sensitivity of the `sum_first_and_second` output","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"b̄ is a matrix entirely of zeros, except for at the index 2, where it is set to the output sensitivity ȳ. ā is similar, except with the non-zero at index 1.","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"What is the action of b̄ upon ā, to get the same result as X̄ = add!!(ā, b̄) (or X̄ = ā + b̄ for that matter)? It is:","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"function b̄_add!(ā)\n    ā[2] += ȳ\n    return ā\nend","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"We don't need to worry about all those zeros since x + 0 == x.","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"InplaceableThunk is the type we have to represent derivatives as gradient accumulating actions. We must note that to do this we do need a value form of ā for b̄ to act upon. For this reason every inplaceable thunk has both a val field holding the value representation, and a add! field holding the action representation. The val field use a plain Thunk to avoid the computation (and thus allocation) if it is unused.","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"note: Do we need both representations?\nRight now every InplaceableThunk has two fields that need to be specified. The value form (represented as a the Thunk typed field), and the action form (represented as the add! field). It is possible in a future version of ChainRulesCore.jl we will work out a clever way to find the zero differential for arbitrary primal values. Given that, we could always just determine the value form from inplaceable.add!(zero_differential(primal)). There are some technical difficulties in finding the zero differentials, but this may be solved at some point.","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"The + operation on InplaceableThunks is overloaded to unthunk that val field to get the value form. Where as the add!! operation is overloaded to call add! to invoke the action.","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"With getindex defined to return an InplaceableThunk, we now get to X̄ = add!!(ā, b̄) requires only a single allocation. This allocation occurs when unthunking ā, which is then mutated to become X̄. This is basically as good as we can get: if we want X̄ to be an Array then at some point we need to allocate that array.","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"note: Can we do more? Deferred accumulation\nWe could keep going further to drop allocations if we really wanted. If we didn't care about X̄ being an Array then we could defer its computation too. X̄ = @thunk add!!(ā, b̄). This kind of deferral will work fine and you can keep chaining it. It does start to burn stack space, and might make the compiler's optimization passes cry. But it's valid and should work fine.","category":"page"},{"location":"gradient_accumulation.html#Examples-of-InplaceableThunks","page":"Gradient Accumulation","title":"Examples of InplaceableThunks","text":"","category":"section"},{"location":"gradient_accumulation.html#getindex","page":"Gradient Accumulation","title":"getindex","text":"","category":"section"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"The aforementioned getindex is really the poster child for this. Consider something like:","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"function mysum(X::Array{Float64})\n    total = 0.0\n    for i in eachindex(X)\n        total += X[i]\n    end\n    return total\nend","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"If one only has value representation of derivatives one ends up having to allocate a derivative array for every single element of the original array X. That's terrible. On the other hand, with the action representation that InplaceableThunks provide, there is just a single Array allocated. One can see the getindex rule in ChainRules.jl for the implementation.","category":"page"},{"location":"gradient_accumulation.html#matmul-etc-(*)","page":"Gradient Accumulation","title":"matmul etc (*)","text":"","category":"section"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"Multiplication of scalars/vectors/matrices of compatible dimensions can all also have their derivatives represented as an InplaceableThunk. These tend to pivot around that add! action being defined along the lines of: X̄ -> mul!(X̄, A', Ȳ, true, true). Where 5-arg mul! is the in place multiply-add operation. mul!(X̄, A', Ȳ, true, true) has the same effect as (X̄ .+= A'*Ȳ) but avoids allocating  the matrix  A'*Ȳ This is one of the fundamental operations provided by BLAS – including the application of the conjugate transpose. e.g. the Matrix-Matrix form is GEMM (GEneralized Matrix-Matrix Multiplication), the Matrix-Vector form is GEMV (GEneralized Matrix-Vector Multiplication) etc. Under the hood doing it out of place is going to call one of these methods anyway, but on a freshly allocated output array. So we are going to hit a very efficient implementation and get the addition for free.","category":"page"},{"location":"gradient_accumulation.html","page":"Gradient Accumulation","title":"Gradient Accumulation","text":"One can see the * rules in ChainRules.jl for the implementations","category":"page"},{"location":"design/changing_the_primal.html#change_primal","page":"Changing the Primal","title":"Design Notes: Why can you change the primal computation?","text":"","category":"section"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"These design notes are to help you understand ChainRules.jl's rrule function. It explains why we have a rrule function that returns both the primal result (i.e. the output for the forward pass) and the pullback as a closure. It might be surprising to some AD authors, who might expect just a function that performs the pullback, that the rrule function computes the primal result as well as the pullback. In particularly, rrule allows you to change how the primal result is computed. We will illustrate in this document why being able to change the computation of the primal is crucial for efficient AD.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"note: What about `frule`?\nDiscussion here is focused on on reverse mode and rrule. Similar concerns do apply to forward mode and frule. In forward mode these concerns lead to the fusing of the pushforward into frule. All the examples given here also apply in forward mode. In fact in forward mode there are even more opportunities to take advantage of sharing work between the primal and derivative computations. A particularly notable example is in efficiently calculating the pushforward of solving a differential equation via expanding the system of equations to also include the derivatives before solving it.","category":"page"},{"location":"design/changing_the_primal.html#The-Journey-to-rrule","page":"Changing the Primal","title":"The Journey to rrule","text":"","category":"section"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"Let's imagine a different system for rules, one that doesn't let you define the computation of the primal. This system is what a lot of AD systems have. It is what Nabla.jl had originally.[1] We will have a primal (i.e. forward) pass that directly executes the primal function and just records the primal function, its inputs and its output onto the tape.[2]. Then during the gradient (i.e. reverse) pass it has a function which receives those records from the tape along with the sensitivity of the output, and gives back the sensitivity of the input. We will call this function pullback_at, as it pulls back the sensitivity at a given primal point. To make this concrete:","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"y = f(x)  # primal program\nx̄ = pullback_at(f, x, y, ȳ)","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"Let's illustrate this with examples for sin and for the logistic sigmoid.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"<details open><summary>Example for `sin`</summary>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"y = sin(x)\npullback_at(::typeof(sin), x, y, ȳ) = ȳ * cos(x)","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"pullback_at uses the primal input x, and the sensitivity being pulled back ȳ.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"</details>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"<details open><summary>Example for the logistic sigmoid</summary>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"σ(x) = 1/(1 + exp(-x))  # = exp(x) / (1 + exp(x))\ny = σ(x)\npullback_at(::typeof(σ), x, y, ȳ) = ȳ * y * σ(-x)  # = ȳ * σ(x) * σ(-x)","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"Notice that in pullback_at we are not only using input x but also using the primal output y . This is a nice bit of symmetry that shows up around exp.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"</details>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"Now let's consider why we implement rrules in the first place. One key reason is to insert domain knowledge so as to compute the derivative more efficiently than AD would just by breaking everything down into +, *, etc.[3] What insights do we have about sin and cos? What about using sincos?","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"<details open><summary>Example for `sin`</summary>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"julia> using BenchmarkTools\n\njulia> @btime sin(x) setup=(x=rand());\n  3.838 ns (0 allocations: 0 bytes)\n\njulia> @btime cos(x) setup=(x=rand());\n  4.795 ns (0 allocations: 0 bytes)\n\njulia> 3.838 + 4.795\n8.633","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"vs computing both together:","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"julia> @btime sincos(x) setup=(x=rand());\n  6.028 ns (0 allocations: 0 bytes)","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"</details>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"What about the logistic sigmoid? We note that the two values we need are σ(x) and σ(-x) If we write these as: sigma(x) = frace^x1+e^x and sigma(-x) = frac11+e^x then we see they have the common term e^x. exp(x) is a much more expensive operation than + and /. So we can save time, if we can reuse that exp(x).","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"<details open><summary>Example for the logistic sigmoid</summary>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"If we have to computing separately:","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"julia> @btime 1/(1+exp(x)) setup=(x=rand());\n  5.622 ns (0 allocations: 0 bytes)\n\njulia> @btime 1/(1+exp(-x)) setup=(x=rand());\n  6.036 ns (0 allocations: 0 bytes)\n\njulia> 5.622 + 6.036\n11.658","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"vs reusing exp(x):","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"julia> @btime exp(x) setup=(x=rand());\n  5.367 ns (0 allocations: 0 bytes)\n\njulia> @btime ex/(1+ex) setup=(ex=exp(rand()));\n  1.255 ns (0 allocations: 0 bytes)\n\njulia> @btime 1/(1+ex) setup=(ex=exp(rand()));\n  1.256 ns (0 allocations: 0 bytes)\n\njulia> 5.367 + 1.255 + 1.256\n7.878","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"</details>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"So we are talking about a 30-40% speed-up from these optimizations.[4]","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"It is faster to  compute sin and cos at the same time via sincos than it is to compute them one after the other. And it is faster to reuse the exp(x) in computing σ(x) and σ(-x). How can we incorporate this insight into our system? We know we can compute both of these in the primal — because they only depend on x and not on ȳ — but there is nowhere to put them that is accessible both to the primal pass and the gradient pass code.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"What if we introduced some variable called intermediates that is also recorded onto the tape during the primal pass? We would need to be able to modify the primal pass to do this, so that we can actually put the data into the intermediates. So we will introduce a function: augmented_primal, that will return the primal output plus the intermediates that we want to reuse in the gradient pass. Then we will make our AD system replace calls to the primal with calls to the augmented_primal of the primal function and take care of all the bookkeeping. So that would look like:","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"y = f(x)  # primal program\ny, intermediates = augmented_primal(f, x)\nx̄ = pullback_at(f, x, y, ȳ, intermediates)","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"<details open><summary>Example for `sin`</summary>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"function augmented_primal(::typeof(sin), x)\n  y, cx = sincos(x)\n  return y, (; cx=cx)  # use a NamedTuple for the intermediates\nend\n\npullback_at(::typeof(sin), x, y, ȳ, intermediates) = ȳ * intermediates.cx","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"</details>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"<details open><summary>Example for the logistic sigmoid</summary>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"function augmented_primal(::typeof(σ), x)\n  ex = exp(x)\n  y = ex / (1 + ex)\n  return y, (; ex=ex)  # use a NamedTuple for the intermediates\nend\n\npullback_at(::typeof(σ), x, y, ȳ, intermediates) = ȳ * y / (1 + intermediates.ex)","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"</details>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"Cool! That lets us do what we wanted. We net decreased the time it takes to run the primal and gradient passes. We have now demonstrated the title question of why we want to be able to modify the primal pass. We will go into that more later and have some more usage examples, but first let's continue to see how we go from augmented_primal and pullback_at to rrule.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"One thing we notice when looking at pullback_at is it really is starting to have a lot of arguments. It had a fair few already, and now we are adding intermediates as well, making it even more unwieldy. Not to mention these are fairly simple example, the sin and σ functions have 1 input and no keyword arguments. Furthermore, we often don't even use all of the arguments to pullback_at. The new code for pulling back sin — which uses sincos and intermediates — no longer needs x, and it never needed y (though sigmoid σ does). And storing all these things on the tape — inputs, outputs, sensitivities, intermediates — is using up extra memory. What if we generalized the idea of the intermediate named tuple, and had augmented_primal return a struct that just held anything we might want put on the tape.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"struct PullbackMemory{P, S}\n  primal_function::P\n  state::S\nend\n# convenience constructor:\nPullbackMemory(primal_function; state...) = PullbackMemory(primal_function, state)\n# convenience accessor so that `m.x` is same as `m.state.x`\nBase.getproperty(m::PullbackMemory, propname) = getproperty(getfield(m, :state), propname)","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"So changing our API we have:","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"y = f(x)  # primal program\ny, pb = augmented_primal(f, x)\nx̄ = pullback_at(pb, ȳ)","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"which is much cleaner.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"<details open><summary>Example for `sin`</summary>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"function augmented_primal(::typeof(sin), x)\n  y, cx = sincos(x)\n  return y, PullbackMemory(sin; cx=cx)\nend\n\npullback_at(pb::PullbackMemory{typeof(sin)}, ȳ) = ȳ * pb.cx","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"</details>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"<details open><summary>Example for the logistic sigmoid</summary>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"function augmented_primal(::typeof(σ), x)\n  ex = exp(x)\n  y = ex / (1 + ex)\n  return y, PullbackMemory(σ; y=y, ex=ex)\nend\n\npullback_at(pb::PullbackMemory{typeof(σ)}, ȳ) = ȳ * pb.y / (1 + pb.ex)","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"</details>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"That now looks much simpler; pullback_at only ever has 2 arguments.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"One way we could make it nicer to use is by making PullbackMemory a callable object. Conceptually the PullbackMemory is a fixed thing it the contents of the tape for a particular operation. It is fully determined by the end of the primal pass. The during the gradient (reverse) pass the PullbackMemory is used to successively compute the ȳ  argument. So it makes sense to make PullbackMemory a callable object that acts on the sensitivity. We can do that via call overloading:","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"y = f(x)  # primal program\ny, pb = augmented_primal(f, x)\nx̄ = pb(ȳ)","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"<details open><summary>Example for `sin`</summary>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"function augmented_primal(::typeof(sin), x)\n  y, cx = sincos(x)\n  return y, PullbackMemory(sin; cx=cx)\nend\n(pb::PullbackMemory{typeof(sin)})(ȳ) = ȳ * pb.cx","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"</details>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"<details open><summary>Example for the logistic sigmoid</summary>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"function augmented_primal(::typeof(σ), x)\n  ex = exp(x)\n  y = ex / (1 + ex)\n  return y, PullbackMemory(σ; y=y, ex=ex)\nend\n\n(pb::PullbackMemory{typeof(σ)})(ȳ) = ȳ * pb.y / (1 + pb.ex)","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"</details>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"Let's recap what we have done here. We now have an object pb that acts on the cotangent of the output of the primal ȳ to give us the cotangent of the input of the primal function x̄. pb is not just the memory of state required for the pullback, it is the pullback.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"We have one final thing to do, which is to think about how we make the code easy to modify. Let's go back and think about the changes we would have make to go from our original way of writing that only used the inputs/outputs, to one that used the intermediate state.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"<details open><summary>Example for `sin`</summary>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"To rewrite that original formulation in the new pullback form we have:","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"function augmented_primal(::typeof(sin), x)\n  y = sin(x)\n  return y, PullbackMemory(sin; x=x)\nend\n(pb::PullbackMemory)(ȳ) = ȳ * cos(pb.x)","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"To go from that to:","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"function augmented_primal(::typeof(sin), x)\n  y, cx = sincos(x)\n  return y, PullbackMemory(sin; cx=cx)\nend\n(pb::PullbackMemory)(ȳ) = ȳ * pb.cx","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"</details>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"<details open><summary>Example for the logistic sigmoid</summary>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"function augmented_primal(::typeof(σ), x)\n  y = σ(x)\n  return y, PullbackMemory(σ; y=y, x=x)\nend\n(pb::PullbackMemory{typeof(σ)})(ȳ) = ȳ * pb.y * σ(-pb.x)","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"to get to:","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"function augmented_primal(::typeof(σ), x)\n  ex = exp(x)\n  y = ex/(1 + ex)\n  return y, PullbackMemory(σ; y=y, ex=ex)\nend\n(pb::PullbackMemory{typeof(σ)})(ȳ) = ȳ * pb.y/(1 + pb.ex)","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"</details>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"We should think about how we might want to make future changes to this code.[6]","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"We need to make a series of changes:","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"update what work is done in the primal, to compute the intermediate values.\nupdate what is stored in the PullbackMemory.\nupdate the function that applies the pullback so it uses the new thing that was stored.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"It's important these parts all stay in sync. It's not too bad for this simple example with just one or two things to remember. For more complicated multi-argument functions, which we will show below, you often end up needing to remember half a dozen things, like sizes and indices relating to each input/output, so it gets a little more fiddly to make sure you remember all the things you need to and give them the same name in both places. Is there a way we can automatically just have all the things we use remembered for us?  Surprisingly for such a specific request, there actually is: a closure.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"A closure in Julia is a callable structure that automatically contains a field for every object from its parent scope that is used in its body. There are incredible ways to abuse this; but here we can use closures exactly as they are intended. Replacing PullbackMemory with a closure that works the same way lets us avoid having to manually control what is remembered and lets us avoid separately writing the call overload.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"<details open><summary>Example for `sin`</summary>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"function augmented_primal(::typeof(sin), x)\n  y, cx = sincos(x)\n  pb = ȳ -> cx * ȳ  # pullback closure. closes over `cx`\n  return y, pb\nend","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"</details>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"<details open><summary>Example for the logistic sigmoid</summary>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"function augmented_primal(::typeof(σ), x)\n  ex = exp(x)\n  y = ex / (1 + ex)\n  pb = ȳ -> ȳ * y / (1 + ex)  # pullback closure. closes over `y` and `ex`\n  return y, pb\nend","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"</details>","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"This is pretty clean now.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"Our augmented_primal is now within spitting distance of rrule. All that is left is a rename and some extra conventions around multiple outputs and gradients with respect to callable objects.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"This has been a journey into how we get to rrule as it is defined in ChainRulesCore. We started with an unaugmented primal function and a pullback_at function that only saw the inputs and outputs of the primal. We realized a key limitation of this was that we couldn't share computational work between the primal and gradient passes. To solve this we introduced the notation of some intermediate that is shared from the primal to the pullback. We successively improved that idea, first by making it a type that held everything that is needed for the pullback: the PullbackMemory, which we then made callable, so it was itself the pullback. Finally, we replaced that separate callable structure with a closure, which kept everything in one place and made it more convenient.","category":"page"},{"location":"design/changing_the_primal.html#More-Shared-Work-Examples","page":"Changing the Primal","title":"More Shared Work Examples","text":"","category":"section"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"sin and the logistic sigmoid are nice, simple examples of when it is useful to share work between the primal and the pullback. There are many others though. It is actually surprising that in so many cases it is reasonable to write the rules where the only shared information between the primal and the pullback is the primal inputs (like our original sin), or primal outputs (like our original logistic sigmoid). Under our formulation above, those primal inputs/outputs are shared information just like any other. Beyond this, there are a number of other decent applications.","category":"page"},{"location":"design/changing_the_primal.html#getindex","page":"Changing the Primal","title":"getindex","text":"","category":"section"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"In Julia (and many other numerical languages) indexing can take many more arguments than simply a couple of integers, such as boolean masking arrays (logical indexing), ranges for slices, etc. Converting the arguments to plain integers, arrays of integers, and ranges with Base.to_indices is the first thing that getindex does. It then re-calls getindex with these simpler types to get the result.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"The result of pulling back the getindex operation is always an array that is all zeros, except for the elements that are selected, which are set to the appropriate sensitivities being pulled back. To identify which actual positions in the array are being gotten/set is common work to both primal and gradient computations. We really don't want to deal with fancy indexing types during the pullback, because there are weird edge cases like indexing in such a way that the same element is output twice (and thus we have 2 sensitivities we need to add to it). We can pull the to_indices out of the primal computation and remember the plain indexes used, then can reuse them to set gradients during the pullback.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"See the code for this in ChainRules.jl","category":"page"},{"location":"design/changing_the_primal.html#exp(::Matrix)","page":"Changing the Primal","title":"exp(::Matrix)","text":"","category":"section"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"Matrix Functions are generalizations of scalar functions to operate on matrices. Note that this is distinct from simply element-wise application of the function to the matrix's elements. The Matrix Exponential exp(::Matrix) is a particularly important matrix function.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"Al-Mohy and Higham (2009)[7], published a method for computing the pullback of exp(::Matrix). It is pretty complex and very cool. To quote its abstract (emphasis mine):","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"The algorithm is derived from the scaling and squaring method by differentiating the Padé approximants and the squaring recurrence, re-using quantities computed during the evaluation of the Padé approximant, and intertwining the recurrences in the squaring phase.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"Julia does in fact use a Padé approximation to compute exp(::Matrix). So we can extract the code for that into our augmented primal, and add remembering the intermediate quantities that are to be used. See the code for this in ChainRules.jl","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"An interesting scenario here that may be of concern to some: if Julia changes the algorithm it uses to compute exp(::Matrix), then during an AD primal pass, it will continue to use the old Padé approximation based algorithm. This may actually happen, as there are many other algorithms that can compute the matrix exponential. Further, perhaps there might be an improvement to the exact coefficient or cut-offs used by Julia's current Padé approximation. If Julia made this change it wopuld not be considered breaking. Exact floating point numerical values are not generally considered part of the SemVer-bound API. Rather only the general accuracy of the computed value relative to the true mathematical value (e.g. for common scalar operations Julia promises 1 ULP).","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"This change will result in the output of the AD primal pass not being exactly equal to what would be seen from just running the primal code. It will still be accurate because the current implementation is accurate, but it will be different. It is our argument that in general this should be considered acceptable, as long as the AD primal pass is in general about as accurate as the unaugmented primal. E.g. it might overshoot for some values the unaugmented primal undershoots for.","category":"page"},{"location":"design/changing_the_primal.html#eigvals","page":"Changing the Primal","title":"eigvals","text":"","category":"section"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"eigvals is a real case where the algorithm for the augmented primal and the original primal is already different today. To compute the pullback of eigvals you need to know not only the eigenvalues but also the eigenvectors. The eigen function computes both, so that is used in the augmented primal. See the code for this in ChainRules.jl. If we could not compute and remember the eigenvectors in the primal pass, we would have to call eigen in the gradient pass anyway and fully recompute eigenvectors and eigenvalues, more than doubling the total work.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"However, if you trace this down, it actually uses a different algorithm.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"eigvals basically wraps LAPACK.syevr!('N', ...), which goes through DSYEVR and eventually calls DSTERF, which uses \"Pal-Walker-Kahan variant of the QL or QR algorithm.\" to compute eigenvalues","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"In contrast, eigen wraps LAPACK.syevr!('V',...) which also goes through DSYEVR but eventually calls DSTEMR, which calculates eigenvalues \"either by bisection or the dqds algorithm.\".","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"Both of these are very good algorithms. LAPACK has had decades of work by experts and is one of the most trusted libraries for linear algebra. But they are different algorithms that give different results. The differences in practice are around 10^-15, which while very small on absolute terms are as far as Float64 is concerned a very real difference.","category":"page"},{"location":"design/changing_the_primal.html#Matrix-Division","page":"Changing the Primal","title":"Matrix Division","text":"","category":"section"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"Roughly speaking: Y=A\\B is the function that finds the least-square solution to YA ≈ B. When solving such a system, the efficient way to do so is to factorize A into an appropriate factorized form such as Cholesky or QR, then perform the \\ operation on the factorized form. The pullback of A\\B with respect to B is Ȳ -> A' \\ Ȳ. It should be noted that this involves computing the factorization of A' (the adjoint of A).[8] In this computation the factorization of the original A can reused. Doing so can give a 4x speed-up.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"We don't have this in ChainRules.jl yet, because Julia is missing some definitions of adjoint of factorizations (JuliaLang/julia#38293).[8] We have been promised them for Julia v1.7 though. You can see what the code would look like in PR #302.","category":"page"},{"location":"design/changing_the_primal.html#Conclusion","page":"Changing the Primal","title":"Conclusion","text":"","category":"section"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"This document has explained why rrule is the way it is. In particular it has highlighted why the primal computation is able to be changed from simply calling the function. Further, it has explained why rrule returns a closure for the pullback, rather than it being a separate function. It has highlighted several places in ChainRules.jl where this has allowed us to significantly improve performance. Being able to change the primal computation is practically essential for a high performance AD system.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"[1]: I am not just picking on Nabla randomly. Many of the core developers of ChainRules worked on Nabla prior. It's a good AD, but ChainRules incorporates lessons learned from working on Nabla.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"[2]: which may be an explicit tape or an implicit tape that is actually incorporated into generated code (à la Zygote)","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"[3]: Another key reason is if the operation is a primitive that is not defined in terms of more basic operations. In many languages this is the case for sin; where the actual implementation is in some separate libm.so. But actually sin in Julia is defined in terms of a polynomial. It's fairly vanilla Julia code. It shouldn't be too hard for an AD that only knows about basic operations like + and * to AD through it. In any case, that is another discussion for another day.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"[4]: Sure, this is small fries and depending on Julia version might just get solved by the optimizer[5], but go with it for the sake of example.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"[5]: To be precise, this is very likely to be solved by the optimizer inlining both and then performing common subexpression elimination, with the result that it generates the code for sincos just from having sin and cos inside the same function. However, this actually doesn't apply in the case of AD, as it is not possible to inline code called in the gradient pass into the primal pass. Those are separate functions called at very different times. This is something opaque closures should help solve.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"[6]: One change we might consider is to have logistic sigmoid to only remember one thing. Rather than remembering y and ex to use in the pullback, we could compute y / (1 + ex) during the augmented primal, and just remember that.","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"[7]: Al-Mohy, Awad H. and Higham, Nicholas J. (2009) Computing the Fréchet Derivative of the Matrix Exponential, with an application to Condition Number Estimation. SIAM Journal On Matrix Analysis and Applications., 30 (4). pp. 1639-1657. ISSN 1095-7162","category":"page"},{"location":"design/changing_the_primal.html","page":"Changing the Primal","title":"Changing the Primal","text":"[8]: To be clear here we mean adjoint as in the conjugate transpose of a matrix, rather than in the sense of reverse mode AD.","category":"page"},{"location":"design/many_differentials.html#manytypes","page":"Many Differential Types","title":"Design Notes: The many-to-many relationship between differential types and primal types","text":"","category":"section"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"ChainRules has a system where one primal type (the type having its derivative taken) can have multiple possible differential types (the type of the derivative); and where one differential type can correspond to multiple primal types. This is in-contrast to the Swift AD efforts, which has one differential type per primal type (Swift uses the term associated tangent type, rather than differential type).","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"terminology: differential and associated tangent type\nThe use of “associated tangent type” in AD is not technically correct, as differentials naturally live in the cotangent plane instead of the tangent plane. However it is often reasonable for AD to treat the cotangent plane and tangent plane as the same thing, and this was an intentional choice by the Swift team. Here we will just stick to the ChainRules terminology and only say “differential type” instead of “tangent type”.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"One thing to understand about differentials is that they have to form a vector space  (or something very like them). They need to support addition to each other, they need a zero which doesn't change what it is added to, and they need to support scalar multiplication (this isn't really required, but it is handy for things like gradient descent). Beyond being a vector space, differentials need to be able to be added to a primal value to get back another primal value. Or roughly equivalently a differential is a difference between two primal values.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"One thing to note in this example is that the primal does not have to be a vector. As an example, consider DateTime. A DateTime is not a vector space: there is no origin point, and DateTimes cannot be added to each other. The corresponding differential type is any subtype of Period, such as Millisecond, Hour, Day etc.","category":"page"},{"location":"design/many_differentials.html#Natural-differential","page":"Many Differential Types","title":"Natural differential","text":"","category":"section"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"For a given primal type, we say a natural differential type is one which people would intuitively think of as representing the difference between two primal values. It tends to already exist outside of the context of AD. So Millisecond, Hour, Day etc. are examples of natural differentials for the DateTime primal.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Note here that we already have a one primal type to many differential types relationship. We have Millisecond and Hour and Day all being valid differential types for DateTime. In this case we could convert them all to a single differential type, such as Nanoseconds, but that is not always a reasonable decision: we may run in to overflow, or lots of allocations if we need to use a BigInt to represent the number of Nanosecond since the start of the universe. For types with more complex semantics, such as array types, these considerations are much more important.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Natural differential types are the types people tend to think in, and thus the type they tend to write custom sensitivity rules in. An important special case of natural differentials is when the primal type is a vector space (e.g. Real,AbstractMatrix) in which case it is common for the natural differential type to be the same as the primal type. One exception to this is getindex. The ideal choice of differential type for getindex on a dense array would be some type of sparse array, due to the fact the derivative will have only one non-zero element. This actually further brings us to a weirdness of differential types not actually being closed under addition, as it would be ideal for the sparse array to become a dense array if summed over all elements.","category":"page"},{"location":"design/many_differentials.html#Structural-differential-types","page":"Many Differential Types","title":"Structural differential types","text":"","category":"section"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"AD cannot automatically determine natural differential types for a primal. For some types we may be able to declare manually their natural differential type. Other types will not have natural differential types at all - e.g. NamedTuple, Tuple, WebServer, Flux.Dense -  so we are destined to make some up. So beyond natural differential types, we also have structural differential types. ChainRules uses Tangent{P, <:NamedTuple} to represent a structural differential type corresponding to primal type P. Zygote v0.4 uses NamedTuple.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Structural differentials are derived from the structure of the input. Either automatically, as part of the AD, or manually, as part of a custom rule.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Consider the structure of DateTime:","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"julia> dump(now())\nDateTime\n  instant: UTInstant{Millisecond}\n    periods: Millisecond\n      value: Int64 63719890305605","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"The corresponding structural differential is:","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Tangent{DateTime}(\n    instant::Tangent{UTInstant{Millisecond}}(\n        periods::Tangent{Millisecond}(\n            value::Int64\n        )\n    )\n)","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"note: One must be allowed to take derivatives of integer arguments\nThis brings up another contrast to Swift. In Swift Int is considered non-differentiable, which is quite reasonable; it doesn’t have a very good definition of the limit of a small step (as that would be some floating/fixed point type). Int is intrinsically discrete. It is commonly used for indexing, and if one takes a gradient step, say turning x[2] into x[2.1] then that is an error. However, disallowing Int to be used as a differential means we cannot handle cases like DateTime having an inner field of milliseconds counted as an integer from the unix epoch or other cases where an integer is used as a convenience for computational efficiency. In the case where a custom sensitivity rule claims that there is a non-zero derivative for an Int argument that is being used for indexing, that code is simply wrong. We can’t handle incorrect code and trying to is a path toward madness. Julia, unlike Swift, is not well suited to handling rules about what you can and can’t do with particular types.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"So the structural differential is another type of differential. We must support both natural and structural differentials because AD can only create structural differentials (unless using custom sensitivity rules) and all custom sensitivities are only written in terms of natural differentials, as that is what is used in papers about derivatives.","category":"page"},{"location":"design/many_differentials.html#Semi-structural-differentials","page":"Many Differential Types","title":"Semi-structural differentials","text":"","category":"section"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Where there is no natural differential type for the outermost type but there is for some of its fields, we call this a \"semi-structural\" differential.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Consider if we had a representation of a country's GDP as output by some continuous time model like a Gaussian Process, where that representation is as a sequence of TimeSamples structured as follows:","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"julia> struct TimeSample\n           time::DateTime\n           value::Float64\n       end","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"We can look at its structure:","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"julia> dump(TimeSample(now(), 2.6e9))\nTimeSample\n  time: DateTime\n    instant: Dates.UTInstant{Millisecond}\n      periods: Millisecond\n        value: Int64 63720043490844\n  value: Float64 2.6e9","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Thus we see the that structural differential would be:","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Tangent{TimeSample}(\n    time::Tangent{DateTime}(\n        instant::Tangent{UTInstant{Millisecond}}(\n            periods::Tangent{Millisecond}(\n                value::Int64\n            )\n        )\n    ),\n    value::Float64\n)","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"But instead in the custom sensitivity rule we would write a semi-structured differential type. Since there is not a natural differential type for TimeSample but there is for DateTime.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Tangent{TimeSample}(\n    time::Day,\n    value::Float64\n)","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"So the rule author has written a structural differential with some fields that are natural differentials.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Another related case is for types that overload getproperty such as SVD and QR. In this case the structural differential will be based on the fields, but those fields do not always have an easy relation to what is actually used in math. For example, the QR type has fields factors and t, but we would more naturally think in terms of the properties Q and R. So most rule authors would want to write semi-structural differentials based on the properties.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"To return to the question of why ChainRules has Tangent{P, <:NamedTuple} whereas Zygote v0.4 just has NamedTuple, it relates to semi-structural derivatives, and being able to overload things more generally. If one knows that one has a semi-structural derivative based on property names, like Tangent{QR}(Q=..., R=...), and one is adding it to the true structural derivative based on field names Tangent{QR}(factors=..., τ=...), then we need to overload the addition operator to perform that correctly. We cannot happily overload similar things for NamedTuple since we don't know the primal type, only the names of the values contained. In fact we can't actually overload addition at all for NamedTuple as that would be type-piracy, so have to use Zygote.accum instead.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Another use of the primal being a type parameter is to catch errors. ChainRules disallows the addition of Tangent{SVD} to Tangent{QR} since in a correctly differentiated program that can never occur.","category":"page"},{"location":"design/many_differentials.html#Differentials-types-for-computational-efficiency","page":"Many Differential Types","title":"Differentials types for computational efficiency","text":"","category":"section"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"There is another kind of unnatural differential. One that is for computational efficiency. ChainRules has Thunks and InplaceableThunks, which wrap the computation of a derivative and delays that work until it is needed, either via the derivative being added to something or being unthunked manually, thus saving time if it is never used.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Another differential type used for efficiency is ZeroTangent which represents the hard zero (in Zygote v0.4 this is nothing). For example the derivative of f(x, y)=2x with respect to y is ZeroTangent(). Add ZeroTangent() to anything, and one gets back the original thing without change. We noted that all differentials need to be a vector space.  ZeroTangent() is the trivial vector space. Further, add ZeroTangent() to any primal value (no matter the type) and you get back another value of the same primal type (the same value in fact). So it meets the requirements of a differential type for all primal types. ZeroTangent can save on memory (since we can avoid allocating anything) and on time (since performing the multiplication ZeroTangent and Thunk are both examples of a differential type that is valid for multiple primal types.","category":"page"},{"location":"design/many_differentials.html#Conclusion","page":"Many Differential Types","title":"Conclusion","text":"","category":"section"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"Now, you have seen examples of both differential types that work for multiple primal types, and primal types that have  multiple valid differential types. Semantically we can handle these very easily in julia. Just put in a few more dispatching on +. Multiple-dispatch is great like that. The down-side is our type-inference becomes hard. If you have exactly 1 differential type for each primal type, you can very easily workout what all the types on your reverse pass will be - you don't really need type inference - but you lose so much expressibility.","category":"page"},{"location":"design/many_differentials.html#Appendix:-What-Swift-does","page":"Many Differential Types","title":"Appendix: What Swift does","text":"","category":"section"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"I don't know how Swift is handling thunks, maybe they are not, maybe they have an optimizing compiler that can just slice out code-paths that don't lead to values that get used; maybe they have a language built in for lazy computation.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"They are, as I understand it, handling ZeroTangent by requiring every differential type to define a zero method – which it has since it is a vector space. This costs memory and time, but probably not actually all that much. With regards to handling multiple different differential types for one primal, like natural and structural derivatives, everything needs to be converted to the canonical differential type of that primal.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"As I understand it, things can be automatically converted by defining conversion protocols or something like that, so rule authors can return anything that has a conversion protocol to the canonical differential type of the primal.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"However, it seems like this will run into problems. Recall that the natural differential in the case of getindex on an AbstractArray was a sparse array. But for say the standard dense Array, the only reasonable canonical differential type is also a dense Array. But if you convert a sparse array into a dense array you do giant allocations to fill in all the other entries with zero.","category":"page"},{"location":"design/many_differentials.html","page":"Many Differential Types","title":"Many Differential Types","text":"So this is the story about why we have many-to-many differential types in ChainRules.","category":"page"},{"location":"index.html#ChainRules","page":"Introduction","title":"ChainRules","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"ChainRules provides a variety of common utilities that can be used by downstream automatic differentiation (AD) tools to define and execute forward-, reverse-, and mixed-mode primitives.","category":"page"},{"location":"index.html#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"ChainRules is all about providing a rich set of rules for differentiation. When a person learns introductory calculus, they learn that the derivative (with respect to x) of a*x is a, and the derivative of sin(x) is cos(x), etc. And they learn how to combine simple rules, via the chain rule, to differentiate complicated functions. ChainRules is a programmatic repository of that knowledge, with the generalizations to higher dimensions.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Autodiff (AD) tools roughly work by reducing a problem down to simple parts that they know the rules for, and then combining those rules. Knowing rules for more complicated functions speeds up the autodiff process as it doesn't have to break things down as much.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"ChainRules is an AD-independent collection of rules to use in a differentiation system.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"note: The whole field is a mess for terminology\nIt isn't just ChainRules, it is everyone. Internally ChainRules tries to be consistent. Help with that is always welcomed.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"terminology: Primal\nOften we will talk about something as primal. That means it is related to the original problem, not its derivative. For example in y = foo(x), foo is the primal function, and computing foo(x) is doing the primal computation. y is the primal return, and x is a primal argument. typeof(y) and typeof(x) are both primal types.","category":"page"},{"location":"index.html#frule-and-rrule","page":"Introduction","title":"frule and rrule","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"terminology: `frule` and `rrule`\nfrule and rrule are ChainRules specific terms. Their exact functioning is fairly ChainRules specific, though other tools have similar functions. The core notion is sometimes called custom AD primitives, custom adjoints, custom gradients, custom sensitivities.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The rules are encoded as frules and rrules, for use in forward-mode and reverse-mode differentiation respectively.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The rrule for some function foo, which takes the positional arguments args and keyword arguments kwargs, is written:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"function rrule(::typeof(foo), args...; kwargs...)\n    ...\n    return y, pullback\nend","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"where y (the primal result) must be equal to foo(args...; kwargs...). pullback is a function to propagate the derivative information backwards at that point. That pullback function is used like: ∂self, ∂args... = pullback(Δy)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Almost always the pullback will be declared locally within the rrule, and will be a closure over some of the other arguments, and potentially over the primal result too.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The frule is written:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"function frule((Δself, Δargs...), ::typeof(foo), args...; kwargs...)\n    ...\n    return y, ∂Y\nend","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"where again y = foo(args; kwargs...), and ∂Y is the result of propagating the derivative information forwards at that point. This propagation is call the pushforward. Often we will think of the frule as having the primal computation y = foo(args...; kwargs...), and the pushforward ∂Y = pushforward(Δself, Δargs...), even though they are not present in seperate forms in the code.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"note: Why `rrule` returns a pullback but `frule` doesn't return a pushforward\nWhile rrule takes only the arguments to the original function (the primal arguments) and returns a function (the pullback) that operates with the derivative information, the frule does it all at once. This is because the frule fuses the primal computation and the pushforward. This is an optimization that allows frules to contain single large operations that perform both the primal computation and the pushforward at the same time (for example solving an ODE). This operation is only possible in forward mode (where frule is used) because the derivative information needed by the pushforward available with the frule is invoked – it is about the primal function's inputs. In contrast, in reverse mode the derivative information needed by the pullback is about the primal function's output. Thus the reverse mode returns the pullback function which the caller (usually an AD system) keeps hold of until derivative information about the output is available.","category":"page"},{"location":"index.html#The-propagators:-pushforward-and-pullback","page":"Introduction","title":"The propagators: pushforward and pullback","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"terminology: pushforward and pullback\nPushforward and pullback are fancy words that the autodiff community recently adopted from Differential Geometry. The are broadly in agreement with the use of pullback and pushforward in differential geometry. But any geometer will tell you these are the super-boring flat cases. Some will also frown at you. They are also sometimes described in terms of the jacobian: The pushforward is jacobian vector product (jvp), and pullback is jacobian transpose vector product (j'vp). Other terms that may be used include for pullback the backpropagator, and by analogy for pushforward the forwardpropagator, thus these are the propagators. These are also good names because effectively they propagate wiggles and wobbles through them, via the chain rule. (the term backpropagator may originate with \"Lambda The Ultimate Backpropagator\" by Pearlmutter and Siskind, 2008)","category":"page"},{"location":"index.html#Core-Idea","page":"Introduction","title":"Core Idea","text":"","category":"section"},{"location":"index.html#Less-formally","page":"Introduction","title":"Less formally","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The pushforward takes a wiggle in the input space, and tells what wobble you would create in the output space, by passing it through the function.\nThe pullback takes wobbliness information with respect to the function's output, and tells the equivalent wobbliness with respect to the functions input.","category":"page"},{"location":"index.html#More-formally","page":"Introduction","title":"More formally","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The pushforward of f takes the sensitivity of the input of f to a quantity, and gives the sensitivity of the output of f to that quantity The pullback of f takes the sensitivity of a quantity to the output of f, and gives the sensitivity of that quantity to the input of f.","category":"page"},{"location":"index.html#Math","page":"Introduction","title":"Math","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"This is all a bit simplified by talking in 1D.","category":"page"},{"location":"index.html#Lighter-Math","page":"Introduction","title":"Lighter Math","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"For a chain of expressions:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"a = f(x)\nb = g(a)\nc = h(b)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The pullback of g, which incorporates the knowledge of ∂b/∂a, applies the chain rule to go from ∂c/∂b to ∂c/∂a.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The pushforward of g,  which also incorporates the knowledge of ∂b/∂a, applies the chain rule to go from ∂a/∂x to ∂b/∂x.","category":"page"},{"location":"index.html#Geometric-interpretation-of-reverse-and-forwards-mode-AD","page":"Introduction","title":"Geometric interpretation of reverse and forwards mode AD","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Let us think of our types geometrically. In other words, elements of a type form a manifold. This document will explain this point of view in some detail.","category":"page"},{"location":"index.html#Some-terminology/conventions","page":"Introduction","title":"Some terminology/conventions","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Let p be an element of type M, which is defined by some assignment of numbers x_1x_m, say (x_1x_m) = (a_11_m)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"A function fM to K on M is (for simplicity) a polynomial Kx_1  x_m","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The tangent space T_pM of T at point p is the K-vector space spanned by derivations ddx. The tangent space acts linearly on the space of functions. They act as usual on functions. Our starting point is that we know how to write down ddx(f) = dfdx.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The collection of tangent spaces T_pM for pin M is called the tangent bundle of M.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Let df denote the first order information of f at each point. This is called the differential of f. If the derivatives of f and g agree at p, we say that df and dg represent the same cotangent at p. The covectors dx_1  dx_m form the basis of the cotangent space T^*_pM at p. Notice that this vector space is dual to T_p","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The collection of cotangent spaces T^*_pM for pin M is called the cotangent bundle of M.","category":"page"},{"location":"index.html#Push-forwards-and-pullbacks","page":"Introduction","title":"Push-forwards and pullbacks","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Let N be another type, defined by numbers y_1y_n, and let gM to N be a map, that is, an n-dimensional vector (g_1  g_m) of functions on M.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"We define the push-forward g_*TM to TN between tangent bundles by g_*(X)(h) = X(gcirc h) for any tangent vector X and function f. We have g_*(ddx_i)(y_j) = dg_jdx_i, so the push-forward corresponds to the Jacobian, given a chosen basis.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Similarly, the pullback of the differential df is defined by g^*(df) = d(fcirc g). So for a coordinate differential dy_j, we have g^*(dy_j) = d(g_j). Notice that this is a covector, and we could have defined the pullback by its action on vectors by g^*(dh)(X) = g_*(X)(dh) = X(gcirc h) for any function f on N and Xin TM. In particular, g^*(dy_j)(ddx_i) = d(g_j)dx_i. If you work out the action in a basis of the cotangent space, you see that it acts by the adjoint of the Jacobian.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Notice that the pullback of a differential and the pushforward of a vector have a very different meaning, and this should be reflected on how they are used in code.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The information contained in the push-forward map is exactly what does my function do to tangent vectors. Pullbacks, acting on differentials of functions, act by taking the total derivative of a function. This works in a coordinate invariant way, and works without the notion of a metric. Gradients recall are vectors, yet they should contain the same information of the differential df. Assuming we use the standard euclidean metric, we can identify df and nabla f as vectors. But pulling back gradients still should not be a thing.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"If the goal is to evaluate the gradient of a function f=gcirc hM to N to K, where g is a map and h is a function, we have two obvious options: First, we may push-forward a basis of M to TK which we identify with K itself. This results in m scalars, representing components of the gradient. Step-by-step in coordinates:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Compute the push-forward of the basis of T_pM, i.e. just the columns of the Jacobian dg_idx_j.\nCompute the push-forward of the function h (consider it as a map, K is also a manifold!) to get h_*(g_*T_pM) = sum_j dhdy_i (dg_idx_j)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Second, we pull back the differential dh:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"compute dh = dhdy_1dhdy_n in coordinates.\npull back by (in coordinates) multiplying with the adjoint of the Jacobian, resulting in g_*(dh) = sum_i(dg_idx_j)(dhdy_i).","category":"page"},{"location":"index.html#The-anatomy-of-pullback-and-pushforward","page":"Introduction","title":"The anatomy of pullback and pushforward","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"For our function foo(args...; kwargs...) = y:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"function pullback(Δy)\n    ...\n    return ∂self, ∂args...\nend","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The input to the pullback is often called the seed. If the function is y = f(x) often the pullback will be written s̄elf, x̄ = pullback(ȳ).","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"note: Note\nThe pullback returns one ∂arg per arg to the original function, plus one ∂self for the fields of the function itself (explained below).","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"terminology: perturbation, seed, sensitivity\nSometimes perturbation, seed, and even sensitivity will be used interchangeably. They are not generally synonymous, and ChainRules shouldn't mix them up. One must be careful when reading literature. At the end of the day, they are all wiggles or wobbles.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The pushforward is a part of the frule function. Considered alone it would look like:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"function pushforward(Δself, Δargs...)\n    ...\n    return ∂y\nend","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"But because it is fused into frule we see it as part of:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"function frule((Δself, Δargs...), ::typeof(foo), args...; kwargs...)\n    ...\n    return y, ∂y\nend","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The input to the pushforward is often called the perturbation. If the function is y = f(x) often the pushforward will be written ẏ = last(frule((ṡelf, ẋ), f, x)). ẏ is commonly used to represent the perturbation for y.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"note: Note\nIn the frule/pushforward, there is one Δarg per arg to the original function. The Δargs are similar in type/structure to the corresponding inputs args (Δself is explained below). The ∂y are similar in type/structure to the original function's output Y. In particular if that function returned a tuple then ∂y will be a tuple of the same size.","category":"page"},{"location":"index.html#self_derivative","page":"Introduction","title":"Self derivative Δself, ∂self, s̄elf, ṡelf etc","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"terminology: Δself, ∂self, s̄elf, ṡelf\nIt is the derivatives with respect to the internal fields of the function. To the best of our knowledge there is no standard terminology for this. Other good names might be Δinternal/∂internal.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"From the mathematical perspective, one may have been wondering what all this Δself, ∂self is. Given that a function with two inputs, say f(a, b), only has two partial derivatives: dfracfa, dfracfb. Why then does a pushforward take in this extra Δself, and why does a pullback return this extra ∂self?","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The reason is that in Julia the function f may itself have internal fields. For example a closure has the fields it closes over; a callable object (i.e. a functor) like a Flux.Dense has the fields of that object.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Thus every function is treated as having the extra implicit argument self, which captures those fields. So every pushforward takes in an extra argument, which is ignored unless the original function has fields. It is common to write function foo_pushforward(_, Δargs...) in the case when foo does not have fields. Similarly every pullback returns an extra ∂self, which for things without fields is NoTangent(), indicating there are no fields within the function itself.","category":"page"},{"location":"index.html#Pushforward-/-Pullback-summary","page":"Introduction","title":"Pushforward / Pullback summary","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Pullback\nreturned by rrule\ntakes output space wobbles, gives input space wiggles\nArgument structure matches structure of primal function output\nIf primal function returns a tuple, then pullback takes in a tuple of differentials.\n1 return per original function argument + 1 for the function itself\nPushforward:\npart of frule\ntakes input space wiggles, gives output space wobbles\nArgument structure matches primal function argument structure, but passed as a tuple at start of frule\n1 argument per original function argument + 1 for the function itself\n1 return per original function return","category":"page"},{"location":"index.html#Pullback/Pushforward-and-Directional-Derivative/Gradient","page":"Introduction","title":"Pullback/Pushforward and Directional Derivative/Gradient","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The most trivial use of the pushforward from within frule is to calculate the directional derivative:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"If we would like to know the directional derivative of f for an input change of (1.5, 0.4, -1)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"direction = (1.5, 0.4, -1) # (ȧ, ḃ, ċ)\ny, ẏ = frule((ZeroTangent(), direction...), f, a, b, c)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"On the basis directions one gets the partial derivatives of y:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"y, ∂y_∂a = frule((ZeroTangent(), 1, 0, 0), f, a, b, c)\ny, ∂y_∂b = frule((ZeroTangent(), 0, 1, 0), f, a, b, c)\ny, ∂y_∂c = frule((ZeroTangent(), 0, 0, 1), f, a, b, c)","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Similarly, the most trivial use of rrule and returned pullback is to calculate the gradient:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"y, f_pullback = rrule(f, a, b, c)\n∇f = f_pullback(1)  # for appropriate `1`-like seed.\ns̄elf, ā, b̄, c̄ = ∇f","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Then we have that ∇f is the gradient of f at (a, b, c). And we thus have the partial derivatives overlinemathrmself = dfracfmathrmself, overlinea = dfracfa, overlineb = dfracfb, overlinec = dfracfc, including the self-partial derivative, overlinemathrmself.","category":"page"},{"location":"index.html#Differentials","page":"Introduction","title":"Differentials","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The values that come back from pullbacks or pushforwards are not always the same type as the input/outputs of the primal function. They are differentials, which correspond roughly to something able to represent the difference between two values of the primal types. A differential might be such a regular type, like a Number, or a Matrix, matching to the original type; or it might be one of the AbstractTangent subtypes.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Differentials support a number of operations. Most importantly: + and *, which let them act as mathematical objects.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"The most important AbstractTangents when getting started are the ones about avoiding work:","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Thunk: this is a deferred computation. A thunk is a word for a zero argument closure. A computation wrapped in a @thunk doesn't get evaluated until unthunk is called on the thunk. unthunk is a no-op on non-thunked inputs.\nZeroTangent: It is a special representation of 0. It does great things around avoiding expanding Thunks in addition.","category":"page"},{"location":"index.html#Other-AbstractTangents:","page":"Introduction","title":"Other AbstractTangents:","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Tangent{P}: this is the differential for tuples and  structs. Use it like a Tuple or NamedTuple. The type parameter P is for the primal type.\nNoTangent: Zero-like, represents that the operation on this input is not differentiable. Its primal type is normally Integer or Bool.\nInplaceableThunk: it is like a Thunk but it can do in-place add!.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"index.html#Example-of-using-ChainRules-directly","page":"Introduction","title":"Example of using ChainRules directly","text":"","category":"section"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"While ChainRules is largely intended as a backend for autodiff systems, it can be used directly. In fact, this can be very useful if you can constrain the code you need to differentiate to only use things that have rules defined for. This was once how all neural network code worked.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"Using ChainRules directly also helps get a feel for it.","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"using ChainRulesCore\n\nfunction foo(x)\n    a = sin(x)\n    b = 0.2 + a\n    c = asin(b)\n    return c\nend\n\n# Define rules (alternatively get them for free via `using ChainRules`)\n@scalar_rule(sin(x), cos(x))\n@scalar_rule(+(x, y), (1.0, 1.0))\n@scalar_rule(asin(x), inv(sqrt(1 - x^2)))","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"#### Find dfoo/dx via rrules\n#### First the forward pass, gathering up the pullbacks\nx = 3;\na, a_pullback = rrule(sin, x);\nb, b_pullback = rrule(+, 0.2, a);\nc, c_pullback = rrule(asin, b)\n\n#### Then the backward pass calculating gradients\nc̄ = 1;                    # ∂c/∂c\n_, b̄ = c_pullback(c̄);     # ∂c/∂b = ∂c/∂b ⋅ ∂c/∂c\n_, _, ā = b_pullback(b̄);  # ∂c/∂a = ∂c/∂b ⋅ ∂b/∂a\n_, x̄ = a_pullback(ā);     # ∂c/∂x = ∂c/∂a ⋅ ∂a/∂x\nx̄                         # ∂c/∂x = ∂foo/∂x\n# output\n-1.0531613736418153","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"#### Find dfoo/dx via frules\nx = 3;\nẋ = 1;              # ∂x/∂x\nnofields = ZeroTangent();  # ∂self/∂self\n\na, ȧ = frule((nofields, ẋ), sin, x);                    # ∂a/∂x = ∂a/∂x ⋅ ∂x/∂x \nb, ḃ = frule((nofields, ZeroTangent(), ȧ), +, 0.2, a);  # ∂b/∂x = ∂b/∂a ⋅ ∂a/∂x\nc, ċ = frule((nofields, ḃ), asin, b);                   # ∂c/∂x = ∂c/∂b ⋅ ∂b/∂x\nċ                                                       # ∂c/∂x = ∂foo/∂x\n# output\n-1.0531613736418153","category":"page"},{"location":"index.html","page":"Introduction","title":"Introduction","text":"#### Find dfoo/dx via FiniteDifferences.jl\nusing FiniteDifferences\ncentral_fdm(5, 1)(foo, x)\n# output\n-1.0531613736418257\n\n#### Find dfoo/dx via ForwardDiff.jl\nusing ForwardDiff\nForwardDiff.derivative(foo, x)\n# output\n-1.0531613736418153\n\n#### Find dfoo/dx via Zygote.jl\nusing Zygote\nZygote.gradient(foo, x)\n# output\n(-1.0531613736418153,)","category":"page"}]
}
