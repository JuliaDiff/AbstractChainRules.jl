<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>The AbstractArray Dilemma · ChainRules</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/indigo.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../index.html"><img src="../assets/logo.svg" alt="ChainRules logo"/></a><div class="docs-package-name"><span class="docs-autofit">ChainRules</span></div><form class="docs-search" action="../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../index.html">Introduction</a></li><li><a class="tocitem" href="../FAQ.html">FAQ</a></li><li><a class="tocitem" href="../writing_good_rules.html">Writing Good Rules</a></li><li><a class="tocitem" href="../complex.html">Complex Numbers</a></li><li><a class="tocitem" href="../arrays.html">Deriving Array Rules</a></li><li><a class="tocitem" href="../debug_mode.html">Debug Mode</a></li><li><a class="tocitem" href="../gradient_accumulation.html">Gradient Accumulation</a></li><li><span class="tocitem">Usage in AD</span><ul><li><a class="tocitem" href="../autodiff/overview.html">Overview</a></li><li><a class="tocitem" href="../autodiff/operator_overloading.html">Operator Overloading</a></li></ul></li><li><span class="tocitem">Design</span><ul><li><a class="tocitem" href="changing_the_primal.html">Changing the Primal</a></li><li><a class="tocitem" href="many_differentials.html">Many Differential Types</a></li><li class="is-active"><a class="tocitem" href="abstract_array_dilemma.html">The AbstractArray Dilemma</a><ul class="internal"><li><a class="tocitem" href="#The-Aim-Of-This-Document"><span>The Aim Of This Document</span></a></li><li><a class="tocitem" href="#Problem-Statement"><span>Problem Statement</span></a></li><li><a class="tocitem" href="#What-Does-Reverse-Mode-AD-Do?"><span>What Does Reverse-Mode AD Do?</span></a></li><li><a class="tocitem" href="#Dense-(Co)Tangent-Spaces-For-Sparse-Arrays"><span>Dense (Co)Tangent Spaces For Sparse Arrays</span></a></li><li><a class="tocitem" href="#A-Tractable-Tangent-Space-for-Structured-Linear-Algebra"><span>A Tractable Tangent Space for Structured Linear Algebra</span></a></li><li><a class="tocitem" href="#some_peculiar_behaviour"><span>Some Peculiar AD Behaviour</span></a></li><li><a class="tocitem" href="#Conclusion"><span>Conclusion</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../api.html">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Design</a></li><li class="is-active"><a href="abstract_array_dilemma.html">The AbstractArray Dilemma</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="abstract_array_dilemma.html">The AbstractArray Dilemma</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDiff/ChainRulesCore.jl/blob/master/docs/src/design/abstract_array_dilemma.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="The-AbstractArray-Dilemma"><a class="docs-heading-anchor" href="#The-AbstractArray-Dilemma">The AbstractArray Dilemma</a><a id="The-AbstractArray-Dilemma-1"></a><a class="docs-heading-anchor-permalink" href="#The-AbstractArray-Dilemma" title="Permalink"></a></h1><h2 id="The-Aim-Of-This-Document"><a class="docs-heading-anchor" href="#The-Aim-Of-This-Document">The Aim Of This Document</a><a id="The-Aim-Of-This-Document-1"></a><a class="docs-heading-anchor-permalink" href="#The-Aim-Of-This-Document" title="Permalink"></a></h2><p>We seek to provide the information necessary to answer the question: &quot;is it acceptable / desirable to treat <code>struct</code>s which subtype AbstractArray in the same way as other structs, or must they be treated separately.&quot; So after reading this document, you should feel better equipped to form an opinion on this matter.</p><h2 id="Problem-Statement"><a class="docs-heading-anchor" href="#Problem-Statement">Problem Statement</a><a id="Problem-Statement-1"></a><a class="docs-heading-anchor-permalink" href="#Problem-Statement" title="Permalink"></a></h2><p>Consider an <code>rrule</code> with signature</p><pre><code class="language-julia">function rrule(::typeof(*), A::AbstractMatrix, B::AbstractMatrix)
    C = A * B
    function mul_pullback(ΔC)
        ΔA = ΔC * B
        ΔB = A * ΔC&#39;
        return NO_FIELDS, ΔA, ΔB
    end
    return C, mul_pullback
end</code></pre><p>and consider its performance when <code>A</code> is a <code>Diagonal{Float64, Vector{Float64}}</code>, and <code>B</code> a <code>Matrix{Float64}</code>. <code>C</code> will be another <code>Matrix{Float64}</code>, meaning that the cotangent <code>ΔC</code> will usually be a <code>Matrix{Float64}</code>. This means that the cotangent <code>ΔA</code> will be a <code>Matrix{Float64}</code>! Let <code>M := size(A, 1)</code> then the reverse-pass will require at least <code>M^2</code> <code>Float64</code>s, while the primal pass will only require <code>M</code>. Similarly, letting <code>N := size(B, 2)</code>, computing <code>ΔA</code> under this implementation requires <code>O(M^2N)</code> operations, while the forwards requires only <code>O(MN)</code>. Thus, if an AD system utilises this particular <code>rrule</code>, its performance guarantees will be broken. If, as is often the case, a user&#39;s reason for expressing a function in terms of a <code>Diagonal</code> (rather than a <code>Matrix</code> which happens to be diagonal) is at least partly performance-related, then this poor performance is crucially important. To see this, observe that the time and memory required to execute the reverse-pass is roughly the same as would be required to execute a primal pass had <code>A</code> been dense. Thus, and performance gain that one might have hoped to obtain using a <code>Diagonal</code> has been lost.</p><p>The way to rectify this problem is to require that only the diagonal of <code>ΔA</code> be preserved, in which case the same asymptotic complexity can be obtained in the rrule as in the function evaluation itself. Indeed, if this rule did not exist, and the AD tool applied to <code>A * B</code> were successfully able to derive a pullback, this is essentially what it will do. Moreover, if <code>A</code> we some other <code>struct</code>, which was not to be interpretted as an array of any kind, and it had the above <code>*</code> function defined on it, this is what AD would do.</p><p>You might reasonable wonder what other types this kind of thing occurs for. It seems inevitable that all of the following will suffer from the same kinds of dilemma for all functions involving them, each of which are commonly used to accelerate code:</p><ol><li><code>BiDiagonal</code>, <code>TriDiagonal</code>, <code>SymTridiagonal</code> (LinearAlgebra)</li><li><code>SparseMatrixCSC</code> (SparseArrays)</li><li><code>Fill</code>, <code>One</code>, <code>Zero</code> (FillArrays.jl)</li><li><code>BlockDiagonal</code> (BlockDiagonals.jl)</li><li><code>StructArray</code> (StructArrays.jl)</li><li><code>WoodburyPDMat</code> (PDMatsExtras)</li><li><code>PDiagMat</code>, <code>PDSparseMat</code>, <code>ScalMat</code> (PDMats.jl)</li><li><code>InfiniteArray</code> (InfiniteArrays.jl)</li></ol><p>If any of these types are placed inside the various wrapper types that Julia offers (<code>UpperTriangular</code>, <code>LowerTriangular</code>, <code>Diagonal</code>, <code>Symmetric</code>, etc), the same kinds of problems arise. The same is true of things like <code>Matrix{&lt;:Diagonal}</code> i.e. a matrix of <code>Diagonal</code> matrices.</p><p>In a very literal sense, the number of possible arrays that this could affect is unbounded, because Julia users keep writing new array types which express particular types of structure! It&#39;s commonly the case that you think you&#39;ve written code that your preferred AD tool should be able to handle quite straightforwardly, only to find that a rule exists that gets in the way of AD doing its job. When this happens, it is more than a little frustrating!</p><p>However, we <em>do</em> need rules for some &quot;primitive&quot; array types. For example, the above rule works just fine for &quot;dense&quot; arrays such as <code>Matrix{Float64}</code>, <code>StridedMatrix{Float64}</code>, <code>CuArray{Float32}</code>, <code>SMatrix{2, 2, Float64}</code>, as well as for other real element types. Simiarly, when someone defines a new array type for which the above implementation of <code>*</code> and other similar functions works well / is a good choice, it would be unfortunate if they don&#39;t have a quick way to declare their array as a &quot;primitive&quot; array, and to ensure that it inherits these rules.</p><p>Hopefully, this section convinces you that there is something worth thinking about here, and that this is not a problem which can be ignored.</p><h2 id="What-Does-Reverse-Mode-AD-Do?"><a class="docs-heading-anchor" href="#What-Does-Reverse-Mode-AD-Do?">What Does Reverse-Mode AD Do?</a><a id="What-Does-Reverse-Mode-AD-Do?-1"></a><a class="docs-heading-anchor-permalink" href="#What-Does-Reverse-Mode-AD-Do?" title="Permalink"></a></h2><p>Before considering what to do, it&#39;s helpful to have a working definition of reverse-mode AD.</p><p>Define a function <code>reverse_mode_ad</code> in terms of its relationship with a function <code>forwards_mode_ad</code>:</p><pre><code class="language-julia">ẏ = forwards_mode_ad(f, x, ẋ)
x̄ = reverse_mode_ad(f, x, ȳ)
&lt;ȳ, ẏ&gt; == &lt;x̄, ẋ&gt;
</code></pre><p>where <code>&lt;., .&gt;</code> is an inner product – we&#39;ll usually write <code>dot</code> later on. Therefore, if we have defined <code>forwards_mode_ad</code>, we have defined <code>reverse_mode_ad</code>.</p><p>We call the set of values that <code>ẋ</code> can take the <em>tangent</em> <em>space</em> <em>of</em> <code>x</code>, and the set of values that <code>x̄</code> can take the <em>cotangent</em> <em>space</em> <em>of</em> <code>x</code>. The set of values that <code>ẏ</code> and <code>ȳ</code> are called the <em>tangent</em> and <em>cotangent</em> spaces of <code>y</code> respectively.</p><p>For example, assuming <code>forwards_mode_ad</code> computes <code>ẏ = J(f, x) ẋ</code>, where <code>J</code> is the Jacobian of <code>f</code> at <code>x</code>, then <code>x̄ = J(f, x)&#39;ȳ</code>. So in this situation, <code>reverse_mode_ad</code> can be used to compute the Jacobian.</p><p>However, if this assumption about <code>forwards_mode_ad</code> fails to hold for some <code>f</code> and <code>x</code>, then neither <code>forwards_mode_ad</code> nor <code>reverse_mode_ad</code> will compute the above functions of the Jacobian. <a href="abstract_array_dilemma.html#some_peculiar_behaviour">This happens in practice</a>.</p><h2 id="Dense-(Co)Tangent-Spaces-For-Sparse-Arrays"><a class="docs-heading-anchor" href="#Dense-(Co)Tangent-Spaces-For-Sparse-Arrays">Dense (Co)Tangent Spaces For Sparse Arrays</a><a id="Dense-(Co)Tangent-Spaces-For-Sparse-Arrays-1"></a><a class="docs-heading-anchor-permalink" href="#Dense-(Co)Tangent-Spaces-For-Sparse-Arrays" title="Permalink"></a></h2><p>Equipped with an understanding of the relationship between forwards- and reverse-mode AD, we turn to the primary question.</p><p>Let us first consider the consequences of letting the (co)tangent space of a sparse array be equivalent to a dense for functions of a sparse array. We will focus on <code>Diagonal</code> matrices, and <code>Matrix</code> (co)tangent spaces, but the arguments below follow for any kind of array in which it can be crucial to exploit sparsity for performance reasons, such as those listed above.</p><p>Under this definition of the (co)tangent space, reverse-mode AD must <em>always</em> assume that the tangent of a given <code>Diagonal</code> matrix might have non-zero off-diagonals, and therefore must never drop the off-diagonals in its cotangents. For example, let <code>x</code> be a <code>Diagonal{&lt;:Real}</code> matrix, <code>ẋ</code> a <code>Matrix{&lt;:Real}</code> representing a tangent for <code>x</code>, then for some function <code>f</code> and output-cotangent <code>ȳ</code>, then</p><pre><code class="language-julia">dot(reverse_mode_ad(f, x, ȳ), ẋ)</code></pre><p>involves the off-diagonal elements of <code>ẋ</code>, so the off-diagonal elements of <code>reverse_mode_ad(f, x, ȳ)</code> must be retained if reverse-mode is to agree with forwards-mode.</p><p>Conversely, if the tangent space of <code>x</code> is restricted to be the <code>Diagonal</code> matrices, then the off-diagonal elements of <code>ẋ</code> are always zero, and it&#39;s safe to drop the off-diagonal elements of <code>reverse_mode_ad(f, x, ȳ)</code>. A similar result if <code>Diagonal</code> is treated as a <code>struct</code>, and its tangent type therefore required to be a <code>Composite</code>, since they&#39;re isomorphic to one another.</p><h3 id="functions_of_sparse_arrays"><a class="docs-heading-anchor" href="#functions_of_sparse_arrays">Functions of Sparse Arrays</a><a id="functions_of_sparse_arrays-1"></a><a class="docs-heading-anchor-permalink" href="#functions_of_sparse_arrays" title="Permalink"></a></h3><p>To see how the above manifests itself in practice, consider the following function of a <code>Diagonal</code> matrix:</p><pre><code class="language-julia">h(D::AbstractMatrix{&lt;:Real}, x::Vector{&lt;:Real}) = (logdet(cholesky(D)), sum(D), D * x)</code></pre><p>This is a perfectly plausible function – it&#39;s the kind of thing you might see somewhere buried inside code that deals with multivariate Normal distributions with covariance matrix <code>D</code>.</p><p>If you evaluate it using a <code>Diagonal</code> matrix and a <code>Vector</code></p><pre><code class="language-julia">N = 1_000_000
h(Diagonal(rand(N) .+ 1), randn(N))</code></pre><p>you obtain a function which requires O(N) operations, rather than the O(N^2) required if you evaluate it for a <code>Matrix</code> and a <code>Vector</code>.</p><p>If the semantics of AD are such that it is necessary to assume that the tangent of <code>D</code> might be non-diagonal, it is necessary for AD to treat <code>D</code> like a <code>Matrix</code> which happens to be diagonal, as discussed above. This code will clearly not run if that is the case.</p><p>This is a very standard example of taking advantage of Julia&#39;s zero-overhead abstractions to accelerate computation where possible without re-writing your code, and is exactly what you would want if working with a multivariate Gaussian with diagonal covariance matrix. These kinds of structure occur regularly in practice – perhaps you wish to compare Gaussian models with a dense covariance matrix with different covariance structures (diagonal, nearly-low-rank, sparse-inverse, etc) on the same data set to infer something interpretable about the properties of your data. In this case, linear algebra operations which exploit this structure is a nice-to-have, and can accelerate your code a lot.</p><p>These accelerations become utterly essential when dealing with very high-dimensional problems, such as the one above, in which you absolutely <em>need</em> to restrict your covariance matrix in some way to produce something tractable (e.g. there are entire communities of people that work on this kind of thing in the context of Gaussian processes).</p><p>Furthermore, this choice of tangent space for <code>D</code> affords us no new functionality as, if the implementer of <code>h</code> were interested in matrices which <em>happened</em> to be diagonal (at the initialisation of some iterative algorithm, say) they could just <code>collect</code> said matrix.</p><h4 id="Can-Abstraction-Save-Us?"><a class="docs-heading-anchor" href="#Can-Abstraction-Save-Us?">Can Abstraction Save Us?</a><a id="Can-Abstraction-Save-Us?-1"></a><a class="docs-heading-anchor-permalink" href="#Can-Abstraction-Save-Us?" title="Permalink"></a></h4><p>As an aside, perhaps we could avoid treating the (co)tangent space of <code>D</code> as a <code>Matrix</code>, and instead exploit some structure. For example, the cotangent of <code>sum(D)</code> is might be represented by a <code>Fill</code>, <code>D * x</code> by a rank-1 matrix, and perhaps the cotangent of <code>logdet(cholesky(D))</code> by... something? One would then need to know how to add <code>AbstractMatrix</code>s of these types together in a manner that is efficient and retains the structure, if a <code>Matrix</code> is to be avoided at the accumulation step of reverse-mode AD.</p><p>Perhaps this is possible, but it seems incredibly <em>hard</em>, and is certainly not something that could easily generalise to new structured <code>AbstractArray</code> types – consider that an author of a new <code>AbstractArray</code> would need to anticipate all of the operations that might be defined on their matrix (that is clearly not possible), either utilise or define appropriate <code>AbstractArray</code> types to represent the cotangent produced by each of them (which seems hard, and might not be possible in general), and finally to know how to add these types together (again, hard and certainly not easy to automate).</p><p>It is for these reasons that I do not believe this appoach to be plausible – we must assume that a <code>Matrix</code> is necessary (or some other &quot;dense&quot; array like a <code>CuArray</code>) if the (co)tangent space is not restricted.</p><h3 id="intermediate_functions"><a class="docs-heading-anchor" href="#intermediate_functions">Intermediate Functions of Sparse Arrays</a><a id="intermediate_functions-1"></a><a class="docs-heading-anchor-permalink" href="#intermediate_functions" title="Permalink"></a></h3><p>In the previous example, the restrictions that appear to be necessary change the output of AD from that which we might reasonable expect, which is probably a bit surprising for new users. However, it&#39;s very common (certainly not less common than the previous case) to find the above example inside other code, as intermediate computations in some larger function. In such cases, the restriction of the tangent space of a sparse array will <em>not</em> change the outcome, so in such situations we really lose nothing by restricting the tangent space, but gain a lot.</p><p>For example, consider</p><pre><code class="language-julia">function g(d::Vector{&lt;:Real}, x::Vector{&lt;:Real})
    D = Diagonal(d)
    return h(D, x)
end</code></pre><p>The off-diagonal elements of the cotangent of <code>Diagonal(d)</code> are clearly not required in this case due to the call to <code>Diagonal</code>, which necessarily ignores them on the reverse-pass. However, <code>h</code> does not know this – it does not get to know the context in which it is being called – therefore it <em>must</em> assume that the cotangent needs to be non-diagonal if we allow the for non-diagonal tangents of <code>D</code>. For large <code>N</code>, this is clearly prohibitive, and precludes AD from being run on <code>g</code>, despite <code>g</code> being a completely reasonable function to evaluate on a large <code>Diagonal</code> matrix.</p><h3 id="Input-Dependent-Intermediate-Functions-of-Sparse-Arrays"><a class="docs-heading-anchor" href="#Input-Dependent-Intermediate-Functions-of-Sparse-Arrays">Input-Dependent Intermediate Functions of Sparse Arrays</a><a id="Input-Dependent-Intermediate-Functions-of-Sparse-Arrays-1"></a><a class="docs-heading-anchor-permalink" href="#Input-Dependent-Intermediate-Functions-of-Sparse-Arrays" title="Permalink"></a></h3><p>The above example is quite clear-cut – if the <code>Diagonal</code> is <em>always</em> constructed somewhere inside the function, then there&#39;s no need to keep the off diagonals. This in turn led us to the conclusion that the off-diagonals <em>must</em> be dropped to make such programmes tractable to differentiate, and that the off-diagonals must therefore always be dropped.</p><p>A slightly different situation occurs when the <code>Diagonal</code> is constructed in a manner which depends in a particular way on the arguments of the function. We explore this situation extensively elsewhere, in <a href="abstract_array_dilemma.html#some_peculiar_behaviour">Some Peculiar AD Behaviour</a>.</p><p>The upshot is that these situations are a property of AD generally, not specifically a property of sparse linear algebra, so we consider them no further.</p><h3 id="Takeaways"><a class="docs-heading-anchor" href="#Takeaways">Takeaways</a><a id="Takeaways-1"></a><a class="docs-heading-anchor-permalink" href="#Takeaways" title="Permalink"></a></h3><p>Similar arguments could be constructed for the other structured linear algebra types discussed earlier, so the lesson of this example seems to be that in order for users of AD to be able to use structured linear algebra types in the usual way, it is generally <em>necessary</em> to place strong restrictions on the tangent space, and these constraints <em>will</em> lead to results that new users probably won&#39;t expect.</p><p>The consequence of not accepting these restrictions is that important programmes that really ought to be straightforward to apply AD to are rendered intractable, for no notable increase in the functionality available.</p><p>Moreover, the restrictions on the (co)tangent spaces of such types are what AD would naturally do if only rules on e.g. <code>Array{Float64, N}</code> were defined, which is plainly a &quot;primitive&quot; array type, as AD would treat them like any other <code>struct</code>.</p><h2 id="A-Tractable-Tangent-Space-for-Structured-Linear-Algebra"><a class="docs-heading-anchor" href="#A-Tractable-Tangent-Space-for-Structured-Linear-Algebra">A Tractable Tangent Space for Structured Linear Algebra</a><a id="A-Tractable-Tangent-Space-for-Structured-Linear-Algebra-1"></a><a class="docs-heading-anchor-permalink" href="#A-Tractable-Tangent-Space-for-Structured-Linear-Algebra" title="Permalink"></a></h2><p>If you treat a <code>struct</code> which subtypes <code>AbstractArray</code> like any other <code>struct</code> would be treated, everything works. Just let the (co)tangent type be a <code>Composite</code>.</p><p>Often it feels more intuitive to define the cotangent type to be something that mirrors the primal type – for example letting the (co)tangent type of a <code>Diagonal</code> be another <code>Diagonal</code>. This seems to be fine some of the time, as it&#39;s often possible to specify an isomorphism between the <code>Composite</code> and this type.</p><p>Sometimes it makes less sense though. For example, it&#39;s unclear that representing the (co)tangent of a <code>Fill</code> with another <code>Fill</code> makes much sense at all, semantically speaking. Certainly you could do it, but it feels like a hack when you try to get down to what you actually <em>mean</em>, and try to marry that in with the existing meaning that a <code>Fill</code> has.</p><p>A <code>Composite</code> doesn&#39;t suffer this problem, because its only interpretation is as a (co)tangent for a <code>struct</code>.</p><h2 id="some_peculiar_behaviour"><a class="docs-heading-anchor" href="#some_peculiar_behaviour">Some Peculiar AD Behaviour</a><a id="some_peculiar_behaviour-1"></a><a class="docs-heading-anchor-permalink" href="#some_peculiar_behaviour" title="Permalink"></a></h2><p>This scenario has nothing to do with <code>frule</code>s or <code>rrule</code>s for structured array types. The point is to highlight that some of the peculiarities that can arise when you treat structured array types as structs can be found elsewhere. We&#39;ll tie this example back in to sparse linear algebra in the next section.</p><p>Consider the following programme:</p><pre><code class="language-julia">using FiniteDifferences
using ForwardDiff
using LinearAlgebra

f(x::AbstractMatrix{&lt;:Real}) = isdiag(x) ? x[diagind(x)] : x

g = sum ∘ f
X = diagm(randn(3))</code></pre><p>The function <code>g</code> produces the same answer regardless which branch is taken in <code>f</code>.</p><p>FiniteDifferences.jl successfully computes the gradient of <code>g</code> at <code>X</code>:</p><pre><code class="language-julia">julia&gt; FiniteDifferences.grad(central_fdm(5, 1), g, X)[1]
3×3 Matrix{Float64}:
 1.0  1.0  1.0
 1.0  1.0  1.0
 1.0  1.0  1.0</code></pre><p>This seems correct.</p><p>However, ForwardDiff does not produce the same result:</p><pre><code class="language-julia">julia&gt; ForwardDiff.gradient(g, X)
3×3 Matrix{Float64}:
 1.0  0.0  0.0
 0.0  1.0  0.0
 0.0  0.0  1.0</code></pre><p>This discrepancy occurs because <code>ForwardDiff</code> only ever hits the first branch in <code>f</code>. On the other hand, <code>FiniteDifferences</code> will hit the second branch in <code>f</code> for any non-diagonal perturbation of the input. This property seems to arise from AD considering only considering a function at a single point, rather than the neighbourhood of a point.</p><p>We&#39;ve not used any contraversial <code>frule</code>s here, so this property must be something to do with AD&#39;s semantics, not incorrect rule implementation.</p><h3 id="What-About-Reverse-Mode?"><a class="docs-heading-anchor" href="#What-About-Reverse-Mode?">What About Reverse-Mode?</a><a id="What-About-Reverse-Mode?-1"></a><a class="docs-heading-anchor-permalink" href="#What-About-Reverse-Mode?" title="Permalink"></a></h3><p>Unsurprisingly, ReverseDiff.jl and Zygote.jl both behave in the same way as ForwardDiff.jl:</p><pre><code class="language-none">using ReverseDiff
using Zygote

julia&gt; ReverseDiff.gradient(g, X)
3×3 Matrix{Float64}:
 1.0  0.0  0.0
 0.0  1.0  0.0
 0.0  0.0  1.0

julia&gt; Zygote.gradient(g, X)[1]
3×3 Matrix{Float64}:
 1.0  0.0  0.0
 0.0  1.0  0.0
 0.0  0.0  1.0</code></pre><p>Again, this seems to be something inherent in the way that AD handles branches.</p><h3 id="What-Does-This-Have-To-Do-With-Sparse-Arrays?"><a class="docs-heading-anchor" href="#What-Does-This-Have-To-Do-With-Sparse-Arrays?">What Does This Have To Do With Sparse Arrays?</a><a id="What-Does-This-Have-To-Do-With-Sparse-Arrays?-1"></a><a class="docs-heading-anchor-permalink" href="#What-Does-This-Have-To-Do-With-Sparse-Arrays?" title="Permalink"></a></h3><p>Consider a small modification to the previous programme:</p><pre><code class="language-julia">using LinearAlgebra

f(x::AbstractMatrix{&lt;:Real}) = isdiag(x) ? Diagonal(diag(x)) : x

g = sum ∘ f</code></pre><p>We now return a <code>Diagonal</code> rather than a <code>Vector</code>, but <code>g</code> is essentially unchanged beyond this implementation detail. All four tools provide (essentially) the same answer as before:</p><pre><code class="language-julia">using FiniteDifferences
using ForwardDiff
using ReverseDiff
using Zygote

julia&gt; FiniteDifferences.grad(central_fdm(5, 1), g, X)[1]
3×3 Matrix{Float64}:
 1.0  1.0  1.0
 1.0  1.0  1.0
 1.0  1.0  1.0

julia&gt; ForwardDiff.gradient(g, X)
3×3 Matrix{Float64}:
 1.0  0.0  0.0
 0.0  1.0  0.0
 0.0  0.0  1.0

julia&gt; ReverseDiff.gradient(g, X)
3×3 Matrix{Float64}:
 1.0  0.0  0.0
 0.0  1.0  0.0
 0.0  0.0  1.0

julia&gt; Zygote.gradient(g, X)[1]
3×3 Diagonal{Float64, FillArrays.Fill{Float64, 1, Tuple{Base.OneTo{Int64}}}}:
 1.0   ⋅    ⋅
  ⋅   1.0   ⋅
  ⋅    ⋅   1.0</code></pre><p><code>Zygote</code> has hit a couple of <code>rrule</code>s, which are worth diving into for sanity&#39;s sake. The <code>rrule</code> for <code>diag</code> is <a href="https://github.com/JuliaDiff/ChainRules.jl/blob/76ef95c326e773c6c7140fb56eb2fd16a2af468b/src/rulesets/LinearAlgebra/structured.jl#L46">defined</a> as:</p><pre><code class="language-julia">function rrule(::typeof(diag), A::AbstractMatrix)
    function diag_pullback(ȳ)
        return (NO_FIELDS, Diagonal(ȳ))
    end
    return diag(A), diag_pullback
end</code></pre><p>When the input <code>A</code> is a <code>Matrix{Float64}</code> and the cotangent <code>ȳ</code> a <code>Vector{Float64}</code>, it seems uncontraversial.</p><p>The <code>rrule</code> for the constructor for <code>Diagonal</code> is defined <a href="https://github.com/JuliaDiff/ChainRules.jl/blob/76ef95c326e773c6c7140fb56eb2fd16a2af468b/src/rulesets/LinearAlgebra/structured.jl#L32">here</a> and is similarly uncontraversial:</p><pre><code class="language-julia">function rrule(::Type{&lt;:Diagonal}, d::AbstractVector)
    function Diagonal_pullback(ȳ::AbstractMatrix)
        return (NO_FIELDS, diag(ȳ))
    end
    return Diagonal(d), Diagonal_pullback
end</code></pre><p>The point of highlighting these two <code>rrule</code>s is to show that they both seem sensible – certainly it&#39;s unclear how else one might reasonably implement them. The discrepancy between the usual definition of the gradient of <code>g</code> and what AD produces must, therefore, be &quot;AD-related&quot; not &quot;rule-related&quot;.</p><h3 id="The-sparse-Function"><a class="docs-heading-anchor" href="#The-sparse-Function">The <code>sparse</code> Function</a><a id="The-sparse-Function-1"></a><a class="docs-heading-anchor-permalink" href="#The-sparse-Function" title="Permalink"></a></h3><p>Recall that the function <code>sparse</code> returns a <code>SparseMatrixCSC</code> when applied to a <code>Matrix</code>, in which only the non-zero elements of the <code>Matrix</code> are explicitly represented. If we define <code>g</code> as follows:</p><pre><code class="language-julia">using SparseArrays

g = sum ∘ sparse
X = [1.0 0 0; 1.0 0 2.0; 0 3.0 0]</code></pre><p>we have a situation which is analogous to the previous <code>Diagonal</code> example. The precise sparsity pattern in <code>g(X)</code> will depend upon the precise values of <code>X</code>. As before, <code>FiniteDifferences</code> gets the answer you would expect, while the AD tools give something which reflects the sparsity pattern.</p><pre><code class="language-julia">julia&gt; FiniteDifferences.grad(central_fdm(5, 1), g, X)[1]
3×3 Matrix{Float64}:
 1.0  1.0  1.0
 1.0  1.0  1.0
 1.0  1.0  1.0

julia&gt; ForwardDiff.gradient(g, X)
3×3 Matrix{Float64}:
 1.0  0.0  0.0
 1.0  0.0  1.0
 0.0  1.0  0.0

julia&gt; ReverseDiff.gradient(g, X)
3×3 Matrix{Float64}:
 1.0  0.0  0.0
 1.0  0.0  1.0
 0.0  1.0  0.0</code></pre><p>Zygote doesn&#39;t work on this example at the time of writing, hence its exclusion.</p><h3 id="Takeaway"><a class="docs-heading-anchor" href="#Takeaway">Takeaway</a><a id="Takeaway-1"></a><a class="docs-heading-anchor-permalink" href="#Takeaway" title="Permalink"></a></h3><p>There are a at least two possible things you could think to do as a consequence of this. The first is to accept that this is just a feature of AD that sparse linear algebra should inherit. The second is to attempt to alter it using carefully chosen rules for linear algebra.</p><p>The second option appears to necessitate the same kind of dense (co)tangents as <a href="abstract_array_dilemma.html#functions_of_sparse_arrays">before</a>, precluding differentiation of a <a href="abstract_array_dilemma.html#intermediate_functions">very large class of important functions</a>. Conversely, the above examples are somewhat contrived, and it&#39;s not clear whether important examples exist in practice. However, it doesn&#39;t resolve the underlying problem, which is AD-related, not rule-related.</p><p>The first option does leave us with some behaviour which it is hard to argue is desirable, however, all ADs of which we are aware suffer this problem this exact problem regardless your rule system.</p><h2 id="Conclusion"><a class="docs-heading-anchor" href="#Conclusion">Conclusion</a><a id="Conclusion-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusion" title="Permalink"></a></h2><p>It seems clear that the price to pay for AD whose performance is acceptable for sparse array types is seemingly-unintuitive results. However, these results are easily understood if sparse arrays are simply thought of as <code>struct</code>s, rather than array-like things, in the context of AD.</p><p>Indeed, not treating sparse arrays like other <code>struct</code>s would mean that they&#39;re inconsistent with other <code>struct</code>s. When viewed in this light, it is less obvious that these results are undesirable.</p><p>It does not appear, however, that we need lose any functionality by imposing such restrictions (sparse arrays can always be <code>collect</code>ed if necessary).</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="many_differentials.html">« Many Differential Types</a><a class="docs-footer-nextpage" href="../api.html">API »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 6 May 2021 15:24">Thursday 6 May 2021</span>. Using Julia version 1.6.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
