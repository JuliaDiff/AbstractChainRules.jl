<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Changing the Primal · ChainRules</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/chainrules.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../index.html"><img src="../assets/logo.svg" alt="ChainRules logo"/></a><div class="docs-package-name"><span class="docs-autofit">ChainRules</span></div><form class="docs-search" action="../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../index.html">Introduction</a></li><li><a class="tocitem" href="../FAQ.html">FAQ</a></li><li><a class="tocitem" href="../writing_good_rules.html">Writing Good Rules</a></li><li><a class="tocitem" href="../complex.html">Complex Numbers</a></li><li><a class="tocitem" href="../arrays.html">Deriving Array Rules</a></li><li><a class="tocitem" href="../debug_mode.html">Debug Mode</a></li><li><span class="tocitem">Usage in AD</span><ul><li><a class="tocitem" href="../autodiff/overview.html">Overview</a></li><li><a class="tocitem" href="../autodiff/operator_overloading.html">Operator Overloading</a></li></ul></li><li><span class="tocitem">Design</span><ul><li class="is-active"><a class="tocitem" href="changing_the_primal.html">Changing the Primal</a><ul class="internal"><li><a class="tocitem" href="#The-Journey-to-rrule"><span>The Journey to <code>rrule</code></span></a></li><li><a class="tocitem" href="#More-Shared-Work-Examples"><span>More Shared Work Examples</span></a></li><li class="toplevel"><a class="tocitem" href="#Conclusion"><span>Conclusion</span></a></li></ul></li><li><a class="tocitem" href="many_differentials.html">Many Differential Types</a></li></ul></li><li><a class="tocitem" href="../api.html">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Design</a></li><li class="is-active"><a href="changing_the_primal.html">Changing the Primal</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="changing_the_primal.html">Changing the Primal</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDiff/ChainRulesCore.jl/blob/master/docs/src/design/changing_the_primal.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Design-Notes:-Why-can-you-change-the-primal-computation?"><a class="docs-heading-anchor" href="#Design-Notes:-Why-can-you-change-the-primal-computation?">Design Notes: Why can you change the primal computation?</a><a id="Design-Notes:-Why-can-you-change-the-primal-computation?-1"></a><a class="docs-heading-anchor-permalink" href="#Design-Notes:-Why-can-you-change-the-primal-computation?" title="Permalink"></a></h1><p>#TODO generalize intro/title to ask why <code>rrule</code> is how it is, and in particular about changing primal</p><p>These design notes are to help you understand why ChainRules allows the primal computation to be changed. We will focus this discussion on reverse mode and <code>rrule</code>, though the same also applies to forward mode and <code>frule</code>. In fact, it has a particular use in forward mode for efficiently calculating the pushforward of a differential equation solve via expanding the system of equations to also include the derivatives and solving all at once. In forward mode it is related to the fusing of <code>frule</code> and <code>pushforward</code>. In reverse mode we can focus on the the distinct primal and gradient passes.</p><h2 id="The-Journey-to-rrule"><a class="docs-heading-anchor" href="#The-Journey-to-rrule">The Journey to <code>rrule</code></a><a id="The-Journey-to-rrule-1"></a><a class="docs-heading-anchor-permalink" href="#The-Journey-to-rrule" title="Permalink"></a></h2><p>Let&#39;s imagine a different system for rules, one that doesn&#39;t let you do this. This system is what a lot of AD systems have. It is what <a href="https://github.com/invenia/Nabla.jl/">Nabla.jl</a><sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup> had originally. We will have a primal (i.e. forward) pass that directly executes the primal function and just records its <em>inputs</em> and its <em>output</em> (as well as the <em>primal function</em> itself) onto the tape.<sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup>. Then during the gradient (i.e. reverse) pass it has a function which receives those records from the tape along with the sensitivity of the output, and gives back the sensitivity of the input. We will call this function <code>pullback_at</code>, as it pulls back the sensitivity at a given primal point. To make this concrete:</p><pre><code class="language-julia">y = f(x)  # primal program
x̄ = pullback_at(f, x, y, ȳ)</code></pre><p>Let&#39;s write <code>sin</code>:</p><pre><code class="language-julia">y = sin(x)
pullback_at(::typeof(sin), x, y, ȳ) = ȳ * cos(x)</code></pre><p>Great. So far so good. How about the <a href="https://en.wikipedia.org/wiki/Logistic_function#Derivative">logistic sigmoid</a>?</p><pre><code class="language-julia">σ(x) = 1/(1 + exp(-x))  # = exp(x)/(1+exp(x))
y = σ(x)
pullback_at(::typeof(σ), x, y, ȳ) = ȳ * y * σ(-x)  # i.e. ȳ * σ(x) * σ(-x)</code></pre><p>Notice that here we are in the <code>pullback_at</code> not only using <code>x</code> but also <code>y</code> the primal output. This is a nice bit of symmetry that shows up around <code>exp</code>.</p><p>Now let&#39;s consider why we implement <code>rrule</code>s in the first place. One key reason <sup class="footnote-reference"><a id="citeref-3" href="#footnote-3">[3]</a></sup> is to allow us to insert our domain knowledge to do better than the AD would do just by breaking everything down into <code>+</code>, <code>*</code>, etc. What insights do we have about <code>sin</code> and <code>cos</code>? Here is one:</p><pre><code class="language-julia">julia&gt; using BenchmarkTools

julia&gt; @btime sin(x) setup=(x=rand());
  3.838 ns (0 allocations: 0 bytes)

julia&gt; @btime cos(x) setup=(x=rand());
  4.795 ns (0 allocations: 0 bytes)

julia&gt; 3.838 + 4.795
8.633</code></pre><p>vs computing both together:</p><pre><code class="language-julia">julia&gt; @btime sincos(x) setup=(x=rand());
  6.028 ns (0 allocations: 0 bytes)</code></pre><p>What about the logistic sigmoid? We note that the two values we need are <code>σ(x)</code> and <code>σ(-x)</code> If we write these as: <span>$\sigma(x) = \frac{e^x}{1+e^x}$</span> and <span>$\sigma(-x) = \frac{1}{1+e^x}$</span> then we see they have the common term <span>$e^x$</span>. <code>exp(x)</code> is a much more expensive operation than <code>+</code> and <code>/</code>. So we can save time, if we can reuse that <code>exp(x)</code>. If we have to compute it twice:</p><pre><code class="language-julia">julia&gt; @btime 1/(1+exp(x)) setup=(x=rand());
  5.622 ns (0 allocations: 0 bytes)

julia&gt; @btime 1/(1+exp(-x)) setup=(x=rand());
  6.036 ns (0 allocations: 0 bytes)

julia&gt; 5.622 + 6.036
11.658</code></pre><p>vs reusing <code>exp(x)</code>:</p><pre><code class="language-julia">julia&gt; @btime exp(x) setup=(x=rand());
  5.367 ns (0 allocations: 0 bytes)

julia&gt; @btime ex/(1+ex) setup=(ex=exp(rand()));
  1.255 ns (0 allocations: 0 bytes)

julia&gt; @btime 1/(1+ex) setup=(ex=exp(rand()));
  1.256 ns (0 allocations: 0 bytes)

julia&gt; 5.367 + 1.255 + 1.256
7.878</code></pre><p>So we are talking about a 30-40%<sup class="footnote-reference"><a id="citeref-4" href="#footnote-4">[4]</a></sup> speed-up from these optimizations. It&#39;s faster to  compute <code>sin</code> and <code>cos</code> at the same time via <code>sincos</code> than it is to compute them one after the other. And it is faster to reuse the <code>exp(x)</code> in computing <code>σ(x)</code> and <code>σ(-x)</code>. How can we incorporate this insight into our system? We know we can compute both of these in the primal, because they only depend on <code>x</code> –- we don&#39;t need to know <code>ȳ</code>. But there is nowhere to put it that is accessible both to the primal pass and the gradient pass code.</p><p>What if we introduced some variable called <code>intermediates</code> that is also recorded onto the tape during the primal pass? We would need to be able to modify the primal pass to do this, so that we can actually put the data into the <code>intermediates</code>. So we will introduce a function: <code>augmented_primal</code>, that will return the primal output plus the <code>intermediates</code> that we want to reuse in the gradient pass. Then we will make our AD system replace calls to the primal with calls to the <code>augmented_primal</code> of the primal function and take care of all the bookkeeping. So that would look like:</p><pre><code class="language-julia">y = f(x)  # primal program
y, intermediates = augmented_primal(f, x)
x̄ = pullback_at(f, x, y, ȳ, intermediates)</code></pre><p>Let&#39;s try writing this now for <code>sin</code>:</p><pre><code class="language-julia">function augmented_primal(::typeof(sin), x)
  y, cx = sincos(x)
  return y, (; cx=cx)  # use a NamedTuple for the intermediates
end

pullback_at(::typeof(sin), x, y, ȳ, intermediates) = ȳ * intermediates.cx</code></pre><p>and for the logistic sigmoid:</p><pre><code class="language-julia">function augmented_primal(::typeof(σ), x)
  ex = exp(x)
  y = ex/(1 + ex)
  return y, (; ex=ex)  # use a NamedTuple for the intermediates
end

pullback_at(::typeof(σ), x, y, ȳ, intermediates) = ȳ * y /(1 + intermediates.ex)</code></pre><p>Cool! That lets us do what we wanted. We net decreased the time it takes to run the primal and gradient passes. We have now demonstrated the title question of why we needed to be able to modify the primal pass. We will go into that more later and have some more usage examples. But first let&#39;s continue to see how we go from that <code>augmented_primal</code> to <code>pullback_at</code> to <a href="../api.html#ChainRulesCore.rrule-Tuple{Any,Vararg{Any,N} where N}"><code>rrule</code></a>.</p><p>One thing we notice when looking at <code>pullback_at</code> is it really is starting to have a lot of arguments. It had a fair few already, and now we are adding <code>intermediates</code> as well. Not to mention this is a fairly simple function, only 1 input, no keyword arguments. Furthermore, we don&#39;t even use all of them all the time. The original new code for pulling back <code>sin</code> no longer needs the <code>x</code>, and it never needed <code>y</code> (though <code>sigmoid</code> does). The function signature for <code>pullback_at</code> was already rather long with the primal inputs and outputs and the sensitivity. Now it is even longer since we added <code>intermediates</code>. Further, storing all all those things on the tape is using up extra memory. What if we generalized the idea of the <code>intermediate</code> named tuple, and had a struct that just held anything we might want put on the tape.</p><pre><code class="language-julia">struct PullbackMemory{P, S}
  primal_function::P
  state::S
end
# convenience constructor:
Memory(primal_function; state...) = PullbackMemory(primal_function, state)
# convenience accessor so that `m.x` is same as `m.state.x`
Base.getproperty(m::PullbackMemory, propname) = getproperty(getfield(m, :state), propname)</code></pre><p>So changing our API we have:</p><pre><code class="language-julia">y = f(x)  # primal program
y, pb = augmented_primal(f, x)
x̄ = pullback_at(pb, ȳ)</code></pre><p>which is much cleaner.</p><p>Let&#39;s try writing with this for <code>sin</code>:</p><pre><code class="language-julia">function augmented_primal(::typeof(sin), x)
  y, cx = sincos(x)
  return y, PullbackMemory(sin; cx=cx)
end

pullback_at(pb::PullbackMemory{typeof(sin)}, ȳ) = ȳ * pb.cx</code></pre><p>and for the logistic sigmoid:</p><pre><code class="language-julia">function augmented_primal(::typeof(σ), x)
  ex = exp(x)
  y = ex/(1 + ex)
  return y, PullbackMemory(σ; y=y, ex=ex)
end

pullback_at(pb::PullbackMemory{typeof(σ)}, ȳ) = ȳ * pb.y/(1 + pb.ex)</code></pre><p>I think that looks pretty nice.</p><p>One way we could make it look a bit nicer for usage is if the <code>PullbackMemory</code> was actually a callable object. <code>pullback_at</code> only has the 2 arguments. Conceptually the <code>PullbackMemory</code> is a fixed thing it the contents of the tape for a particular operation. It is fully determined by the end of the primal pass. The during the gradient (reverse) pass the <code>PullbackMemory</code> is used to successively compute the <code>ȳ</code>  argument. So it makes sense to have <code>PullbackMemory</code> being a callable object that acts on the sensitivity. We can do that via call overloading:</p><pre><code class="language-julia">y = f(x)  # primal program
y, pb = augmented_primal(f, x)
x̄ = pb(ȳ)</code></pre><p>and for <code>sin</code>:</p><pre><code class="language-julia">function augmented_primal(::typeof(sin), x)
  y, cx = sincos(x)
  return y, PullbackMemory(sin; cx=cx)
end
(pb::PullbackMemory)(ȳ) = ȳ * pb.cx</code></pre><p>and for the logistic sigmoid:</p><pre><code class="language-julia">function augmented_primal(::typeof(σ), x)
  ex = exp(x)
  y = ex/(1 + ex)
  return y, PullbackMemory(σ; y=y, ex=ex)
end

(pb::PullbackMemory{typeof(σ)})(ȳ) = ȳ * pb.y/(1 + pb.ex)</code></pre><p>Those looking closely will spot what we have done here. We now have an object (<code>pb</code>) that acts on the cotangent of the output of the primal (<code>ȳ</code>) to give us the cotangent of the input of the primal function (<code>x̄</code>). <em><code>pb</code> is not just the <strong>memory</strong> of state required for the <code>pullback</code>, it <strong>is</strong> the pullback.</em></p><p>We have one final thing to do. Let&#39;s think about making the code easy to modify. Let&#39;s go back and think about the changes we would have make to go from our original way of writing that only used the inputs/outputs, to one that used the intermediate state. For <code>sin</code> to rewrite that original formulation in the new pullback form we have:</p><pre><code class="language-julia">function augmented_primal(::typeof(sin), x)
  y = sin(x)
  return y, PullbackMemory(sin; x=x)
end
(pb::PullbackMemory)(ȳ) = ȳ * cos(pb.x)</code></pre><p>To go from that to:</p><pre><code class="language-julia">function augmented_primal(::typeof(sin), x)
  y, cx = sincos(x)
  return y, PullbackMemory(sin; cx=cx)
end
(pb::PullbackMemory)(ȳ) = ȳ * pb.cx</code></pre><p>and for logistic sigmoid, the original would have been:</p><pre><code class="language-julia">function augmented_primal(::typeof(σ), x)
  y = σ(x)
  return y, PullbackMemory(σ; y=y, x=x)
end
(pb::PullbackMemory{typeof(σ)})(ȳ) = ȳ * pb.y * σ(-pb.x)</code></pre><p>to get to:</p><pre><code class="language-julia">function augmented_primal(::typeof(σ), x)
  ex = exp(x)
  y = ex/(1 + ex)
  return y, PullbackMemory(σ; y=y, ex=ex)
end
(pb::PullbackMemory{typeof(σ)})(ȳ) = ȳ * pb.y/(1 + pb.ex)</code></pre><p>(NB: there is actually a further optimization that can be made to the logistic sigmoid, to avoid remembering two things and just remember one. As an exercise to the reader, consider how the code would need to be changed and where.)</p><p>We need to make a series of changes. We need to update what work is done in the primal to compute the intermediate values. We need to update what was stored in the <code>PullbackMemory</code>. And we need to update the the function that applies the pullback so it uses the new thing that was stored. It&#39;s important these parts all stay in sync. It&#39;s not too bad for this simple example with just one or two things to remember. For more complicated multi-argument functions, like will be talked about below, you often end up needing to remember half a dozen things, like sizes and indices relating to each input/output. So it gets a little more fiddly to make sure you remember all the things you need to and give them the same name in both places. <em>Is there a way we can automatically just have all the things we use remembered for us?</em></p><p>Surprisingly for such a specific request, there actually is. This is a closure. A closure in Julia is a callable structure that automatically contains a field for every object from its parent scope that is used in its body. There are <a href="https://invenia.github.io/blog/2019/10/30/julialang-features-part-1#closures-give-us-classic-object-oriented-programming">incredible ways to abuse this</a>; but here we can in-fact use closures exactly as they are intended. Replacing <code>PullbackMemory</code> with a closure that works the same way lets us avoid having to manually control what is remembered <em>and</em> lets us avoid separately writing the call overload. So we have for <code>sin</code>:</p><pre><code class="language-julia">function augmented_primal(::typeof(sin), x)
  y, cx = sincos(x)
  pb = ȳ -&gt; cx * ȳ  # pullback closure. closes over `cx`
  return y, pb
end</code></pre><p>and for logistic sigmoid:</p><pre><code class="language-julia">function augmented_primal(::typeof(σ), x)
  ex = exp(x)
  y = ex/(1 + ex)
  pb = ȳ -&gt; ȳ * y/(1 + ex)  # pullback closure. closes over `y` and `ex`
  return y, pb
end</code></pre><p>This is pretty clean now.</p><p>Our <code>augmented_primal</code> is now within spitting distance of <code>rrule</code>. All that is left is a rename and some extra conventions around multiple outputs and gradients with respect to callable objects.</p><p>This has been a journey into how we get to <a href="../api.html#ChainRulesCore.rrule-Tuple{Any,Vararg{Any,N} where N}"><code>rrule</code></a> as it is defined in <code>ChainRulesCore</code>. We started with an unaugmented primal function and a <code>pullback_at</code> function that only saw the inputs and outputs of the primal. We realized a key limitation of this was that we couldn&#39;t share computational work between the primal and and gradient passes. To solve this we introduced the notation of some <code>intermediate</code> that is shared from the primal to the pullback. We successively improved that idea, first by making it a type that held everything that is needed for the pullback: the <code>PullbackMemory</code>, which we then made callable, so it was itself the pullback. Finally, we replaced that separate callable structure with a closure, which kept everything in one place and made it more convenient.</p><h2 id="More-Shared-Work-Examples"><a class="docs-heading-anchor" href="#More-Shared-Work-Examples">More Shared Work Examples</a><a id="More-Shared-Work-Examples-1"></a><a class="docs-heading-anchor-permalink" href="#More-Shared-Work-Examples" title="Permalink"></a></h2><p><code>sin</code> and the logistic sigmoid are nice, simple examples of when it is useful to share work between the primal and the pullback. There are many others though. It is actually surprising that in so many cases it is reasonable to write the rules where the only shared information between the primal and the pullback is the primal inputs (like our original <code>sin</code>), or primal outputs (like our original logistic sigmoid). Under our formulation above, those primal inputs/outputs are shared information just like any other. Beyond this, there are a number of other decent applications.</p><h3 id="getindex"><a class="docs-heading-anchor" href="#getindex"><code>getindex</code></a><a id="getindex-1"></a><a class="docs-heading-anchor-permalink" href="#getindex" title="Permalink"></a></h3><p>In Julia (and many other numerical languages) indexing can take many more arguments than simply a couple of integers, such as boolean masking arrays (logical indexing), ranges for slices, etc. Converting the arguments to plain integers, arrays of integers, and ranges with <code>Base.to_indices</code> is the first thing that <code>getindex</code> does. It then re-calls <code>getindex</code> with these simpler types to get the result.</p><p>The result of pulling back the <code>getindex</code> operation is always an array that is all zeros, except for the elements that are selected, which are set to the appropriate sensitivities being pulled back. To identify which actual positions in the array are being gotten/set is common work to both primal and gradient computations. We really don&#39;t want to deal with fancy indexing types during the pullback, because there are weird edge cases like indexing in such a way that the same element is output twice (and thus we have 2 sensitivities we need to add to it). We can pull the <code>to_indices</code> out of the primal computation and remember the plain indexes used, then can reuse them to set gradients during the pullback.</p><p>See the <a href="https://github.com/JuliaDiff/ChainRules.jl/blob/v0.7.49/src/rulesets/Base/indexing.jl">code for this in ChainRules.jl</a></p><h3 id="exp(::Matrix)"><a class="docs-heading-anchor" href="#exp(::Matrix)"><code>exp(::Matrix)</code></a><a id="exp(::Matrix)-1"></a><a class="docs-heading-anchor-permalink" href="#exp(::Matrix)" title="Permalink"></a></h3><p><a href="https://en.wikipedia.org/wiki/Matrix_function">Matrix Functions</a> are generalizations of scalar functions to operate on matrices. Note that this is distinct from simply element-wise application of the function to the matrix&#39;s elements. The <a href="https://en.wikipedia.org/wiki/Matrix_exponential">Matrix Exponential</a> <code>exp(::Matrix)</code> is a particularly important matrix function.</p><p>Al-Mohy and Higham (2009)<sup class="footnote-reference"><a id="citeref-6" href="#footnote-6">[6]</a></sup>, published a method for computing the pullback of <code>exp(::Matrix)</code>. It is pretty complex and very cool. To quote its abstract (emphasis mine):</p><blockquote><p>The algorithm is derived from the scaling and squaring method by differentiating the Padé approximants and the squaring recurrence, <strong>re-using quantities computed during the evaluation of the Padé approximant</strong>, and intertwining the recurrences in the squaring phase.</p></blockquote><p>Julia does in fact use a Padé approximation to compute <code>exp(::Matrix)</code>. So we can extract the code for that into our augmented primal, and add remembering the intermediate quantities that are to be used. See the <a href="https://github.com/JuliaDiff/ChainRules.jl/blob/v0.7.49/src/rulesets/LinearAlgebra/matfun.jl">code for this in ChainRules.jl</a></p><p>An interesting scenario here that may be of concern to some: if Julia changes the algorithm it uses to compute <code>exp(::Matrix)</code>, then during an AD primal pass, it will continue to use the old Padé approximation based algorithm. This may actually happen, as there are many other algorithms that can compute the matrix exponential. Further, perhaps there might be an improvement to the exact coefficient or cut-offs used by Julia&#39;s current Padé approximation. If Julia made this change it wopuld not be considered breaking. <a href="http://colprac.sciml.ai/#changes-that-are-not-considered-breaking">Exact floating point numerical values are not generally considered part of the SemVer-bound API</a>. Rather only the general accuracy of the computed value relative to the true mathematical value (e.g. for common scalar operations Julia promises 1 <a href="https://en.wikipedia.org/wiki/Unit_in_the_last_place">ULP</a>).</p><p>This change will result in the output of the AD primal pass not being exactly equal to what would be seen from just running the primal code. It will still be accurate because the current implementation is accurate, but it will be different. It is <a href="https://forums.swift.org/t/agreement-of-valuewithdifferential-and-value/31869">our argument</a> that in general this should be considered acceptable, as long as the AD primal pass is in general about as accurate as the unaugmented primal. E.g. it might overshoot for some values the unaugmented primal undershoots for.</p><h3 id="eigvals"><a class="docs-heading-anchor" href="#eigvals"><code>eigvals</code></a><a id="eigvals-1"></a><a class="docs-heading-anchor-permalink" href="#eigvals" title="Permalink"></a></h3><p><code>eigvals</code> is a real case where the algorithm for the augmented primal and the original primal <em>is already different today</em>. To compute the pullback of <code>eigvals</code> you need to know not only the eigenvalues but also the eigenvectors. The <code>eigen</code> function computes both, so that is used in the augmented primal. See the <a href="https://github.com/JuliaDiff/ChainRules.jl/blob/v0.7.49/src/rulesets/LinearAlgebra/factorization.jl#L209-L218">code for this in ChainRules.jl</a>. If we could not compute and remember the eigenvectors in the primal pass, we would have to call <code>eigen</code> in the gradient pass anyway and fully recompute eigenvectors and eigenvalues, more than doubling the total work.</p><p>However, if you trace this down, it actually uses a different algorithm.</p><p><code>eigvals</code> basically wraps <code>LAPACK.syevr!(&#39;N&#39;, ...)</code>, which goes through <a href="http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_gaeed8a131adf56eaa2a9e5b1e0cce5718.html">DSYEVR</a> and eventually calls <a href="http://www.netlib.org/lapack/explore-html/d2/d24/group__aux_o_t_h_e_rcomputational_gaf0616552c11358ae8298d0ac18ac023c.html#gaf0616552c11358ae8298d0ac18ac023c">DSTERF</a>, which uses <em>&quot;Pal-Walker-Kahan variant of the QL or QR algorithm.&quot;</em> to compute eigenvalues</p><p>In contrast, <code>eigen</code> wraps <code>LAPACK.syevr!(&#39;V&#39;,...)</code> which also goes through <a href="http://www.netlib.org/lapack/explore-html/d2/d8a/group__double_s_yeigen_gaeed8a131adf56eaa2a9e5b1e0cce5718.html">DSYEVR</a> but eventually calls <a href="http://www.netlib.org/lapack/explore-html/da/dba/group__double_o_t_h_e_rcomputational_ga14daa3ac4e7b5d3712244f54ce40cc92.html#ga14daa3ac4e7b5d3712244f54ce40cc92">DSTEMR</a>, which calculates eigenvalues <em>&quot;either by bisection or the dqds algorithm.&quot;</em>.</p><p>Both of these are very good algorithms. LAPACK has had decades of work by experts and is one of the most trusted libraries for linear algebra. But they are different algorithms that give different results. The differences in practice are around <span>$10^{-15}$</span>, which while very small on absolute terms are as far as <code>Float64</code> is concerned a very real difference.</p><h3 id="Matrix-Division"><a class="docs-heading-anchor" href="#Matrix-Division">Matrix Division</a><a id="Matrix-Division-1"></a><a class="docs-heading-anchor-permalink" href="#Matrix-Division" title="Permalink"></a></h3><p>Roughly speaking: <code>Y=A\B</code> is the function that finds the least-square solution to <code>YA ≈ B</code>. When solving such a system, the efficient way to do so is to factorize <code>A</code> into an appropriate factorized form such as <code>Cholesky</code> or <code>QR</code>, then perform the <code>\</code> operation on the factorized form. The pullback of <code>A\B</code> with respect to <code>B</code> is <code>Ȳ-&gt; A&#39; \ Ȳ</code>. It should be noted that this involves computing the factorization of <code>A&#39;</code> (the adjoint of <code>A</code>). In this computation the factorization of the original <code>A</code> can reused. Doing so can give a 4x speed-up.</p><p>We don&#39;t have this in ChainRules.jl yet, because Julia is missing some definitions of <code>adjoint</code> of factorizations (<a href="https://github.com/JuliaLang/julia/issues/38293">JuliaLang/julia#38293</a>). I have been promised them for Julia v1.7 though. You can see what the code would look like in <a href="https://github.com/JuliaDiff/ChainRules.jl/pull/302">PR #302</a>.</p><h1 id="Conclusion"><a class="docs-heading-anchor" href="#Conclusion">Conclusion</a><a id="Conclusion-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusion" title="Permalink"></a></h1><p>This document has explained why <a href="../api.html#ChainRulesCore.rrule-Tuple{Any,Vararg{Any,N} where N}"><code>rrule</code></a> is the way it is. In particular it has highlighted why the primal computation is able to be changed from simply calling the function. Further, it has explained why <code>rrule</code> returns a closure for the pullback, rather than it being a separate function. It has highlighted several places in ChainRules.jl where this has allowed us to significantly improve performance. Being able to change the primal computation is practically essential for a high performance AD system.</p><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>I am not just picking on Nabla randomly. Many of the core developers of ChainRules worked on Nabla prior. It&#39;s a good AD, but ChainRules incorporates lessons learned from working on Nabla.</li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>which may be an explicit tape or an implicit tape that is actually incorporated into generated code (à la Zygote)</li><li class="footnote" id="footnote-3"><a class="tag is-link" href="#citeref-3">3</a>Another key reason is if the operation is a primitive that is not defined in terms of more basic operations. In many languages this is the case for <code>sin</code>; where the actual implementation is in some separate <code>libm.so</code>. But actually <code>sin</code> in Julia is <a href="https://github.com/JuliaLang/julia/blob/caeaceff8af97565334f35309d812566183ec687/base/special/trig.jl">defined in terms of a polynomial</a>. It&#39;s fairly vanilla Julia code. It shouldn&#39;t be too hard for an AD that only knows about basic operations like <code>+</code> and <code>*</code> to AD through it. In any case, that is another discussion for another day.</li><li class="footnote" id="footnote-4"><a class="tag is-link" href="#citeref-4">4</a>Sure, this is small fries and depending on Julia version might just get solved by the optimizer<sup class="footnote-reference"><a id="citeref-5" href="#footnote-5">[5]</a></sup>, but go with it for the sake of example.</li><li class="footnote" id="footnote-5"><a class="tag is-link" href="#citeref-5">5</a>To be precise, this is very likely to be solved by the optimizer inlining both and then performing common subexpression elimination, with the result that it generates the code for <code>sincos</code> just from having <code>sin</code> and <code>cos</code> inside the same function. However, this actually doesn&#39;t apply in the case of AD, as it is not possible to inline code called in the gradient pass into the primal pass. Those are separate functions called at very different times. This is something <a href="https://github.com/JuliaLang/julia/pull/37849">opaque closures</a> should help solve.</li><li class="footnote" id="footnote-6"><a class="tag is-link" href="#citeref-6">6</a><a href="http://eprints.maths.manchester.ac.uk/1218/">Al-Mohy, Awad H. and Higham, Nicholas J. (2009) <em>Computing the Fréchet Derivative of the Matrix Exponential, with an application to Condition Number Estimation</em>. SIAM Journal On Matrix Analysis and Applications., 30 (4). pp. 1639-1657. ISSN 1095-7162</a></li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../autodiff/operator_overloading.html">« Operator Overloading</a><a class="docs-footer-nextpage" href="many_differentials.html">Many Differential Types »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 26 January 2021 14:34">Tuesday 26 January 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
